{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d441c3-bc5f-4c43-a071-57de3ef6bb20",
   "metadata": {},
   "source": [
    "# process_rnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db131dd9-0034-41d4-abc4-ae0f85f9665e",
   "metadata": {},
   "source": [
    "**Author:** Marilyn Braojos Gutierrez\\\n",
    "**Purpose:** This program aims to process the local RINEX files with GPS satellite broadcast information and create files for each satellite.\\\n",
    "**PhD Milestone:** #1: *Leverage deep learning models to GPS satellite clock bias corrections.*\\\n",
    "**Project:** This program is Step (1) in this PhD milestone. Obtaining the data is the first critical step.\\\n",
    "**References:**\\\n",
    "[1] https://cddis.nasa.gov/Data_and_Derived_Products/GNSS/broadcast_ephemeris_data.html#GPShourly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59044d89-3468-4362-9153-2e3d632a2057",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbdccf52-4b54-4775-969a-204f388e0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import matplotlib.pyplot as plt                               # https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n",
    "import numpy as np                                            # https://numpy.org/\n",
    "import os                                                     # https://docs.python.org/3/library/os.html\n",
    "import time                                                   # https://docs.python.org/3/library/time.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e710422-015e-48ec-a268-8a20fedd6307",
   "metadata": {},
   "source": [
    "# Processing RINEX Broadcast Files and Extracting Clock Bias Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac69a7cb-38e1-4914-8115-21f045e86425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rnx(yr, dd):\n",
    "    try:\n",
    "        base_dir = 'rnx'        \n",
    "        day_folder = f\"gps_rnx_daily_{yr}{dd}\"\n",
    "        target_dir = os.path.join(base_dir, day_folder)\n",
    "\n",
    "        if not os.path.isdir(target_dir):\n",
    "            raise FileNotFoundError(f\"Directory {target_dir} does not exist.\")\n",
    "\n",
    "\n",
    "        rnx_files = [file for file in os.listdir(target_dir) if file.endswith('.rnx')] # case-sensitive\n",
    "        \n",
    "        data = []\n",
    "        encoding_errors = []  # List to keep track of encoding errors\n",
    "\n",
    "        line_ct = 0\n",
    "\n",
    "\n",
    "        for i, rnx_file in enumerate(rnx_files):\n",
    "            try: \n",
    "                file_path = os.path.join(target_dir, rnx_file) \n",
    "    \n",
    "                with open(file_path, 'rb') as file:\n",
    "                    raw_data = file.read()\n",
    "                    result = chardet.detect(raw_data)\n",
    "                    encoding = result['encoding']\n",
    "                    confidence = result['confidence']\n",
    "    \n",
    "                # Use fallback encoding if detection fails\n",
    "                if encoding is None:\n",
    "                    encoding = 'utf-8'\n",
    "                    print(f\"Using fallback encoding {encoding} for file {file_path}\")\n",
    "                    print(f\"Detected encoding {encoding} with confidence {confidence} for file {file_path}\")\n",
    "    \n",
    "                try: \n",
    "                    with open(file_path, 'r', encoding=encoding) as file:\n",
    "                        first_line = file.readline()\n",
    "        \n",
    "                        is_version_3 = False\n",
    "                        is_not_version_3 = False\n",
    "                        end_header = False\n",
    "        \n",
    "                        if '3' in first_line[0:21]: \n",
    "                            is_version_3 = True\n",
    "                        else:\n",
    "                            is_not_version_3 = True\n",
    "        \n",
    "                        for line in file:                \n",
    "                            if not end_header:\n",
    "                                if 'END OF HEADER' in line:\n",
    "                                    end_header = True\n",
    "                                continue\n",
    "                            \n",
    "                            if line[0:3].startswith('G'):\n",
    "                                if end_header: \n",
    "                                    if is_version_3:  \n",
    "                                        subs_lines = [file.readline() for _ in range(6)]\n",
    "                                        \n",
    "                                        if len(subs_lines) == 6 and all(subs_lines):\n",
    "                                            subs_line1, subs_line2, subs_line3, subs_line4, subs_line5, subs_line6 = subs_lines\n",
    "        \n",
    "                                        # subs_line1 = file.readline()\n",
    "                                        # subs_line2 = file.readline()\n",
    "                                        # subs_line3 = file.readline()\n",
    "                                        # subs_line4 = file.readline()\n",
    "                                        # subs_line5 = file.readline()\n",
    "                                        # subs_line6 = file.readline()\n",
    "        \n",
    "                                        \n",
    "                                        row = {\n",
    "                                            'SV Name': line[0:3],\n",
    "                                            'Epoch Year': int(line[3:8]),\n",
    "                                            'Epoch Month': int(line[8:11]),\n",
    "                                            'Epoch Day': int(line[11:14]),\n",
    "                                            'Epoch Hour': int(line[14:17]),\n",
    "                                            'Epoch Minute': int(line[17:20]),\n",
    "                                            'Epoch Second': int(line[20:23]),\n",
    "                                            'Clock Bias Coefficient (seconds)': float(line[23:42].lower().replace('d', 'e')),\n",
    "                                            'Clock Bias Drift Coefficient (seconds/second)': float(line[42:61].lower().replace('d', 'e')),\n",
    "                                            'Clock Bias Drift Rate Coefficient (seconds/second^2)': float(line[61:80].lower().replace('d', 'e')),\n",
    "                                            \n",
    "                                            'Mean Motion Difference (semi-circles/sec)': float(subs_line1[42:61].lower().replace('d', 'e')),\n",
    "                                            'Mean Anomaly at Reference (semi-circles)': float(subs_line1[61:80].lower().replace('d', 'e')),\n",
    "                                            \n",
    "                                            'Eccentricity (unitless)': float(subs_line2[23:42].lower().replace('d', 'e')),\n",
    "                                            'SQRT(Semi-Major Axis) (SQRT(meters))': float(subs_line2[61:80].lower().replace('d', 'e')),\n",
    "                                            \n",
    "                                            'Time of Ephemeris (seconds)': float(subs_line3[4:23].lower().replace('d', 'e')),\n",
    "        \n",
    "                                            'SV Health (0=OK)': float(subs_line6[23:42].lower().replace('d', 'e')),\n",
    "        \n",
    "                                            'File': file_path,\n",
    "                                                }\n",
    "                                        data.append(row)    \n",
    "                                    \n",
    "                                    # https://files.igs.org/pub/data/format/rinex_clock300.txt\n",
    "                                    elif is_not_version_3:   \n",
    "                                        if line[3:13].startswith('G'):\n",
    "                                            print(\"File is an unreadable version for now\")\n",
    "        \n",
    "                    print(f\"Done reading file {i + 1} out of {len(rnx_files)}. Filepath: {file_path}\")\n",
    "                \n",
    "                except (UnicodeDecodeError, FileNotFoundError) as decode_err:\n",
    "                    encoding_errors.append((file_path, str(decode_err)))\n",
    "                    print(f\"Encoding error for file {file_path}: {decode_err}. Skipping this file.\")\n",
    "            \n",
    "            except Exception as e: \n",
    "                print(f\"Error processing file {file_path}: {e}. Skipping this file.\")\n",
    "                continue\n",
    "                \n",
    "        if encoding_errors:\n",
    "            print(\"Files with encoding errors:\")\n",
    "            for file_path, error in encoding_errors:\n",
    "                print(f\"{file_path}: {error}\")\n",
    "\n",
    "        sv_names = [entry['SV Name'] for entry in data]\n",
    "        print(f\"SV Names for GPS day {dd} of year {yr} have been saved.\")\n",
    "        \n",
    "        epoch_years = [entry['Epoch Year'] for entry in data]\n",
    "        print(f\"Epoch Years for GPS day {dd} of year {yr} have been saved.\")\n",
    "        \n",
    "        epoch_months = [entry['Epoch Month'] for entry in data]\n",
    "        print(f\"Epoch Months for GPS day {dd} of year {yr} have been saved.\")\n",
    "        \n",
    "        epoch_days = [entry['Epoch Day'] for entry in data]\n",
    "        print(f\"Epoch Days for GPS day {dd} of year {yr} have been saved.\")\n",
    "        \n",
    "        epoch_hours = [entry['Epoch Hour'] for entry in data]\n",
    "        print(f\"Epoch Hours for GPS day {dd} of year {yr} have been saved.\")\n",
    "        \n",
    "        epoch_minutes = [entry['Epoch Minute'] for entry in data]\n",
    "        print(f\"Epoch Minutes for GPS day {dd} of year {yr} have been saved.\")\n",
    "        \n",
    "        epoch_seconds = [entry['Epoch Second'] for entry in data]\n",
    "        print(f\"Epoch Seconds for GPS day {dd} of year {yr} have been saved.\")\n",
    "        \n",
    "        coeff_clock_biases = [entry['Clock Bias Coefficient (seconds)'] for entry in data]\n",
    "        print(f\"Clock bias coefficients (seconds) for GPS day {dd} of year {yr} have been saved.\")\n",
    "\n",
    "        coeff_clock_drifts = [entry['Clock Bias Drift Coefficient (seconds/second)'] for entry in data]\n",
    "        print(f\"Clock drift coefficients (seconds) for GPS day {dd} of year {yr} have been saved.\")\n",
    "\n",
    "        coeff_clock_drift_rates = [entry['Clock Bias Drift Rate Coefficient (seconds/second^2)'] for entry in data]\n",
    "        print(f\"Clock drift rates (seconds) for GPS day {dd} of year {yr} have been saved.\")\n",
    "\n",
    "        mean_motion_diffs = [entry['Mean Motion Difference (semi-circles/sec)'] for entry in data]\n",
    "        print(f\"Mean Motion Differences (semi-circles/sec) for GPS day {dd} of year {yr} have been saved.\")\n",
    "                \n",
    "        mean_anomaly_refs = [entry['Mean Anomaly at Reference (semi-circles)'] for entry in data]\n",
    "        print(f\"Mean Anomaly Reference (semi-circles) for GPS day {dd} of year {yr} have been saved.\")\n",
    "                \n",
    "        eccs = [entry['Eccentricity (unitless)'] for entry in data]\n",
    "        print(f\"Eccentricity (unitless) for GPS day {dd} of year {yr} have been saved.\")\n",
    "                \n",
    "        semi_major_axs = [entry['SQRT(Semi-Major Axis) (SQRT(meters))'] for entry in data]\n",
    "        print(f\"SQRT(Semi-Major Axis) (SQRT(m)) for GPS day {dd} of year {yr} have been saved.\")\n",
    "                \n",
    "        t_oes = [entry['Time of Ephemeris (seconds)'] for entry in data]\n",
    "        print(f\"Time of Ephemeris (seconds) for GPS day {dd} of year {yr} have been saved.\")\n",
    "\n",
    "        health = [entry['SV Health (0=OK)'] for entry in data]\n",
    "        print(f\"SV Health (0=OK) for GPS day {dd} of year {yr} have been saved.\")\n",
    "\n",
    "        file_names = [entry['File'] for entry in data]\n",
    "        print(f\"Filepaths for GPS day {dd} of year {yr} have been saved.\")\n",
    "\n",
    "        combined = list(zip(sv_names,\n",
    "                            epoch_years,\n",
    "                            epoch_months,\n",
    "                            epoch_days,\n",
    "                            epoch_hours,\n",
    "                            epoch_minutes,\n",
    "                            epoch_seconds,\n",
    "                            coeff_clock_biases, \n",
    "                            coeff_clock_drifts, \n",
    "                            coeff_clock_drift_rates, \n",
    "                            mean_motion_diffs,\n",
    "                            mean_anomaly_refs,\n",
    "                            eccs,\n",
    "                            semi_major_axs,\n",
    "                            t_oes,\n",
    "                            health,\n",
    "                            file_names,\n",
    "                           ))\n",
    "        \n",
    "        combined_sorted = sorted(combined, key=lambda x: (x[1], x[2], x[3], x[4], x[5], x[6]))\n",
    "\n",
    "        sv_names_com,epoch_years_com,epoch_months_com,epoch_days_com,epoch_hours_com,epoch_minutes_com,epoch_seconds_com, coeff_clock_biases_com, coeff_clock_drifts_com, coeff_clock_drift_rates_com, mean_motion_diffs_com, mean_anomaly_refs_com, eccs_com, semi_major_axs_com, t_oes_com, health_com, file_names_com = zip(*combined_sorted)\n",
    "\n",
    "        sv_names_com = list(sv_names_com)\n",
    "        epoch_years_com = list(epoch_years_com)\n",
    "        epoch_months_com = list(epoch_months_com)\n",
    "        epoch_days_com = list(epoch_days_com)\n",
    "        epoch_hours_com = list(epoch_hours_com)\n",
    "        epoch_minutes_com = list(epoch_minutes_com)\n",
    "        epoch_seconds_com = list(epoch_seconds_com)\n",
    "        coeff_clock_biases_com = list(coeff_clock_biases_com)\n",
    "        coeff_clock_drifts_com = list(coeff_clock_drifts_com)\n",
    "        coeff_clock_drift_rates_com = list(coeff_clock_drift_rates_com)\n",
    "        mean_motion_diffs_com = list(mean_motion_diffs_com)\n",
    "        mean_anomaly_refs_com = list(mean_anomaly_refs_com)\n",
    "        eccs_com = list(eccs_com)\n",
    "        semi_major_axs_com = list(semi_major_axs_com)\n",
    "        t_oes_com = list(t_oes_com)\n",
    "        health_com = list(health_com)\n",
    "        file_names_com = list(file_names_com)\n",
    "\n",
    "        print(f\"Data is sorted by ascending epoch for GPS day {dd} of year {yr} has been completed.\")\n",
    "        \n",
    "        npz_dir = 'rnx_npz'\n",
    "        os.makedirs(npz_dir, exist_ok=True)\n",
    "        \n",
    "        npz_filename = os.path.join(npz_dir, f'gps_rnx_daily_{yr}{dd}.npz')\n",
    "        \n",
    "        np.savez(npz_filename, \n",
    "                 satellite=sv_names_com, \n",
    "                 yyyy=epoch_years_com, \n",
    "                 mm=epoch_months_com, \n",
    "                 dd=epoch_days_com, \n",
    "                 hh=epoch_hours_com, \n",
    "                 mi=epoch_minutes_com, \n",
    "                 ss=epoch_seconds_com, \n",
    "                 coeff_clock_bias=coeff_clock_biases_com, \n",
    "                 coeff_clock_bias_drift = coeff_clock_drifts_com, \n",
    "                 coeff_clock_bias_drift_rate = coeff_clock_drift_rates_com, \n",
    "                 delta_mean_motion = mean_motion_diffs_com,\n",
    "                 ref_mean_anomaly = mean_anomaly_refs_com, \n",
    "                 eccentricity = eccs_com, \n",
    "                 semi_major_ax = semi_major_axs_com,\n",
    "                 t_oe = t_oes_com,\n",
    "                 sv_health = health_com,\n",
    "                 files = file_names_com)\n",
    "        \n",
    "        print(f\"npz file for GPS day {dd} of year {yr} has been created and stored as {npz_filename}.\")\n",
    "\n",
    "        print(f\"Data collection for GPS day {dd} of year {yr} has been completed.\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for day {dd} of year {yr}: {e}. Filepath: {file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4386da87-52ac-4d5f-89ef-648f8dd70ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "days = range(1, 366)\n",
    "days = [f'{day:03d}' for day in days]\n",
    "\n",
    "for dd in days:\n",
    "    process_rnx('2018', dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93148359-978e-4208-af4d-9cd53b5a8573",
   "metadata": {},
   "source": [
    "# Separate and Store Data Based on SV PRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c80592-9037-40fb-b1e6-0a9cb8af3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_rnx(satellite_name,yr,dd):\n",
    "\n",
    "    input_folder = \"rnx_npz\"\n",
    "    input_filename = os.path.join(input_folder, f\"gps_rnx_daily_{yr}{dd}.npz\")\n",
    "\n",
    "    if not os.path.exists(input_filename):\n",
    "        raise FileNotFoundError(f\"The file {input_filename} does not exist.\")\n",
    "    \n",
    "    data = np.load(input_filename)\n",
    "    \n",
    "    satellite_names = data['satellite']\n",
    "    year = data['yyyy']\n",
    "    month = data['mm']\n",
    "    day = data['dd']\n",
    "    hour = data['hh']\n",
    "    minutes = data['mi']\n",
    "    second = data['ss']\n",
    "    coeff_clock_bias = data['coeff_clock_bias']\n",
    "    coeff_block_bias_drift = data['coeff_clock_bias_drift']\n",
    "    coeff_clock_bias_drift_rate = data['coeff_clock_bias_drift_rate']\n",
    "    dn = data['delta_mean_motion']\n",
    "    m0 = data['ref_mean_anomaly']\n",
    "    ecc = data['eccentricity']\n",
    "    sqA = data['semi_major_ax']\n",
    "    toe = data['t_oe']\n",
    "    health = data['sv_health']\n",
    "    files = data['files']\n",
    "    \n",
    "    matching_indices = [i for i, name in enumerate(satellite_names) if name.startswith(satellite_name)]\n",
    "\n",
    "    matching_satellite_names = satellite_names[matching_indices]\n",
    "    matching_year= year[matching_indices]\n",
    "    matching_month= month[matching_indices]\n",
    "    matching_day= day[matching_indices]\n",
    "    matching_hour= hour[matching_indices]\n",
    "    matching_minutes= minutes[matching_indices]\n",
    "    matching_second= second[matching_indices]\n",
    "    matching_coeff_clock_bias= coeff_clock_bias[matching_indices]\n",
    "    matching_coeff_clock_bias_drift= coeff_block_bias_drift[matching_indices]\n",
    "    matching_coeff_clock_bias_drift_rate= coeff_clock_bias_drift_rate[matching_indices]\n",
    "    matching_dn = dn[matching_indices]\n",
    "    matching_m0 = m0[matching_indices]\n",
    "    matching_ecc = ecc[matching_indices]\n",
    "    matching_sqA = sqA[matching_indices]\n",
    "    matching_toe = toe[matching_indices]\n",
    "    matching_health = health[matching_indices]\n",
    "    matching_files= files[matching_indices]\n",
    "\n",
    "    output_folder = \"rnx_npz_sat\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    satellite_folder = os.path.join(output_folder, satellite_name)\n",
    "    os.makedirs(satellite_folder, exist_ok=True)\n",
    "\n",
    "    output_filename = os.path.join(satellite_folder, f\"gps_rnx_{yr}{dd}_{satellite_name}.npz\")\n",
    "    \n",
    "    np.savez(output_filename, \n",
    "             satellite = matching_satellite_names, \n",
    "             yyyy = matching_year, \n",
    "             mm = matching_month, \n",
    "             dd = matching_day, \n",
    "             hh = matching_hour, \n",
    "             mi = matching_minutes, \n",
    "             ss = matching_second, \n",
    "             clock_bias_coefficients = matching_coeff_clock_bias, \n",
    "             clock_bias_drift_coefficients = matching_coeff_clock_bias_drift, \n",
    "             clock_bias_drift_rate_coefficients = matching_coeff_clock_bias_drift_rate,\n",
    "             diff_mean_motion = matching_dn,\n",
    "             ref_mean_anomaly = matching_m0,\n",
    "             eccen = matching_ecc,\n",
    "             sqrt_semi = matching_sqA,\n",
    "             t_eph = matching_toe,\n",
    "             health_sv = matching_health,\n",
    "             filename = matching_files)\n",
    "    \n",
    "    print(f\"Data has been saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9ff7d-04d9-4d32-a662-e44d7bf6facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "days = range(1, 366)\n",
    "days = [f'{day:03d}' for day in days]\n",
    "\n",
    "for satellite_id in range(1, 32):\n",
    "    sat_id_str = f'G{satellite_id:02d}'\n",
    "    for dd in days:\n",
    "        try:  \n",
    "            isolate_rnx(sat_id_str,'2018', dd)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception for satellite {sat_id_str} on day {dd}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb2038-a3a6-4f1d-bb53-0fa183c81824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
