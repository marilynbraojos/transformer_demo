{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77422411-0235-4036-8f99-fa6a3e9a841e",
   "metadata": {},
   "source": [
    "# preprocess_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c53f27-a82b-4f04-91bc-af9eebfb3882",
   "metadata": {},
   "source": [
    "**Author:** Marilyn Braojos Gutierrez\\\n",
    "**Purpose:** This program aims to preprocess the all the data obtained from 2019 for G21 (polynomial values calculated from broadcast data station GRG). First, preprocessing was done to consider outliers. This method replaced values below the 1st percentile with the 1st percentile value, and the values above the 99th percentile with the 99th percentile value. Then min-max scaling was applied. Note: broadcast clock bias and broadcast clock bias were jointly \"clipped\" within the 1st and 99th percentile because they are supposed to be the same category of variable therefore to have an accurate 1:1 scaling and outlier elimination, they were considered jointly. Clock corrections were scaled separately.\\\n",
    "**PhD Milestone:** #1: *Leverage deep learning models to GPS satellite clock bias corrections.*\\\n",
    "**Project:** This program is Step (1) in this PhD milestone. Obtaining the data is the first critical step.\\\n",
    "**References:**\\\n",
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b409f1b-31ab-4ebd-a5fd-0e680d57ce84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e603fcf4-44c0-4304-a376-c04a2b04409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057587fe",
   "metadata": {},
   "source": [
    "# Remove Duplicated Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560cd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data/2019/correction_data_2019_str_update.npz') # raw data\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = data['matching_epochs']\n",
    "final_clock_bias = data['matching_clock_bias']\n",
    "broadcast_clock_bias = data['matching_poly_values']\n",
    "correction_value = data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc368932",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(epochs))\n",
    "print(type(final_clock_bias))\n",
    "print(type(broadcast_clock_bias))\n",
    "print(type(correction_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochs[4300:4350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(epochs))\n",
    "print(len(final_clock_bias))\n",
    "print(len(broadcast_clock_bias))\n",
    "print(len(correction_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a set to track unique (epoch, correction_value) pairs\n",
    "seen_pairs = set()\n",
    "\n",
    "# initialize lists to store unique values\n",
    "unique_epochs = []\n",
    "unique_final_clock_bias = []\n",
    "unique_broadcast_clock_bias = []\n",
    "unique_correction_value = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(epochs)):\n",
    "    # create a pair of (epoch, correction_value)\n",
    "    pair = (epochs[i], correction_value[i])\n",
    "    \n",
    "    # check if this pair has already been seen\n",
    "    if pair not in seen_pairs:\n",
    "        # if not, add it to the unique lists and mark this pair as seen\n",
    "        unique_epochs.append(epochs[i])\n",
    "        unique_final_clock_bias.append(final_clock_bias[i])\n",
    "        unique_broadcast_clock_bias.append(broadcast_clock_bias[i])\n",
    "        unique_correction_value.append(correction_value[i])\n",
    "        \n",
    "        # Add the pair to the set\n",
    "        seen_pairs.add(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e248ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(seen_pairs)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to arrays\n",
    "unique_epochs = np.array(unique_epochs)\n",
    "unique_final_clock_bias = np.array(unique_final_clock_bias)\n",
    "unique_broadcast_clock_bias = np.array(unique_broadcast_clock_bias)\n",
    "unique_correction_value = np.array(unique_correction_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_epochs))\n",
    "print(len(unique_final_clock_bias))\n",
    "print(len(unique_broadcast_clock_bias))\n",
    "print(len(unique_correction_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5243da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_epochs[6000:6010])\n",
    "print(unique_final_clock_bias[6000:6010])\n",
    "print(unique_broadcast_clock_bias[6000:6010])\n",
    "print(unique_correction_value[6000:6010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2019.npz',\n",
    "         matching_epochs=unique_epochs,\n",
    "         matching_clock_bias=unique_final_clock_bias,\n",
    "         matching_poly_values=unique_broadcast_clock_bias,\n",
    "         correction_vals=unique_correction_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b6708",
   "metadata": {},
   "source": [
    "# Choose Range of Data (1/1 to 12/31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2019.npz') # data without duplicates (hence unique)\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = data['matching_epochs']\n",
    "final_clock_bias = data['matching_clock_bias']\n",
    "broadcast_clock_bias = data['matching_poly_values']\n",
    "correction_value = data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the start and end of date range\n",
    "start_date = datetime(2019, 1, 1, 0, 0, 0)\n",
    "end_date = datetime(2019, 12, 31, 11, 59, 30)\n",
    "\n",
    "# convert the unique_epochs to datetime objects\n",
    "epoch_datetime = [datetime.strptime(epoch, '%Y:%m:%d:%H:%M:%S') for epoch in epochs]\n",
    "\n",
    "# create boolean mask for date range\n",
    "date_range_mask = [(start_date <= dt <= end_date) for dt in epoch_datetime]\n",
    "\n",
    "# filter the arrays\n",
    "filtered_epochs = epochs[date_range_mask]\n",
    "filtered_final_clock_bias = final_clock_bias[date_range_mask]\n",
    "filtered_broadcast_clock_bias = broadcast_clock_bias[date_range_mask]\n",
    "filtered_correction_value = correction_value[date_range_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ade64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_epochs))\n",
    "print(len(filtered_final_clock_bias))\n",
    "print(len(filtered_broadcast_clock_bias))\n",
    "print(len(filtered_correction_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_epochs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392110ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cut data to a new npz file\n",
    "np.savez('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2019_jan1_dec31.npz', \n",
    "         matching_epochs=filtered_epochs, \n",
    "         matching_clock_bias=filtered_final_clock_bias, \n",
    "         matching_poly_values=filtered_broadcast_clock_bias, \n",
    "         correction_vals=filtered_correction_value)\n",
    "\n",
    "print(f\"Filtered data saved with {len(filtered_epochs)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d990e-3a3b-4757-a980-50e1d7eaf63b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc72d7-aabf-4db9-b949-50639cdc9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2018 = np.load('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2018_jan1_dec31.npz')\n",
    "data2018.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc8914-e801-4fb6-9764-882fc088ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs2018 = data2018['matching_epochs']\n",
    "final_bias2018 = data2018['matching_clock_bias']\n",
    "broadcast_bias2018 = data2018['matching_poly_values']\n",
    "correction2018 = data2018['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9dbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs2018[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7135dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019 = np.load('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2019_jan1_dec31.npz')\n",
    "data2019.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34989b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs2019 = data2019['matching_epochs']\n",
    "final_bias2019 = data2019['matching_clock_bias']\n",
    "broadcast_bias2019 = data2019['matching_poly_values']\n",
    "correction2019 = data2019['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs2019[:10]\n",
    "print(len(epochs2018))\n",
    "print(len(epochs2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365da172",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = np.concatenate((epochs2018,epochs2019), axis=0)\n",
    "print(total_epochs[:50])\n",
    "print(total_epochs[-1])\n",
    "print(min(total_epochs))\n",
    "print(max(total_epochs))\n",
    "print(len(total_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_final_bias = np.concatenate((final_bias2018,final_bias2019), axis=0)\n",
    "print(total_final_bias[:50])\n",
    "print(total_final_bias[-1])\n",
    "print(min(total_final_bias))\n",
    "print(max(total_final_bias))\n",
    "print(len(total_final_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1dbd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_broadcast_bias = np.concatenate((broadcast_bias2018,broadcast_bias2019), axis=0)\n",
    "print(total_broadcast_bias[:50])\n",
    "print(total_broadcast_bias[-1])\n",
    "print(min(total_broadcast_bias))\n",
    "print(max(total_broadcast_bias))\n",
    "print(len(total_broadcast_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correction = np.concatenate((correction2018,correction2019), axis=0)\n",
    "print(total_correction[:50])\n",
    "print(total_correction[-1])\n",
    "print(min(total_correction))\n",
    "print(max(total_correction))\n",
    "print(len(total_correction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322bbfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cut data to a new npz file\n",
    "np.savez('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2018_2019_jan1_dec31.npz', \n",
    "         matching_epochs=total_epochs, \n",
    "         matching_clock_bias=total_final_bias, \n",
    "         matching_poly_values=total_broadcast_bias, \n",
    "         correction_vals=total_correction)\n",
    "\n",
    "print(f\"Filtered data saved with {len(total_epochs)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611cb121-4ec4-4175-858f-9b322075c279",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Removing Outliers (Considering Only Data b/w 0.15th and 99.85th Percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb2291-2000-47fa-973e-3bc4848d04bd",
   "metadata": {},
   "source": [
    "The method described replaces values outside the 5th and 95th percentiles with the 5th or 95th percentile values, respectively. The length of data does not change; it's modifying the values within the original dataset.\n",
    "\n",
    "Clipping: Any values below the 5th percentile are set to the 5th percentile value, and any values above the 95th percentile are set to the 95th percentile value. This means that extreme values are capped but no data is added or removed; the dataset size remains the same.\n",
    "\n",
    "[Text partially provided from ChatGPT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2018_2019_jan1_dec31.npz') # raw data\n",
    "\n",
    "epochs = data['matching_epochs']\n",
    "final_clock_bias = data['matching_clock_bias']\n",
    "broadcast_clock_bias = data['matching_poly_values']\n",
    "correction_value = data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88073f58-3c43-4e4b-a492-e86d78c98a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_biases = np.column_stack((final_clock_bias, broadcast_clock_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415361b-66c9-451e-a6c8-b5740cf0773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles_stacked_bias_1st = np.percentile(stacked_biases, 0.15)\n",
    "percentiles_stacked_bias_99th = np.percentile(stacked_biases, 99.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b88c9-3c2e-4735-ab1f-0e1e35bdd614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(percentiles_stacked_bias_1st)\n",
    "print(percentiles_stacked_bias_99th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340cd17-d940-4a7b-97df-fda95e2cf9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles_correction_1st = np.percentile(correction_value, 0.15)\n",
    "percentiles_correction_99th = np.percentile(correction_value, 99.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf0400-bfe9-4ba3-a4fa-80445797e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(percentiles_correction_1st)\n",
    "print(percentiles_correction_99th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f072548-3c90-47ec-98e6-2aa75d8ff8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_stacked_bias = np.clip(stacked_biases, percentiles_stacked_bias_1st, percentiles_stacked_bias_99th)\n",
    "clipped_correction = np.clip(correction_value, percentiles_correction_1st, percentiles_correction_99th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd90b6a-8dce-4d56-8f95-10be459abfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_final_bias = clipped_stacked_bias[:, 0]\n",
    "clipped_broadcast_bias = clipped_stacked_bias[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7eeb3-ad17-47aa-b3f1-f4edadcb6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2018_2019_jan1_dec31_clipped_015_9985.npz',\n",
    "         matching_epochs=epochs,\n",
    "         matching_clock_bias=clipped_final_bias,\n",
    "         matching_poly_values=clipped_broadcast_bias,\n",
    "         correction_vals=clipped_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5c9f4-72ed-4378-9d68-b8b909dbd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Maximum Clipped Final Bias: {max(clipped_final_bias)}')\n",
    "print(f'Minimum Clipped Final Bias: {min(clipped_final_bias)}')\n",
    "print(f'Maximum Clipped Broadcast Bias: {max(clipped_broadcast_bias)}')\n",
    "print(f'Minimum Clipped Broadcast Bias: {min(clipped_broadcast_bias)}')\n",
    "print(f'Maximum Clipped Correction: {max(clipped_correction)}')\n",
    "print(f'Minimum Clipped Correction: {min(clipped_correction)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8699657-74f2-4d72-91d6-ca57ae0a80ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Maximum Final Bias: {max(final_clock_bias)}')\n",
    "print(f'Minimum Final Bias: {min(final_clock_bias)}')\n",
    "print(f'Maximum Broadcast Bias: {max(broadcast_clock_bias)}')\n",
    "print(f'Minimum Broadcast Bias: {min(broadcast_clock_bias)}')\n",
    "print(f'Maximum Correction: {max(correction_value)}')\n",
    "print(f'Minimum Correction: {min(correction_value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e88cc-7b83-4d4d-aa59-a46f15d14d66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Plotting Data After Outlier Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d78f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2018_2019_jan1_dec31_clipped_015_9985.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = data['matching_epochs']\n",
    "final_clock_bias = data['matching_clock_bias']\n",
    "broadcast_clock_bias = data['matching_poly_values']\n",
    "correction_value = data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000a3e2-e10d-4d69-ae68-f4f65f35f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_datetimes = [datetime.strptime(ts, '%Y:%m:%d:%H:%M:%S') for ts in epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e9cb4-f279-44a6-85f3-698cfec06a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in range(1, 13):\n",
    "    start_date = datetime(2018, month, 1)\n",
    "    if month == 12:\n",
    "        end_date = datetime(2019, 1, 1)\n",
    "    else:\n",
    "        end_date = datetime(2018, month + 1, 1)\n",
    "\n",
    "    monthly_indices = [i for i, epoch in enumerate(time_datetimes) if start_date <= epoch < end_date]\n",
    "    monthly_epochs = [time_datetimes[i] for i in monthly_indices]\n",
    "    monthly_clock_bias = [final_clock_bias[i] for i in monthly_indices]\n",
    "    monthly_poly_values = [broadcast_clock_bias[i] for i in monthly_indices]\n",
    "    monthly_correction_vals = [correction_value[i] for i in monthly_indices]\n",
    "\n",
    "    # Plot clock_bias and poly_values\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.scatter(monthly_epochs, monthly_clock_bias, label='Clock Bias (Station: GRG)')\n",
    "    plt.scatter(monthly_epochs, monthly_poly_values, label='Broadcast Polynomial Values', s=5)\n",
    "    plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "    plt.ylabel('Bias Values (s)')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=5))  # Set major ticks every hour\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.title(f'Clock Bias and Poly Values for G21 {start_date.strftime(\"%B %Y\")}')\n",
    "    plt.savefig(f'/Volumes/MARI/ssdl_gps/correction_data/2018_2019/plots_threesigma/clock_bias_poly_values_{month:02d}_2018.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot correction values\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.scatter(monthly_epochs, monthly_correction_vals, label='Clock Bias Correction (s)')\n",
    "    plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "    plt.ylabel('Bias Correction Values (s)')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=5))  # Set major ticks every hour\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.title(f'Clock Bias Correction Values for G21 {start_date.strftime(\"%B %Y\")}')\n",
    "    plt.savefig(f'/Volumes/MARI/ssdl_gps/correction_data/2018_2019/plots_threesigma/correction_values_{month:02d}_2018.png')\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd05581",
   "metadata": {},
   "source": [
    "# Remove Discontinuities and Trust Preceding Clock Bias and Broadcast Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/unique_correction_data_2018_2019_jan1_dec31_clipped_015_9985.npz')\n",
    "epochs = data['matching_epochs']\n",
    "final_clock_bias = data['matching_clock_bias']\n",
    "broadcast_clock_bias = data['matching_poly_values']\n",
    "correction_value = data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_datetime = [datetime.strptime(epoch, '%Y:%m:%d:%H:%M:%S') for epoch in epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946af2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the time_datetimes to check the time differences to see where interval is not 30 seconds\n",
    "for i in range(len(epoch_datetime) - 1):\n",
    "    time_difference = (epoch_datetime[i + 1] - epoch_datetime[i]).total_seconds()\n",
    "    if time_difference not in [0, 30]:\n",
    "        print(f\"Time where delta_time is not 0 or 30 seconds: B/w {epoch_datetime[i]} & {epoch_datetime[i+1]}, Idx: {i}, {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90538ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_epoch_datetime = []\n",
    "new_final_clock_bias = []\n",
    "new_broadcast_clock_bias = []\n",
    "new_correction_value = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert missing 30-second intervals\n",
    "for i in range(len(epoch_datetime) - 1):\n",
    "    new_epoch_datetime.append(epoch_datetime[i])\n",
    "    new_final_clock_bias.append(final_clock_bias[i]) \n",
    "    new_broadcast_clock_bias.append(broadcast_clock_bias[i])\n",
    "    new_correction_value.append(correction_value[i])\n",
    "    \n",
    "    time_difference = (epoch_datetime[i + 1] - epoch_datetime[i]).total_seconds()\n",
    "    \n",
    "    while time_difference > 30:\n",
    "        epoch_datetime[i] += timedelta(seconds=30)\n",
    "        new_epoch_datetime.append(epoch_datetime[i])\n",
    "        new_final_clock_bias.append(final_clock_bias[i]) # keeping previous broadcast info for these discontinuities\n",
    "        new_broadcast_clock_bias.append(broadcast_clock_bias[i]) # trusting broadcast info for these discontinuities\n",
    "        new_correction_value.append(0)\n",
    "        time_difference -= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the last elements of the original data\n",
    "new_epoch_datetime.append(epoch_datetime[-1])\n",
    "new_final_clock_bias.append(final_clock_bias[-1])\n",
    "new_broadcast_clock_bias.append(broadcast_clock_bias[-1])\n",
    "new_correction_value.append(correction_value[-1])\n",
    "\n",
    "# Convert the new epochs back to strings\n",
    "matching_epoch_strings = [epoch.strftime('%Y:%m:%d:%H:%M:%S') for epoch in new_epoch_datetime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455776ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_epoch_strings = np.array(matching_epoch_strings)\n",
    "new_final_clock_bias = np.array(new_final_clock_bias)\n",
    "new_broadcast_clock_bias = np.array(new_broadcast_clock_bias)\n",
    "new_correction_value = np.array(new_correction_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d326fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(matching_epoch_strings))\n",
    "print(len(new_epoch_datetime))\n",
    "print(min(new_epoch_datetime))\n",
    "print(max(new_epoch_datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_epoch_datetime[2159:2400]\n",
    "new_epoch_datetime[5039:5280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf118e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new data to an npz file\n",
    "np.savez('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/continuous_unique_correction_data_2018_2019_jan1_dec31_clipped_015_9985.npz', \n",
    "         matching_epochs=matching_epoch_strings, \n",
    "         matching_clock_bias=new_final_clock_bias, \n",
    "         matching_poly_values=new_broadcast_clock_bias, \n",
    "         correction_vals=new_correction_value)\n",
    "\n",
    "print(\"New dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920baab2-31ea-431a-bcaa-847c4e4b7eb5",
   "metadata": {},
   "source": [
    "# Min-Max Scaling and Plot Scaled Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0cb6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/continuous_unique_correction_data_2018_2019_jan1_dec31_clipped_015_9985.npz')\n",
    "epochs = data['matching_epochs']\n",
    "final_clock_bias = data['matching_clock_bias']\n",
    "broadcast_clock_bias = data['matching_poly_values']\n",
    "correction_value = data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fcbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_datetimes = [datetime.strptime(ts, '%Y:%m:%d:%H:%M:%S') for ts in epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6712d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_biases = np.column_stack((final_clock_bias, broadcast_clock_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b4d7c4-ecf5-458b-8a8f-4e3a9a3ffc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26830159-e772-45be-9222-a98716c9a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_biases = scaler.fit_transform(stacked_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc020807-7d04-4042-b499-35dfa6aff726",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_final_bias = scaled_biases[:, 0]\n",
    "scaled_broadcast_bias = scaled_biases[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "309ffa04-122e-4504-8806-602e5e2f60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a6ca727-287a-475c-9434-9764ec6153fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_correction = correction_scaler.fit_transform(correction_value.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9344f9-ceae-4330-9ad4-8620af73a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/scaled_continuous_unique_correction_data_2018_2019_jan1_dec31_clipped_015_9985.npz',\n",
    "         matching_epochs=epochs,\n",
    "         matching_clock_bias=scaled_final_bias,\n",
    "         matching_poly_values=scaled_broadcast_bias,\n",
    "         correction_vals=scaled_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "420468ab-6e09-47ed-86ea-9aa43aaff872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Scaled Final Bias: 1.0000000000000002\n",
      "Minimum Scaled Final Bias: 0.0\n",
      "Maximum Scaled Broadcast Bias: 1.0000000000000002\n",
      "Minimum Scaled Broadcast Bias: 0.0\n",
      "Maximum Scaled Correction: 0.9999999999999999\n",
      "Minimum Scaled Correction: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Maximum Scaled Final Bias: {max(scaled_final_bias)}')\n",
    "print(f'Minimum Scaled Final Bias: {min(scaled_final_bias)}')\n",
    "print(f'Maximum Scaled Broadcast Bias: {max(scaled_broadcast_bias)}')\n",
    "print(f'Minimum Scaled Broadcast Bias: {min(scaled_broadcast_bias)}')\n",
    "print(f'Maximum Scaled Correction: {max(scaled_correction)}')\n",
    "print(f'Minimum Scaled Correction: {min(scaled_correction)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f4aae58-acdf-4d42-90d9-0c6fb7e6cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in range(1, 13):\n",
    "    start_date = datetime(2019, month, 1)\n",
    "    if month == 12:\n",
    "        end_date = datetime(2020, 1, 1)\n",
    "    else:\n",
    "        end_date = datetime(2019, month + 1, 1)\n",
    "\n",
    "    monthly_indices = [i for i, epoch in enumerate(time_datetimes) if start_date <= epoch < end_date]\n",
    "    monthly_epochs = [time_datetimes[i] for i in monthly_indices]\n",
    "    monthly_clock_bias = [scaled_final_bias[i] for i in monthly_indices]\n",
    "    monthly_poly_values = [scaled_broadcast_bias[i] for i in monthly_indices]\n",
    "    monthly_correction_vals = [scaled_correction[i] for i in monthly_indices]\n",
    "\n",
    "    # Plot clock_bias and poly_values\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.scatter(monthly_epochs, monthly_clock_bias, label='Clock Bias (Station: GRG)')\n",
    "    plt.scatter(monthly_epochs, monthly_poly_values, label='Broadcast Polynomial Values', s=5)\n",
    "    plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "    plt.ylabel('Bias Values (s)')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=5))  # Set major ticks every hour\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.title(f'Clock Bias and Poly Values for G21 {start_date.strftime(\"%B %Y\")}')\n",
    "    plt.savefig(f'/Volumes/MARI/ssdl_gps/correction_data/2018_2019/plots_threesigma_scaled/clock_bias_poly_values_{month:02d}_2019.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot correction values\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.scatter(monthly_epochs, monthly_correction_vals, label='Clock Bias Correction (s)')\n",
    "    plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "    plt.ylabel('Bias Correction Values (s)')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=5))  # Set major ticks every hour\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.title(f'Clock Bias Correction Values for G21 {start_date.strftime(\"%B %Y\")}')\n",
    "    plt.savefig(f'/Volumes/MARI/ssdl_gps/correction_data/2018_2019/plots_threesigma_scaled/correction_values_{month:02d}_2019.png')\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a200f23-d1ac-4880-9548-231cb63efc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f9134a-a31c-4e84-9652-3efdac1a2267",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Archive Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d6eb4-65f5-418d-ac6b-7bc7fecf6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import math\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import TensorDataset\n",
    "# from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc5c2d-890d-45b6-bd94-319cc1224697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from torch.utils.data import TensorDataset\n",
    "\n",
    "# # Load data\n",
    "# data = np.load('/Volumes/MARI/ssdl_gps/correction_data_2019_mo1.npz')\n",
    "\n",
    "# matching_poly_values = data['matching_poly_values']\n",
    "# correction_vals = data['correction_vals']\n",
    "\n",
    "# # Ensure data is in the correct shape [num_samples, num_features]\n",
    "# # Assuming each sample has one feature (i.e., 1D data)\n",
    "# matching_poly_values = matching_poly_values[:, np.newaxis]  # Shape: [num_samples, 1]\n",
    "# correction_vals = correction_vals[:, np.newaxis]  # Shape: [num_samples, 1]\n",
    "\n",
    "# # Apply min-max scaling\n",
    "# scaler_poly = MinMaxScaler()\n",
    "# scaler_correction = MinMaxScaler()\n",
    "\n",
    "# matching_poly_values_scaled = scaler_poly.fit_transform(matching_poly_values)\n",
    "# correction_vals_scaled = scaler_correction.fit_transform(correction_vals)\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# train_size = int(0.7 * len(matching_poly_values_scaled))\n",
    "# test_size = len(matching_poly_values_scaled) - train_size\n",
    "\n",
    "# train_poly_values = matching_poly_values_scaled[:train_size]\n",
    "# train_correction_vals = correction_vals_scaled[:train_size]\n",
    "\n",
    "# test_poly_values = matching_poly_values_scaled[train_size:]\n",
    "# test_correction_vals = correction_vals_scaled[train_size:]\n",
    "\n",
    "# # Convert to torch tensors\n",
    "# train_poly_values = torch.from_numpy(train_poly_values).float()\n",
    "# train_correction_vals = torch.from_numpy(train_correction_vals).float()\n",
    "# test_poly_values = torch.from_numpy(test_poly_values).float()\n",
    "# test_correction_vals = torch.from_numpy(test_correction_vals).float()\n",
    "\n",
    "# # Create datasets\n",
    "# train_dataset = TensorDataset(train_poly_values, train_correction_vals)\n",
    "# test_dataset = TensorDataset(test_poly_values, test_correction_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83471a7-85e5-489a-98bf-e4e66a279982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(3)\n",
    "\n",
    "# def generate_data(data_path: Path, num_steps: int = 1000000, interval: float = 0.1) -> None:\n",
    "#     \"\"\"\n",
    "#     Generate synthetic data and save it to a specified file.\n",
    "\n",
    "#     Parameters ----------------------------------------------------------------\n",
    "#     data_path : Path\n",
    "#         The path where the generated data will be saved.\n",
    "#     num_steps : int, optional\n",
    "#         The number of data points to generate.\n",
    "#     interval : float, optional\n",
    "#         The spacing between data points in the x-axis (default is 0.1).\n",
    "\n",
    "#     Returns --------------------------------------------------------------------\n",
    "#     -------\n",
    "#     None\n",
    "#         The function saves the generated data to a file at `data_path`.\n",
    "#     \"\"\"\n",
    "#     x = np.linspace(0, num_steps * interval, num_steps)\n",
    "#     y = np.sin(x) + np.random.normal(0, 0.1, x.shape)\n",
    "\n",
    "#     np.savez(data_path, y=y)\n",
    "\n",
    "#     return num_steps, interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e44a42-1a36-4c07-8b7d-9b0989ede134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sine = Path(\"sine_data.npz\")\n",
    "# sawtooth = Path(\"saw_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0d653-18d4-4f1e-b553-ffb738c37512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "\n",
    "# def generate_data(data_path: Path, num_steps: int = 1000000, interval: float = 0.1, waveform_type: str = 'sine') -> None:\n",
    "#     \"\"\"\n",
    "#     Generate synthetic data (sine or noisy sawtooth wave) and save it to a specified file.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data_path : Path\n",
    "#         The path where the generated data will be saved.\n",
    "#     num_steps : int, optional\n",
    "#         The number of data points to generate.\n",
    "#     interval : float, optional\n",
    "#         The spacing between data points in the x-axis (default is 0.1).\n",
    "#     waveform_type : str, optional\n",
    "#         Type of waveform to generate ('sine' or 'sawtooth'). Default is 'sine'.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     None\n",
    "#         The function saves the generated data to a file at `data_path`.\n",
    "#     \"\"\"\n",
    "#     if waveform_type == 'sine':\n",
    "#         x = np.linspace(0, num_steps * interval, num_steps)\n",
    "#         y = np.sin(x) + np.random.normal(0, 0.1, x.shape)\n",
    "#     elif waveform_type == 'sawtooth':\n",
    "#         sampling_rate = 4\n",
    "#         duration = 100000\n",
    "#         x = np.linspace(0.0, duration, int(duration * sampling_rate), endpoint=False)\n",
    "#         amplitude = 0.5  # Amplitude of the sawtooth wave\n",
    "#         noise_amplitude = 0.1  # Amplitude of the noise\n",
    "#         frequency = .04  # Frequency of the sawtooth wave in Hz\n",
    "\n",
    "#         # Sawtooth wave\n",
    "#         sawtooth_wave = amplitude * (2 * (frequency * x - np.floor(frequency * x + 0.5)))\n",
    "#         noise = noise_amplitude * np.random.randn(len(x))\n",
    "#         y = sawtooth_wave + noise\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported waveform_type. Choose 'sine' or 'sawtooth'.\")\n",
    "\n",
    "#     np.savez(data_path, y=y)\n",
    "\n",
    "#     return x, num_steps, interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6823ab9-0c95-40c9-8ad2-908e648641ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_sine, num_steps, interval) = generate_data(sine, num_steps=1000000, interval=0.1, waveform_type='sine')\n",
    "# (x_saw, num_steps, interval) = generate_data(sawtooth, num_steps=1000000, interval=0.1, waveform_type='sawtooth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5c8db-2ba9-4b86-9d6e-00de12f7f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x_sine, np.load('sine_data.npz')['y'], color='r', label='Noisy Sine Wave')\n",
    "# plt.grid(True)\n",
    "# plt.xlim(0,100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c7bbd-a544-4a44-9171-9b8916ba997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x_saw, np.load('saw_data.npz')['y'], color='r', label='Noisy Sawtooth Wave')\n",
    "# plt.grid(True)\n",
    "# plt.xlim(0,100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531b5e1-4d46-4a6f-9394-b34ca99a6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_sequence(\n",
    "#     sequence: np.ndarray, ratio: float = 0.8\n",
    "# ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#     \"\"\"Splits a sequence into 2 (3) parts, as is required by our transformer\n",
    "#     model.\n",
    "\n",
    "#     Assume our sequence length is L, we then split this into src of length N\n",
    "#     and tgt_y of length M, with N + M = L.\n",
    "#     src, the first part of the input sequence, is the input to the encoder, and we\n",
    "#     expect the decoder to predict tgt_y, the second part of the input sequence.\n",
    "#     In addition we generate tgt, which is tgt_y but \"shifted left\" by one - i.e. it\n",
    "#     starts with the last token of src, and ends with the second-last token in tgt_y.\n",
    "#     This sequence will be the input to the decoder.\n",
    "\n",
    "\n",
    "#     Args:\n",
    "\n",
    "#         sequence: batched input sequences to split [bs, seq_len, num_features]\n",
    "#         ratio: split ratio, N = ratio * L\n",
    "\n",
    "#     Returns:\n",
    "#         tuple[torch.Tensor, torch.Tensor, torch.Tensor]: src, tgt, tgt_y\n",
    "#     \"\"\"\n",
    "#     src_end = int(sequence.shape[1] * ratio)\n",
    "#     # [bs, src_seq_len, num_features]\n",
    "#     src = sequence[:, :src_end]\n",
    "#     # [bs, tgt_seq_len, num_features]\n",
    "#     tgt = sequence[:, src_end - 1 : -1]\n",
    "#     # [bs, tgt_seq_len, num_features]\n",
    "#     tgt_y = sequence[:, src_end:]\n",
    "\n",
    "#     return src, tgt, tgt_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe79fbd-3e4e-4a6d-b350-ccfbd1e13b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html,\n",
    "# # only modified to account for \"batch first\"\n",
    "\n",
    "# class PositionalEncoding(torch.nn.Module):\n",
    "#     def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "#       \"\"\"\n",
    "#       Args:\n",
    "#         d_model (int): Dimension of the model (embedding dimension)\n",
    "#         dropout (float, optional): Dropout probability. Default is 0.1.\n",
    "#         max_len (int, optional): Maximum length of input sequences. Default is 5000.\n",
    "\n",
    "#       Attributes:\n",
    "#         pe (torch.Tensor): Positional encoding tensor. Shape: (1, max_len, d_model)\n",
    "\n",
    "#       Returns:\n",
    "#         torch.Tensor: input tensor with added positional encoding.\n",
    "\n",
    "#       \"\"\"\n",
    "#       super().__init__()\n",
    "#       self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "#       position = torch.arange(max_len).unsqueeze(1)                           # 1-D tensor from 0 to max_len -1. Unsqueeze \"adds\" a superficial 1 dim.\n",
    "#       div_term = torch.exp(\n",
    "#           torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "#       )\n",
    "#       pe = torch.zeros(1, max_len, d_model)\n",
    "#       pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "#       pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "#       self.register_buffer(\"pe\", pe)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#       \"\"\"Adds positional encoding to the given tensor.\n",
    "\n",
    "#       Args:\n",
    "#           x: tensor to add PE to [bs, seq_len, embed_dim]\n",
    "\n",
    "#       Returns:\n",
    "#           torch.Tensor: tensor with PE [bs, seq_len, embed_dim]\n",
    "#       \"\"\"\n",
    "#       x = x + self.pe[:, : x.size(1)]\n",
    "#       return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067ee42-24dd-44c2-8c27-db191bcf95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerWithPE(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self, in_dim: int, out_dim: int, embed_dim: int, num_heads: int, num_layers: int\n",
    "#     ) -> None:\n",
    "#         \"\"\"\n",
    "#         Initializes a transformer model with positional encoding.\n",
    "\n",
    "#         Args:\n",
    "#             in_dim: number of input features\n",
    "#             out_dim: number of features to predict\n",
    "#             embed_dim: embed features to this dimension\n",
    "#             num_heads: number of transformer heads\n",
    "#             num_layers: number of encoder and decoder layers\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.positional_encoding = PositionalEncoding(embed_dim)\n",
    "\n",
    "#         # transform input features into embedded features\n",
    "#         self.encoder_embedding = torch.nn.Linear(\n",
    "#             in_features=in_dim, out_features=embed_dim\n",
    "#         )\n",
    "#         self.decoder_embedding = torch.nn.Linear(\n",
    "#             in_features=out_dim, out_features=embed_dim\n",
    "#         )\n",
    "\n",
    "#         # map output into output dimension\n",
    "#         self.output_layer = torch.nn.Linear(in_features=embed_dim, out_features=out_dim)\n",
    "\n",
    "\n",
    "#         self.transformer = torch.nn.Transformer(\n",
    "#             nhead=num_heads,\n",
    "#             num_encoder_layers=num_layers,\n",
    "#             num_decoder_layers=num_layers,\n",
    "#             d_model=embed_dim,\n",
    "#             batch_first=True,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"Forward function of the model.\n",
    "\n",
    "#         Args:\n",
    "#             src: input sequence to the encoder [bs, src_seq_len, num_features]\n",
    "#             tgt: input sequence to the decoder [bs, tgt_seq_len, num_features]\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: predicted sequence [bs, tgt_seq_len, feat_dim]\n",
    "#         \"\"\"\n",
    "#         # if self.train:\n",
    "#         # Add noise to decoder inputs during training\n",
    "#         # tgt = tgt + torch.normal(0, 0.1, size=tgt.shape).to(tgt.device)\n",
    "\n",
    "#         # Embed encoder input and add positional encoding.\n",
    "#         # [bs, src_seq_len, embed_dim]\n",
    "#         src = self.encoder_embedding(src)\n",
    "#         src = self.positional_encoding(src)\n",
    "\n",
    "#         # Generate mask to avoid attention to future outputs.\n",
    "#         # [tgt_seq_len, tgt_seq_len]\n",
    "#         tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(tgt.shape[1])\n",
    "#         # Embed decoder input and add positional encoding.\n",
    "#         # [bs, tgt_seq_len, embed_dim]\n",
    "#         tgt = self.decoder_embedding(tgt)\n",
    "#         tgt = self.positional_encoding(tgt)\n",
    "\n",
    "#         # Get prediction from transformer and map to output dimension.\n",
    "#         # [bs, tgt_seq_len, embed_dim]\n",
    "#         pred = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "#         pred = self.output_layer(pred)\n",
    "\n",
    "#         return pred                                                             # return predicted sequence\n",
    "\n",
    "\n",
    "#     def infer(self, src: torch.Tensor, tgt_len: int) -> torch.Tensor:\n",
    "#         \"\"\"Runs inference with the model, meaning: predicts future values\n",
    "#         for an unknown sequence.\n",
    "#         For this, iteratively generate the next output token while\n",
    "#         feeding the already generated ones as input sequence to the decoder.\n",
    "\n",
    "#         Args:\n",
    "#             src: input to the encoder [bs, src_seq_len, num_features]\n",
    "#             tgt_len: desired length of the output\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: inferred sequence\n",
    "#         \"\"\"\n",
    "#         output = torch.zeros((src.shape[0], tgt_len + 1, src.shape[2])).to(src.device)\n",
    "#         output[:, 0] = src[:, -1]\n",
    "#         for i in range(tgt_len):\n",
    "#             output[:, i + 1] = self.forward(src, output)[:, i]\n",
    "\n",
    "#         return output[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55898f03-8f68-4828-be9d-ffe065f95960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_partition_data(\n",
    "#     data_path: Path, seq_length: int = 100\n",
    "# ) -> tuple[np.ndarray, int]:\n",
    "#     \"\"\"Loads the given data and paritions it into sequences of equal length.\n",
    "\n",
    "#     Args:\n",
    "#         data_path: path to the dataset\n",
    "#         sequence_length: length of the generated sequences\n",
    "\n",
    "#     Returns:\n",
    "#         tuple[np.ndarray, int]: tuple of generated sequences and number of\n",
    "#             features in dataset\n",
    "#     \"\"\"\n",
    "#     data = np.load(data_path)\n",
    "#     num_features = len(data.keys())\n",
    "\n",
    "#     # Check that each feature provides the same number of data points\n",
    "#     data_lens = [len(data[key]) for key in data.keys()]\n",
    "#     assert len(set(data_lens)) == 1\n",
    "\n",
    "#     num_sequences = data_lens[0] // seq_length\n",
    "#     sequences = np.empty((num_sequences, seq_length, num_features))\n",
    "\n",
    "#     for i in range(0, num_sequences):\n",
    "#         # [sequence_length, num_features]\n",
    "#         sample = np.asarray(\n",
    "#             [data[key][i * seq_length : (i + 1) * seq_length] for key in data.keys()]\n",
    "#         ).swapaxes(0, 1)\n",
    "#         sequences[i] = sample\n",
    "\n",
    "#     return sequences, num_features\n",
    "\n",
    "\n",
    "# def make_datasets(sequences: np.ndarray) -> tuple[TensorDataset, TensorDataset]:\n",
    "#     \"\"\"Create train and test dataset.\n",
    "\n",
    "#     Args:\n",
    "#         sequences: sequences to use [num_sequences, sequence_length, num_features]\n",
    "\n",
    "#     Returns:\n",
    "#         tuple[TensorDataset, TensorDataset]: train and test dataset\n",
    "#     \"\"\"\n",
    "#     # Split sequences into train and test split\n",
    "#     train, test = train_test_split(sequences, test_size=0.2)\n",
    "#     return TensorDataset(torch.Tensor(train)), TensorDataset(torch.Tensor(test))\n",
    "\n",
    "\n",
    "# def visualize(\n",
    "#     src: torch.Tensor,\n",
    "#     tgt: torch.Tensor,\n",
    "#     pred: torch.Tensor,\n",
    "#     pred_infer: torch.Tensor,\n",
    "#     idx=0,\n",
    "# ) -> None:\n",
    "#     \"\"\"Visualizes a given sample including predictions.\n",
    "\n",
    "#     Args:\n",
    "#         src: source sequence [bs, src_seq_len, num_features]\n",
    "#         tgt: target sequence [bs, tgt_seq_len, num_features]\n",
    "#         pred: prediction of the model [bs, tgt_seq_len, num_features]\n",
    "#         pred_infer: prediction obtained by running inference\n",
    "#             [bs, tgt_seq_len, num_features]\n",
    "#         idx: batch index to visualize\n",
    "#     \"\"\"\n",
    "#     x = np.arange(src.shape[1] + tgt.shape[1])\n",
    "#     src_len = src.shape[1]\n",
    "\n",
    "#     plt.plot(x[:src_len], src[idx].cpu().detach(), \"bo-\", label=\"src\")\n",
    "#     plt.plot(x[src_len:], tgt[idx].cpu().detach(), \"go-\", label=\"tgt\")\n",
    "#     plt.plot(x[src_len:], pred[idx].cpu().detach(), \"ro-\", label=\"pred\")\n",
    "#     plt.plot(x[src_len:], pred_infer[idx].cpu().detach(), \"yo-\", label=\"pred_infer\")\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "#     plt.clf()\n",
    "\n",
    "\n",
    "# def split_sequence(\n",
    "#     sequence: np.ndarray, ratio: float = 0.8\n",
    "# ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#     \"\"\"Splits a sequence into 2 (3) parts, as is required by our transformer\n",
    "#     model.\n",
    "\n",
    "#     Assume our sequence length is L, we then split this into src of length N\n",
    "#     and tgt_y of length M, with N + M = L.\n",
    "#     src, the first part of the input sequence, is the input to the encoder, and we\n",
    "#     expect the decoder to predict tgt_y, the second part of the input sequence.\n",
    "#     In addition we generate tgt, which is tgt_y but \"shifted left\" by one - i.e. it\n",
    "#     starts with the last token of src, and ends with the second-last token in tgt_y.\n",
    "#     This sequence will be the input to the decoder.\n",
    "\n",
    "\n",
    "#     Args:\n",
    "#         sequence: batched input sequences to split [bs, seq_len, num_features]\n",
    "#         ratio: split ratio, N = ratio * L\n",
    "\n",
    "#     Returns:\n",
    "#         tuple[torch.Tensor, torch.Tensor, torch.Tensor]: src, tgt, tgt_y\n",
    "#     \"\"\"\n",
    "#     src_end = int(sequence.shape[1] * ratio)\n",
    "#     # [bs, src_seq_len, num_features]\n",
    "#     src = sequence[:, :src_end]\n",
    "#     # [bs, tgt_seq_len, num_features]\n",
    "#     tgt = sequence[:, src_end - 1 : -1]\n",
    "#     # [bs, tgt_seq_len, num_features]\n",
    "#     tgt_y = sequence[:, src_end:]\n",
    "\n",
    "#     return src, tgt, tgt_y\n",
    "\n",
    "\n",
    "# def move_to_device(device: torch.Tensor, *tensors: torch.Tensor) -> list[torch.Tensor]:\n",
    "#     \"\"\"Move all given tensors to the given device.\n",
    "\n",
    "#     Args:\n",
    "#         device: device to move tensors to\n",
    "#         tensors: tensors to move\n",
    "\n",
    "#     Returns:\n",
    "#         list[torch.Tensor]: moved tensors\n",
    "#     \"\"\"\n",
    "#     moved_tensors = []\n",
    "#     for tensor in tensors:\n",
    "#         if isinstance(tensor, torch.Tensor):\n",
    "#             moved_tensors.append(tensor.to(device))\n",
    "#         else:\n",
    "#             moved_tensors.append(tensor)\n",
    "#     return moved_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7ee2a-732d-4ebd-9e30-814020ea7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS = 100                                                                        # batch size\n",
    "# FEATURE_DIM = 128                                                               # dimensionality of input features\n",
    "# NUM_HEADS = 8                                                                   # number of attention heads in the multi-head attention mechanism\n",
    "# NUM_EPOCHS = 10                                                                 # number of times entire dataset is passed for training\n",
    "# NUM_VIS_EXAMPLES = 1\n",
    "# NUM_LAYERS = 2                                                                  # number of encoder and decoder layers in the mdel\n",
    "# LR = 0.001  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900495bb-d790-4184-a0aa-4303564137ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data and generate train and test datasets / dataloaders\n",
    "# sequences, num_features = load_and_partition_data(\"test_correction_data_2019_mo1.npz\")               # change data file name\n",
    "# train_set, test_set = make_datasets(sequences)\n",
    "# train_loader, test_loader = DataLoader(\n",
    "#     train_set, batch_size=BS, shuffle=True\n",
    "# ), DataLoader(test_set, batch_size=BS, shuffle=False)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d63081-df7c-4726-bb97-33dee9012d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize model, optimizer and loss criterion\n",
    "# model = TransformerWithPE(\n",
    "#     num_features, num_features, FEATURE_DIM, NUM_HEADS, NUM_LAYERS\n",
    "# ).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb645fb5-d0af-4a48-8d13-7318d7df8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Train loop\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     epoch_loss = 0.0                                                            # initialize epoch loss\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         src, tgt, tgt_y = split_sequence(batch[0])\n",
    "#         src, tgt, tgt_y = move_to_device(device, src, tgt, tgt_y)\n",
    "#         # [bs, tgt_seq_len, num_features]\n",
    "#         pred = model(src, tgt)\n",
    "#         loss = criterion(pred, tgt_y)\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(\n",
    "#         f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: \"\n",
    "#         f\"{(epoch_loss / len(train_loader)):.4f}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d8c78-f793-495d-b699-83d2199fea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Evaluate model\n",
    "# model.eval()\n",
    "# eval_loss = 0.0\n",
    "# infer_loss = 0.0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for idx, batch in enumerate(test_loader):\n",
    "#         src, tgt, tgt_y = split_sequence(batch[0])\n",
    "#         src, tgt, tgt_y = move_to_device(device, src, tgt, tgt_y)\n",
    "\n",
    "#         # [bs, tgt_seq_len, num_features]\n",
    "#         pred = model(src, tgt)\n",
    "#         loss = criterion(pred, tgt_y)\n",
    "#         eval_loss += loss.item()\n",
    "\n",
    "#         # Run inference with model\n",
    "#         pred_infer = model.infer(src, tgt.shape[1])\n",
    "#         loss_infer = criterion(pred_infer, tgt_y)\n",
    "#         infer_loss += loss_infer.item()\n",
    "\n",
    "#         if idx < NUM_VIS_EXAMPLES:\n",
    "#             visualize(src, tgt, pred, pred_infer)\n",
    "\n",
    "# avg_eval_loss = eval_loss / len(test_loader)\n",
    "# avg_infer_loss = infer_loss / len(test_loader)\n",
    "\n",
    "# print(f\"Eval Loss on test set: {avg_eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae2fa9-eab9-4061-a14c-7602b0577cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b652c9d-af74-4f74-9a6d-9666e918917b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
