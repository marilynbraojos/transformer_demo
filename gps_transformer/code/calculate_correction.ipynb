{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba6b525",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/marilynbraojos/transformer_demo/blob/main/gps2309.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7b3a1-bc7b-4b96-9832-8fbea6618467",
   "metadata": {},
   "source": [
    "# calculate_correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9838fe-7a7e-48a5-b115-c75eec42715e",
   "metadata": {},
   "source": [
    "**Author:** Marilyn Braojos Gutierrez\\\n",
    "**Purpose:** This program aims to calculate the correction values between broadcast clock bias and final clock bias.\\\n",
    "**PhD Milestone:** #1: *Leverage deep learning models to GPS satellite clock bias corrections.*\\\n",
    "**Project:** This program is Step (1) in this PhD milestone. Obtaining the data is the first critical step.\\\n",
    "**References:**\\\n",
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae416ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8042dd92-c795-4295-b7d2-4fa6fd7a2e15",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76885664",
   "metadata": {
    "id": "76885664"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612f7f7-3173-4e4c-aac7-404d3eb05f43",
   "metadata": {},
   "source": [
    "# Calculate Correction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62004a1c-b662-471c-8fef-3acc4d214a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load broadcast clock bias polynomial data npz without relativistic effects considered in the polynomial for G21 satellite only\n",
    "\n",
    "def load_broadcast_file(day):\n",
    "    filename = f'/Volumes/MARI/ssdl_gps/rnx_polynomial/gps_poly_2018{day:03d}_G21.npz'\n",
    "    data = np.load(filename)\n",
    "    time_strings = data['time_strings']\n",
    "    poly_values = data['poly_values']\n",
    "    time_datetimes = [datetime.strptime(ts, '%Y:%m:%d:%H:%M:%S') for ts in time_strings]\n",
    "    return time_datetimes, poly_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0a41b-c391-4e4b-ba21-e61ddf62fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load broadcast clock bias polynomial data npz with relativistic effects considered in the polynomial for G21 satellite only \n",
    "\n",
    "# def load_broadcast_file_w_rel(day):\n",
    "#     filename = f'/Volumes/MARI/ssdl_gps/rnx_polynomial/gps_poly_rel_2019{day:03d}_G21.npz'\n",
    "#     data = np.load(filename)\n",
    "#     time_strings = data['time_strings']\n",
    "#     poly_values = data['poly_values']\n",
    "#     time_datetimes = [datetime.strptime(ts, '%Y:%m:%d:%H:%M:%S') for ts in time_strings]\n",
    "#     return time_datetimes, poly_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc7c0cf-3f4f-40bc-a860-6b9619702c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load final clock bias data npz for G21 satellite only and GRG station only\n",
    "\n",
    "def load_clk_file(week):\n",
    "    filename = f'/Volumes/MARI/ssdl_gps/clk_npz_sat/G21/grg_gps_clk_{week}_G21.npz'\n",
    "    data = np.load(filename)\n",
    "    epochs = [datetime(year, month, day, hour, minute, second)\n",
    "              for year, month, day, hour, minute, second \n",
    "              in zip(data['yyyy'], data['mm'], data['dd'], data['hh'], data['mi'], data['ss'])]\n",
    "    clock_bias = data['clock_bias_vals']\n",
    "    return epochs, clock_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d6eac93-cd1c-4744-b56a-249da2e737b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize arrays\n",
    "\n",
    "all_matching_epochs = []\n",
    "all_matching_clock_bias = []\n",
    "all_matching_poly_values = []\n",
    "all_correction_vals = []\n",
    "\n",
    "all_time_datetimes = []\n",
    "all_poly_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb15b174-d2bb-427e-9596-1a7d55f5a92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished storing all epochs and corresponding polynomial values for day 1\n",
      "Finished storing all epochs and corresponding polynomial values for day 2\n",
      "Finished storing all epochs and corresponding polynomial values for day 3\n",
      "Finished storing all epochs and corresponding polynomial values for day 4\n",
      "Finished storing all epochs and corresponding polynomial values for day 5\n",
      "Finished storing all epochs and corresponding polynomial values for day 6\n",
      "Finished storing all epochs and corresponding polynomial values for day 7\n",
      "Finished storing all epochs and corresponding polynomial values for day 8\n",
      "Finished storing all epochs and corresponding polynomial values for day 9\n",
      "Finished storing all epochs and corresponding polynomial values for day 10\n",
      "Finished storing all epochs and corresponding polynomial values for day 11\n",
      "Finished storing all epochs and corresponding polynomial values for day 12\n",
      "Finished storing all epochs and corresponding polynomial values for day 13\n",
      "Finished storing all epochs and corresponding polynomial values for day 14\n",
      "Finished storing all epochs and corresponding polynomial values for day 15\n",
      "Finished storing all epochs and corresponding polynomial values for day 16\n",
      "Finished storing all epochs and corresponding polynomial values for day 17\n",
      "Finished storing all epochs and corresponding polynomial values for day 18\n",
      "Finished storing all epochs and corresponding polynomial values for day 19\n",
      "Finished storing all epochs and corresponding polynomial values for day 20\n",
      "Finished storing all epochs and corresponding polynomial values for day 21\n",
      "Finished storing all epochs and corresponding polynomial values for day 22\n",
      "Finished storing all epochs and corresponding polynomial values for day 23\n",
      "Finished storing all epochs and corresponding polynomial values for day 24\n",
      "Finished storing all epochs and corresponding polynomial values for day 25\n",
      "Finished storing all epochs and corresponding polynomial values for day 26\n",
      "Finished storing all epochs and corresponding polynomial values for day 27\n",
      "Finished storing all epochs and corresponding polynomial values for day 28\n",
      "Finished storing all epochs and corresponding polynomial values for day 29\n",
      "Finished storing all epochs and corresponding polynomial values for day 30\n",
      "Finished storing all epochs and corresponding polynomial values for day 31\n",
      "Finished storing all epochs and corresponding polynomial values for day 32\n",
      "Finished storing all epochs and corresponding polynomial values for day 33\n",
      "Finished storing all epochs and corresponding polynomial values for day 34\n",
      "Finished storing all epochs and corresponding polynomial values for day 35\n",
      "Finished storing all epochs and corresponding polynomial values for day 36\n",
      "Finished storing all epochs and corresponding polynomial values for day 37\n",
      "Finished storing all epochs and corresponding polynomial values for day 38\n",
      "Finished storing all epochs and corresponding polynomial values for day 39\n",
      "Finished storing all epochs and corresponding polynomial values for day 40\n",
      "Finished storing all epochs and corresponding polynomial values for day 41\n",
      "Finished storing all epochs and corresponding polynomial values for day 42\n",
      "Finished storing all epochs and corresponding polynomial values for day 43\n",
      "Finished storing all epochs and corresponding polynomial values for day 44\n",
      "Finished storing all epochs and corresponding polynomial values for day 45\n",
      "Finished storing all epochs and corresponding polynomial values for day 46\n",
      "Finished storing all epochs and corresponding polynomial values for day 47\n",
      "Finished storing all epochs and corresponding polynomial values for day 48\n",
      "Finished storing all epochs and corresponding polynomial values for day 49\n",
      "Finished storing all epochs and corresponding polynomial values for day 50\n",
      "Finished storing all epochs and corresponding polynomial values for day 51\n",
      "Finished storing all epochs and corresponding polynomial values for day 52\n",
      "Finished storing all epochs and corresponding polynomial values for day 53\n",
      "Finished storing all epochs and corresponding polynomial values for day 54\n",
      "Finished storing all epochs and corresponding polynomial values for day 55\n",
      "Finished storing all epochs and corresponding polynomial values for day 56\n",
      "Finished storing all epochs and corresponding polynomial values for day 57\n",
      "Finished storing all epochs and corresponding polynomial values for day 58\n",
      "Finished storing all epochs and corresponding polynomial values for day 59\n",
      "Finished storing all epochs and corresponding polynomial values for day 60\n",
      "Finished storing all epochs and corresponding polynomial values for day 61\n",
      "Finished storing all epochs and corresponding polynomial values for day 62\n",
      "Finished storing all epochs and corresponding polynomial values for day 63\n",
      "Finished storing all epochs and corresponding polynomial values for day 64\n",
      "Finished storing all epochs and corresponding polynomial values for day 65\n",
      "Finished storing all epochs and corresponding polynomial values for day 66\n",
      "Finished storing all epochs and corresponding polynomial values for day 67\n",
      "Finished storing all epochs and corresponding polynomial values for day 68\n",
      "Finished storing all epochs and corresponding polynomial values for day 69\n",
      "Finished storing all epochs and corresponding polynomial values for day 70\n",
      "Finished storing all epochs and corresponding polynomial values for day 71\n",
      "Finished storing all epochs and corresponding polynomial values for day 72\n",
      "Finished storing all epochs and corresponding polynomial values for day 73\n",
      "Finished storing all epochs and corresponding polynomial values for day 74\n",
      "Finished storing all epochs and corresponding polynomial values for day 75\n",
      "Finished storing all epochs and corresponding polynomial values for day 76\n",
      "Finished storing all epochs and corresponding polynomial values for day 77\n",
      "Finished storing all epochs and corresponding polynomial values for day 78\n",
      "Finished storing all epochs and corresponding polynomial values for day 79\n",
      "Finished storing all epochs and corresponding polynomial values for day 80\n",
      "Finished storing all epochs and corresponding polynomial values for day 81\n",
      "Finished storing all epochs and corresponding polynomial values for day 82\n",
      "Finished storing all epochs and corresponding polynomial values for day 83\n",
      "Finished storing all epochs and corresponding polynomial values for day 84\n",
      "Finished storing all epochs and corresponding polynomial values for day 85\n",
      "Finished storing all epochs and corresponding polynomial values for day 86\n",
      "Finished storing all epochs and corresponding polynomial values for day 87\n",
      "Finished storing all epochs and corresponding polynomial values for day 88\n",
      "Finished storing all epochs and corresponding polynomial values for day 89\n",
      "Finished storing all epochs and corresponding polynomial values for day 90\n",
      "Finished storing all epochs and corresponding polynomial values for day 91\n",
      "Finished storing all epochs and corresponding polynomial values for day 92\n",
      "Finished storing all epochs and corresponding polynomial values for day 93\n",
      "Finished storing all epochs and corresponding polynomial values for day 94\n",
      "Finished storing all epochs and corresponding polynomial values for day 95\n",
      "Finished storing all epochs and corresponding polynomial values for day 96\n",
      "Finished storing all epochs and corresponding polynomial values for day 97\n",
      "Finished storing all epochs and corresponding polynomial values for day 98\n",
      "Finished storing all epochs and corresponding polynomial values for day 99\n",
      "Finished storing all epochs and corresponding polynomial values for day 100\n",
      "Finished storing all epochs and corresponding polynomial values for day 101\n",
      "Finished storing all epochs and corresponding polynomial values for day 102\n",
      "Finished storing all epochs and corresponding polynomial values for day 103\n",
      "Finished storing all epochs and corresponding polynomial values for day 104\n",
      "Finished storing all epochs and corresponding polynomial values for day 105\n",
      "Finished storing all epochs and corresponding polynomial values for day 106\n",
      "Finished storing all epochs and corresponding polynomial values for day 107\n",
      "Finished storing all epochs and corresponding polynomial values for day 108\n",
      "Finished storing all epochs and corresponding polynomial values for day 109\n",
      "Finished storing all epochs and corresponding polynomial values for day 110\n",
      "Finished storing all epochs and corresponding polynomial values for day 111\n",
      "Finished storing all epochs and corresponding polynomial values for day 112\n",
      "Finished storing all epochs and corresponding polynomial values for day 113\n",
      "Finished storing all epochs and corresponding polynomial values for day 114\n",
      "Finished storing all epochs and corresponding polynomial values for day 115\n",
      "Finished storing all epochs and corresponding polynomial values for day 116\n",
      "Finished storing all epochs and corresponding polynomial values for day 117\n",
      "Finished storing all epochs and corresponding polynomial values for day 118\n",
      "Finished storing all epochs and corresponding polynomial values for day 119\n",
      "Finished storing all epochs and corresponding polynomial values for day 120\n",
      "Finished storing all epochs and corresponding polynomial values for day 121\n",
      "Finished storing all epochs and corresponding polynomial values for day 122\n",
      "Finished storing all epochs and corresponding polynomial values for day 123\n",
      "Finished storing all epochs and corresponding polynomial values for day 124\n",
      "Finished storing all epochs and corresponding polynomial values for day 125\n",
      "Finished storing all epochs and corresponding polynomial values for day 126\n",
      "Finished storing all epochs and corresponding polynomial values for day 127\n",
      "Finished storing all epochs and corresponding polynomial values for day 128\n",
      "Finished storing all epochs and corresponding polynomial values for day 129\n",
      "Finished storing all epochs and corresponding polynomial values for day 130\n",
      "Finished storing all epochs and corresponding polynomial values for day 131\n",
      "Finished storing all epochs and corresponding polynomial values for day 132\n",
      "Finished storing all epochs and corresponding polynomial values for day 133\n",
      "Finished storing all epochs and corresponding polynomial values for day 134\n",
      "Finished storing all epochs and corresponding polynomial values for day 135\n",
      "Finished storing all epochs and corresponding polynomial values for day 136\n",
      "Finished storing all epochs and corresponding polynomial values for day 137\n",
      "Finished storing all epochs and corresponding polynomial values for day 138\n",
      "Finished storing all epochs and corresponding polynomial values for day 139\n",
      "Finished storing all epochs and corresponding polynomial values for day 140\n",
      "Finished storing all epochs and corresponding polynomial values for day 141\n",
      "Finished storing all epochs and corresponding polynomial values for day 142\n",
      "Finished storing all epochs and corresponding polynomial values for day 143\n",
      "Finished storing all epochs and corresponding polynomial values for day 144\n",
      "Finished storing all epochs and corresponding polynomial values for day 145\n",
      "Finished storing all epochs and corresponding polynomial values for day 146\n",
      "Finished storing all epochs and corresponding polynomial values for day 147\n",
      "Finished storing all epochs and corresponding polynomial values for day 148\n",
      "Finished storing all epochs and corresponding polynomial values for day 149\n",
      "Finished storing all epochs and corresponding polynomial values for day 150\n",
      "Finished storing all epochs and corresponding polynomial values for day 151\n",
      "Finished storing all epochs and corresponding polynomial values for day 152\n",
      "Finished storing all epochs and corresponding polynomial values for day 153\n",
      "Finished storing all epochs and corresponding polynomial values for day 154\n",
      "Finished storing all epochs and corresponding polynomial values for day 155\n",
      "Finished storing all epochs and corresponding polynomial values for day 156\n",
      "Finished storing all epochs and corresponding polynomial values for day 157\n",
      "Finished storing all epochs and corresponding polynomial values for day 158\n",
      "Finished storing all epochs and corresponding polynomial values for day 159\n",
      "Finished storing all epochs and corresponding polynomial values for day 160\n",
      "Finished storing all epochs and corresponding polynomial values for day 161\n",
      "Finished storing all epochs and corresponding polynomial values for day 162\n",
      "Finished storing all epochs and corresponding polynomial values for day 163\n",
      "Finished storing all epochs and corresponding polynomial values for day 164\n",
      "Finished storing all epochs and corresponding polynomial values for day 165\n",
      "Finished storing all epochs and corresponding polynomial values for day 166\n",
      "Finished storing all epochs and corresponding polynomial values for day 167\n",
      "Finished storing all epochs and corresponding polynomial values for day 168\n",
      "Finished storing all epochs and corresponding polynomial values for day 169\n",
      "Finished storing all epochs and corresponding polynomial values for day 170\n",
      "Finished storing all epochs and corresponding polynomial values for day 171\n",
      "Finished storing all epochs and corresponding polynomial values for day 172\n",
      "Finished storing all epochs and corresponding polynomial values for day 173\n",
      "Finished storing all epochs and corresponding polynomial values for day 174\n",
      "Finished storing all epochs and corresponding polynomial values for day 175\n",
      "Finished storing all epochs and corresponding polynomial values for day 176\n",
      "Finished storing all epochs and corresponding polynomial values for day 177\n",
      "Finished storing all epochs and corresponding polynomial values for day 178\n",
      "Finished storing all epochs and corresponding polynomial values for day 179\n",
      "Finished storing all epochs and corresponding polynomial values for day 180\n",
      "Finished storing all epochs and corresponding polynomial values for day 181\n",
      "Finished storing all epochs and corresponding polynomial values for day 182\n",
      "Finished storing all epochs and corresponding polynomial values for day 183\n",
      "Finished storing all epochs and corresponding polynomial values for day 184\n",
      "Finished storing all epochs and corresponding polynomial values for day 185\n",
      "Finished storing all epochs and corresponding polynomial values for day 186\n",
      "Finished storing all epochs and corresponding polynomial values for day 187\n",
      "Finished storing all epochs and corresponding polynomial values for day 188\n",
      "Finished storing all epochs and corresponding polynomial values for day 189\n",
      "Finished storing all epochs and corresponding polynomial values for day 190\n",
      "Finished storing all epochs and corresponding polynomial values for day 191\n",
      "Finished storing all epochs and corresponding polynomial values for day 192\n",
      "Finished storing all epochs and corresponding polynomial values for day 193\n",
      "Finished storing all epochs and corresponding polynomial values for day 194\n",
      "Finished storing all epochs and corresponding polynomial values for day 195\n",
      "Finished storing all epochs and corresponding polynomial values for day 196\n",
      "Finished storing all epochs and corresponding polynomial values for day 197\n",
      "Finished storing all epochs and corresponding polynomial values for day 198\n",
      "Finished storing all epochs and corresponding polynomial values for day 199\n",
      "Finished storing all epochs and corresponding polynomial values for day 200\n",
      "Finished storing all epochs and corresponding polynomial values for day 201\n",
      "Finished storing all epochs and corresponding polynomial values for day 202\n",
      "Finished storing all epochs and corresponding polynomial values for day 203\n",
      "Finished storing all epochs and corresponding polynomial values for day 204\n",
      "Finished storing all epochs and corresponding polynomial values for day 205\n",
      "Finished storing all epochs and corresponding polynomial values for day 206\n",
      "Finished storing all epochs and corresponding polynomial values for day 207\n",
      "Finished storing all epochs and corresponding polynomial values for day 208\n",
      "Finished storing all epochs and corresponding polynomial values for day 209\n",
      "Finished storing all epochs and corresponding polynomial values for day 210\n",
      "Finished storing all epochs and corresponding polynomial values for day 211\n",
      "Finished storing all epochs and corresponding polynomial values for day 212\n",
      "Finished storing all epochs and corresponding polynomial values for day 213\n",
      "Finished storing all epochs and corresponding polynomial values for day 214\n",
      "Finished storing all epochs and corresponding polynomial values for day 215\n",
      "Finished storing all epochs and corresponding polynomial values for day 216\n",
      "Finished storing all epochs and corresponding polynomial values for day 217\n",
      "Finished storing all epochs and corresponding polynomial values for day 218\n",
      "Finished storing all epochs and corresponding polynomial values for day 219\n",
      "Finished storing all epochs and corresponding polynomial values for day 220\n",
      "Finished storing all epochs and corresponding polynomial values for day 221\n",
      "Finished storing all epochs and corresponding polynomial values for day 222\n",
      "Finished storing all epochs and corresponding polynomial values for day 223\n",
      "Finished storing all epochs and corresponding polynomial values for day 224\n",
      "Finished storing all epochs and corresponding polynomial values for day 225\n",
      "Finished storing all epochs and corresponding polynomial values for day 226\n",
      "Finished storing all epochs and corresponding polynomial values for day 227\n",
      "Finished storing all epochs and corresponding polynomial values for day 228\n",
      "Finished storing all epochs and corresponding polynomial values for day 229\n",
      "Finished storing all epochs and corresponding polynomial values for day 230\n",
      "Finished storing all epochs and corresponding polynomial values for day 231\n",
      "Finished storing all epochs and corresponding polynomial values for day 232\n",
      "Finished storing all epochs and corresponding polynomial values for day 233\n",
      "Finished storing all epochs and corresponding polynomial values for day 234\n",
      "Finished storing all epochs and corresponding polynomial values for day 235\n",
      "Finished storing all epochs and corresponding polynomial values for day 236\n",
      "Finished storing all epochs and corresponding polynomial values for day 237\n",
      "Finished storing all epochs and corresponding polynomial values for day 238\n",
      "Finished storing all epochs and corresponding polynomial values for day 239\n",
      "Finished storing all epochs and corresponding polynomial values for day 240\n",
      "Finished storing all epochs and corresponding polynomial values for day 241\n",
      "Finished storing all epochs and corresponding polynomial values for day 242\n",
      "Finished storing all epochs and corresponding polynomial values for day 243\n",
      "Finished storing all epochs and corresponding polynomial values for day 244\n",
      "Finished storing all epochs and corresponding polynomial values for day 245\n",
      "Finished storing all epochs and corresponding polynomial values for day 246\n",
      "Finished storing all epochs and corresponding polynomial values for day 247\n",
      "Finished storing all epochs and corresponding polynomial values for day 248\n",
      "Finished storing all epochs and corresponding polynomial values for day 249\n",
      "Finished storing all epochs and corresponding polynomial values for day 250\n",
      "Finished storing all epochs and corresponding polynomial values for day 251\n",
      "Finished storing all epochs and corresponding polynomial values for day 252\n",
      "Finished storing all epochs and corresponding polynomial values for day 253\n",
      "Finished storing all epochs and corresponding polynomial values for day 254\n",
      "Finished storing all epochs and corresponding polynomial values for day 255\n",
      "Finished storing all epochs and corresponding polynomial values for day 256\n",
      "Finished storing all epochs and corresponding polynomial values for day 257\n",
      "Finished storing all epochs and corresponding polynomial values for day 258\n",
      "Finished storing all epochs and corresponding polynomial values for day 259\n",
      "Finished storing all epochs and corresponding polynomial values for day 260\n",
      "Finished storing all epochs and corresponding polynomial values for day 261\n",
      "Finished storing all epochs and corresponding polynomial values for day 262\n",
      "Finished storing all epochs and corresponding polynomial values for day 263\n",
      "Finished storing all epochs and corresponding polynomial values for day 264\n",
      "Finished storing all epochs and corresponding polynomial values for day 265\n",
      "Finished storing all epochs and corresponding polynomial values for day 266\n",
      "Finished storing all epochs and corresponding polynomial values for day 267\n",
      "Finished storing all epochs and corresponding polynomial values for day 268\n",
      "Finished storing all epochs and corresponding polynomial values for day 269\n",
      "Finished storing all epochs and corresponding polynomial values for day 270\n",
      "Finished storing all epochs and corresponding polynomial values for day 271\n",
      "Finished storing all epochs and corresponding polynomial values for day 272\n",
      "Finished storing all epochs and corresponding polynomial values for day 273\n",
      "Finished storing all epochs and corresponding polynomial values for day 274\n",
      "Finished storing all epochs and corresponding polynomial values for day 275\n",
      "Finished storing all epochs and corresponding polynomial values for day 276\n",
      "Finished storing all epochs and corresponding polynomial values for day 277\n",
      "Finished storing all epochs and corresponding polynomial values for day 278\n",
      "Finished storing all epochs and corresponding polynomial values for day 279\n",
      "Finished storing all epochs and corresponding polynomial values for day 280\n",
      "Finished storing all epochs and corresponding polynomial values for day 281\n",
      "Finished storing all epochs and corresponding polynomial values for day 282\n",
      "Finished storing all epochs and corresponding polynomial values for day 283\n",
      "Finished storing all epochs and corresponding polynomial values for day 284\n",
      "Finished storing all epochs and corresponding polynomial values for day 285\n",
      "Finished storing all epochs and corresponding polynomial values for day 286\n",
      "Finished storing all epochs and corresponding polynomial values for day 287\n",
      "Finished storing all epochs and corresponding polynomial values for day 288\n",
      "Finished storing all epochs and corresponding polynomial values for day 289\n",
      "Finished storing all epochs and corresponding polynomial values for day 290\n",
      "Finished storing all epochs and corresponding polynomial values for day 291\n",
      "Finished storing all epochs and corresponding polynomial values for day 292\n",
      "Finished storing all epochs and corresponding polynomial values for day 293\n",
      "Finished storing all epochs and corresponding polynomial values for day 294\n",
      "Finished storing all epochs and corresponding polynomial values for day 295\n",
      "Finished storing all epochs and corresponding polynomial values for day 296\n",
      "Finished storing all epochs and corresponding polynomial values for day 297\n",
      "Finished storing all epochs and corresponding polynomial values for day 298\n",
      "Finished storing all epochs and corresponding polynomial values for day 299\n",
      "Finished storing all epochs and corresponding polynomial values for day 300\n",
      "Finished storing all epochs and corresponding polynomial values for day 301\n",
      "Finished storing all epochs and corresponding polynomial values for day 302\n",
      "Finished storing all epochs and corresponding polynomial values for day 303\n",
      "Finished storing all epochs and corresponding polynomial values for day 304\n",
      "Finished storing all epochs and corresponding polynomial values for day 305\n",
      "Finished storing all epochs and corresponding polynomial values for day 306\n",
      "Finished storing all epochs and corresponding polynomial values for day 307\n",
      "Finished storing all epochs and corresponding polynomial values for day 308\n",
      "Finished storing all epochs and corresponding polynomial values for day 309\n",
      "Finished storing all epochs and corresponding polynomial values for day 310\n",
      "Finished storing all epochs and corresponding polynomial values for day 311\n",
      "Finished storing all epochs and corresponding polynomial values for day 312\n",
      "Finished storing all epochs and corresponding polynomial values for day 313\n",
      "Finished storing all epochs and corresponding polynomial values for day 314\n",
      "Finished storing all epochs and corresponding polynomial values for day 315\n",
      "Finished storing all epochs and corresponding polynomial values for day 316\n",
      "Finished storing all epochs and corresponding polynomial values for day 317\n",
      "Finished storing all epochs and corresponding polynomial values for day 318\n",
      "Finished storing all epochs and corresponding polynomial values for day 319\n",
      "Finished storing all epochs and corresponding polynomial values for day 320\n",
      "Finished storing all epochs and corresponding polynomial values for day 321\n",
      "Finished storing all epochs and corresponding polynomial values for day 322\n",
      "Finished storing all epochs and corresponding polynomial values for day 323\n",
      "Finished storing all epochs and corresponding polynomial values for day 324\n",
      "Finished storing all epochs and corresponding polynomial values for day 325\n",
      "Finished storing all epochs and corresponding polynomial values for day 326\n",
      "Finished storing all epochs and corresponding polynomial values for day 327\n",
      "Finished storing all epochs and corresponding polynomial values for day 328\n",
      "Finished storing all epochs and corresponding polynomial values for day 329\n",
      "Finished storing all epochs and corresponding polynomial values for day 330\n",
      "Finished storing all epochs and corresponding polynomial values for day 331\n",
      "Finished storing all epochs and corresponding polynomial values for day 332\n",
      "Finished storing all epochs and corresponding polynomial values for day 333\n",
      "Finished storing all epochs and corresponding polynomial values for day 334\n",
      "Finished storing all epochs and corresponding polynomial values for day 335\n",
      "Finished storing all epochs and corresponding polynomial values for day 336\n",
      "Finished storing all epochs and corresponding polynomial values for day 337\n",
      "Finished storing all epochs and corresponding polynomial values for day 338\n",
      "Finished storing all epochs and corresponding polynomial values for day 339\n",
      "Finished storing all epochs and corresponding polynomial values for day 340\n",
      "Finished storing all epochs and corresponding polynomial values for day 341\n",
      "Finished storing all epochs and corresponding polynomial values for day 342\n",
      "Finished storing all epochs and corresponding polynomial values for day 343\n",
      "Finished storing all epochs and corresponding polynomial values for day 344\n",
      "Finished storing all epochs and corresponding polynomial values for day 345\n",
      "Finished storing all epochs and corresponding polynomial values for day 346\n",
      "Finished storing all epochs and corresponding polynomial values for day 347\n",
      "Finished storing all epochs and corresponding polynomial values for day 348\n",
      "Finished storing all epochs and corresponding polynomial values for day 349\n",
      "Finished storing all epochs and corresponding polynomial values for day 350\n",
      "Finished storing all epochs and corresponding polynomial values for day 351\n",
      "Finished storing all epochs and corresponding polynomial values for day 352\n",
      "Finished storing all epochs and corresponding polynomial values for day 353\n",
      "Finished storing all epochs and corresponding polynomial values for day 354\n",
      "Finished storing all epochs and corresponding polynomial values for day 355\n",
      "Finished storing all epochs and corresponding polynomial values for day 356\n",
      "Finished storing all epochs and corresponding polynomial values for day 357\n",
      "Finished storing all epochs and corresponding polynomial values for day 358\n",
      "Finished storing all epochs and corresponding polynomial values for day 359\n",
      "Finished storing all epochs and corresponding polynomial values for day 360\n",
      "Finished storing all epochs and corresponding polynomial values for day 361\n",
      "Finished storing all epochs and corresponding polynomial values for day 362\n",
      "Finished storing all epochs and corresponding polynomial values for day 363\n",
      "Finished storing all epochs and corresponding polynomial values for day 364\n",
      "Finished storing all epochs and corresponding polynomial values for day 365\n",
      "CPU times: user 6.89 s, sys: 229 ms, total: 7.12 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# store all the polynomial values and corresponding times in the time_datetimes and poly_values array - Wall time: 25.2 s \n",
    "\n",
    "for day in range(1, 366):\n",
    "    time_datetimes, poly_values = load_broadcast_file(day)\n",
    "    all_time_datetimes.extend(time_datetimes)\n",
    "    all_poly_values.extend(poly_values)\n",
    "    print(f'Finished storing all epochs and corresponding polynomial values for day {day}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c682e371-4612-41ba-b1b9-9e29581aa4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed corrections for week: 1982\n",
      "Completed corrections for week: 1983\n",
      "Completed corrections for week: 1984\n",
      "Completed corrections for week: 1985\n",
      "Completed corrections for week: 1986\n",
      "Completed corrections for week: 1987\n",
      "Completed corrections for week: 1988\n",
      "Completed corrections for week: 1989\n",
      "Completed corrections for week: 1990\n",
      "Completed corrections for week: 1991\n",
      "Completed corrections for week: 1992\n",
      "Completed corrections for week: 1993\n",
      "Completed corrections for week: 1994\n",
      "Completed corrections for week: 1995\n",
      "Completed corrections for week: 1996\n",
      "Completed corrections for week: 1997\n",
      "Completed corrections for week: 1998\n",
      "Completed corrections for week: 1999\n",
      "Completed corrections for week: 2000\n",
      "Completed corrections for week: 2001\n",
      "Completed corrections for week: 2002\n",
      "Completed corrections for week: 2003\n",
      "Completed corrections for week: 2004\n",
      "Completed corrections for week: 2005\n",
      "Completed corrections for week: 2006\n",
      "Completed corrections for week: 2007\n",
      "Completed corrections for week: 2008\n",
      "Completed corrections for week: 2009\n",
      "Completed corrections for week: 2010\n",
      "Completed corrections for week: 2011\n",
      "Completed corrections for week: 2012\n",
      "Completed corrections for week: 2013\n",
      "Completed corrections for week: 2014\n",
      "Completed corrections for week: 2015\n",
      "Completed corrections for week: 2016\n",
      "Completed corrections for week: 2017\n",
      "Completed corrections for week: 2018\n",
      "Completed corrections for week: 2019\n",
      "Completed corrections for week: 2020\n",
      "Completed corrections for week: 2021\n",
      "Completed corrections for week: 2022\n",
      "Completed corrections for week: 2023\n",
      "Completed corrections for week: 2024\n",
      "Completed corrections for week: 2025\n",
      "Completed corrections for week: 2026\n",
      "Completed corrections for week: 2027\n",
      "Completed corrections for week: 2028\n",
      "Completed corrections for week: 2029\n",
      "Completed corrections for week: 2030\n",
      "Completed corrections for week: 2031\n",
      "Completed corrections for week: 2032\n",
      "Completed corrections for week: 2033\n",
      "Completed corrections for week: 2034\n",
      "CPU times: user 7h 25min 55s, sys: 1min 12s, total: 7h 27min 7s\n",
      "Wall time: 10h 14min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# previously completed weeks 2034 until 2060 - stopped after 2060 was completed :: 7/29/2024\n",
    "# previously completed weeks 2034 until 2086 :: 8/2/2024 (Wall Time: 15 hr 26 minutes 25 seconds)\n",
    "# previously completed weeks 1982 until 2034 :: 8/29/2024 (Wall Time:  10 hr  14 minutes 56 seconds)\n",
    "\n",
    "\n",
    "for week in range(1982, 2035):\n",
    "    epochs, clock_bias = load_clk_file(week)\n",
    "    matching_indices = [i for i, epoch in enumerate(epochs) if epoch in all_time_datetimes]\n",
    "    matching_epochs = [epochs[i] for i in matching_indices]\n",
    "    matching_clock_bias = clock_bias[matching_indices]\n",
    "    matching_poly_values = [all_poly_values[all_time_datetimes.index(epoch)] for epoch in matching_epochs]\n",
    "\n",
    "    correction_vals = matching_clock_bias - matching_poly_values\n",
    "    all_matching_epochs.extend(matching_epochs)\n",
    "    all_matching_clock_bias.extend(matching_clock_bias)\n",
    "    all_matching_poly_values.extend(matching_poly_values)\n",
    "    all_correction_vals.extend(correction_vals)\n",
    "    print(f'Completed corrections for week: {week}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "709141bb-aa90-4913-bf07-604359e443c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/Volumes/MARI/ssdl_gps/correction_data/2018/correction_data_2018.npz',\n",
    "         matching_epochs=all_matching_epochs,\n",
    "         matching_clock_bias=all_matching_clock_bias,\n",
    "         matching_poly_values=all_matching_poly_values,\n",
    "         correction_vals=all_correction_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b4bc1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7a1191-dff3-4b5c-9653-1c7a9448e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_epoch_strings = [epoch.strftime('%Y:%m:%d:%H:%M:%S') for epoch in all_matching_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a966193-31f1-4b3b-b990-680dfcdbe9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/Volumes/MARI/ssdl_gps/correction_data/2018/correction_data_2018_str_update.npz',\n",
    "         matching_epochs=matching_epoch_strings,\n",
    "         matching_clock_bias=all_matching_clock_bias,\n",
    "         matching_poly_values=all_matching_poly_values,\n",
    "         correction_vals=all_correction_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bf6e7-01c0-4cf6-ba41-29fd47753fac",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad64b01e-6424-4ecd-927c-b0f7609530d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data/2018/correction_data_2018_str_update.npz')\n",
    "epochs = data['matching_epochs']\n",
    "final_clock_bias = data['matching_clock_bias']\n",
    "broadcast_clock_bias = data['matching_poly_values']\n",
    "correction_value = data['correction_vals']\n",
    "\n",
    "epoch_datetime = [datetime.strptime(epoch, '%Y:%m:%d:%H:%M:%S') for epoch in epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ac50415-fa66-4520-9b53-226bc6875751",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in range(1, 13):\n",
    "    start_date = datetime(2018, month, 1)\n",
    "    if month == 12:\n",
    "        end_date = datetime(2019, 1, 1)\n",
    "    else:\n",
    "        end_date = datetime(2018, month + 1, 1)\n",
    "\n",
    "    monthly_indices = [i for i, epoch in enumerate(epoch_datetime) if start_date <= epoch < end_date]\n",
    "    monthly_epochs = [epoch_datetime[i] for i in monthly_indices]\n",
    "    monthly_clock_bias = [final_clock_bias[i] for i in monthly_indices]\n",
    "    monthly_poly_values = [broadcast_clock_bias[i] for i in monthly_indices]\n",
    "    monthly_correction_vals = [correction_value[i] for i in monthly_indices]\n",
    "\n",
    "    # Plot clock_bias and poly_values\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.scatter(monthly_epochs, monthly_clock_bias, label='Clock Bias (Station: GRG)')\n",
    "    plt.scatter(monthly_epochs, monthly_poly_values, label='Broadcast Polynomial Values', s=5)\n",
    "    plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "    plt.ylabel('Bias Values (s)')\n",
    "    # plt.ylim(-0.00002734, -0.00002722)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=5))  # Set major ticks every hour\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.title(f'Clock Bias and Poly Values for G21 {start_date.strftime(\"%B %Y\")}')\n",
    "    plt.savefig(f'/Volumes/MARI/ssdl_gps/plots_raw/clock_bias_poly_values_{month:02d}_2018.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot correction values\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.scatter(monthly_epochs, monthly_correction_vals, label='Clock Bias Correction (s)')\n",
    "    plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "    plt.ylabel('Bias Correction Values (s)')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=5))  # Set major ticks every hour\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.title(f'Clock Bias Correction Values for G21 {start_date.strftime(\"%B %Y\")}')\n",
    "    plt.savefig(f'/Volumes/MARI/ssdl_gps/plots_raw/correction_values_{month:02d}_2018.png')\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df8e0965-b1f6-4e2b-9858-0a595a5d16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in range(1, 13):\n",
    "    start_date = datetime(2018, month, 1)\n",
    "    if month == 12:\n",
    "        end_date = datetime(2019, 1, 1)\n",
    "    else:\n",
    "        end_date = datetime(2018, month + 1, 1)\n",
    "\n",
    "    monthly_indices = [i for i, epoch in enumerate(epoch_datetime) if start_date <= epoch < end_date]\n",
    "    monthly_epochs = [epoch_datetime[i] for i in monthly_indices]\n",
    "    monthly_clock_bias = [epoch_datetime[i] for i in monthly_indices]\n",
    "    monthly_poly_values = [epoch_datetime[i] for i in monthly_indices]\n",
    "    monthly_correction_vals = [epoch_datetime[i] for i in monthly_indices]\n",
    "\n",
    "    # Plot clock_bias and poly_values\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.plot(monthly_epochs, monthly_clock_bias, label='Clock Bias (Station: GRG)', linewidth=4)\n",
    "    plt.plot(monthly_epochs, monthly_poly_values, label='Broadcast Polynomial Values', linewidth=2)\n",
    "    plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "    plt.ylabel('Bias Values (s)')\n",
    "    # plt.ylim(-0.0002734, -0.0002722)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=5))  # Set major ticks every hour\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.title(f'Clock Bias and Poly Values for G21 {start_date.strftime(\"%B %Y\")}')\n",
    "    plt.savefig(f'/Volumes/MARI/ssdl_gps/plots_raw/plt_clock_bias_poly_values_{month:02d}_2018.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot correction values\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.plot(monthly_epochs, monthly_correction_vals, label='Clock Bias Correction (s)')\n",
    "    plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "    plt.ylabel('Bias Correction Values (s)')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=5))  # Set major ticks every hour\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.title(f'Clock Bias Correction Values for G21 {start_date.strftime(\"%B %Y\")}')\n",
    "    plt.savefig(f'/Volumes/MARI/ssdl_gps/plots_raw/plt_correction_values_{month:02d}_2018.png')\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80492caa-09b6-49b9-9416-d32c98fd7922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2e72586-adc9-4137-86f6-2031c343f746",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fcf85-90cd-48e0-8989-24246545c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast_01082019 = np.load('/Volumes/MARI/ssdl_gps/rnx_polynomial/gps_poly_2019008_G21.npz')\n",
    "# broadcast_01082019.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a37ac-663b-4bda-afe2-63e09717fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_strings = broadcast_01082019['time_strings']\n",
    "# poly_values = broadcast_01082019['poly_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad4e0c-dd36-4c12-b79f-3da0ec69770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_datetimes = [datetime.strptime(ts, '%Y:%m:%d:%H:%M:%S') for ts in time_strings]\n",
    "# # time_datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b70824-c5b8-46b6-b562-77a003c310cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clk = np.load('/Volumes/MARI/ssdl_gps/clk_npz_sat/G21/grg_gps_clk_2035_G21.npz')\n",
    "# # clk = np.load('gps_2035_G21_test.npz')\n",
    "\n",
    "# clk.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92585f4-1ab4-4643-8488-f6b217406643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the arrays\n",
    "# satellite = clk['satellite']\n",
    "# yyyy = clk['yyyy']\n",
    "# mm = clk['mm']\n",
    "# dd = clk['dd']\n",
    "# hh = clk['hh']\n",
    "# mi = clk['mi']\n",
    "# ss = clk['ss']\n",
    "# clock_bias = clk['clock_bias_vals']\n",
    "# ver = clk['vers']\n",
    "# filepath = clk['filename']\n",
    " \n",
    "# # Combine date and time into datetime objects\n",
    "# epochs = [datetime(year, month, day, hour, minute, second)\n",
    "#           for year, month, day, hour, minute, second \n",
    "#           in zip(yyyy, mm, dd, hh, mi, ss)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16bca3-6059-4a00-aedc-7438d4ded5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a7be3-2265-45ad-8049-edf5356384e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching_indices = [i for i, epoch in enumerate(epochs) if epoch in time_datetimes]\n",
    "# matching_epochs = [epochs[i] for i in matching_indices]\n",
    "# matching_clock_bias = clock_bias[matching_indices]\n",
    "# # matching_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4011e-d179-4a5a-a9b6-68520276ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching_poly_values = [poly_values[time_datetimes.index(epoch)] for epoch in epochs if epoch in time_datetimes]\n",
    "# # matching_poly_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e151c-a140-4ec6-b47f-370ce4d4234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot clock_bias and poly_values on the same plot\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.scatter(matching_epochs, matching_clock_bias, label='Clock Bias (Station: GRG)')\n",
    "# plt.scatter(matching_epochs, matching_poly_values, label='Broadcast Polynomial Values', s = 5)\n",
    "# plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "# plt.ylabel('Bias Values (s)')\n",
    "# plt.ylim(-0.0002734,-0.0002722)\n",
    "# # plt.xlim(datetime.strptime('2019:01:08:18:00:00', '%Y:%m:%d:%H:%M:%S'),datetime.strptime('2019:01:08:21:00:00', '%Y:%m:%d:%H:%M:%S'))\n",
    "# plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "# plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=1))  # Set major ticks every hour\n",
    "# plt.tight_layout()\n",
    "# plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.title('Clock Bias and Poly Values for G21 Jan 8, 2019')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65de7f-a0ae-46f8-9f99-278d606a1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction_val = matching_clock_bias - matching_poly_values\n",
    "# correction_val\n",
    "\n",
    "# e1 = datetime.strptime('2019:01:08:00:00:00', '%Y:%m:%d:%H:%M:%S')\n",
    "# e2 = datetime.strptime('2019:01:09:00:30:00', '%Y:%m:%d:%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678145f-9a51-4a41-883d-bd841f1d6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot clock_bias and poly_values on the same plot\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.scatter(matching_epochs, correction_val, label='Clock Bias Correction (s)')\n",
    "# plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "# plt.ylabel('Bias Correction Values (s)')\n",
    "# # plt.xlim(e1,e2)\n",
    "# plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "# plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=1))  # Set major ticks every hour\n",
    "# plt.tight_layout()\n",
    "# plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.title('Clock Bias Correction Value for G21 Jan 8, 2019')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269ed41-170b-4797-b805-1148c319ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tabulate import tabulate\n",
    "\n",
    "# # Load the data\n",
    "# data_g21 = np.load('gps_rnx_2019008_G21.npz')\n",
    "\n",
    "# # Get the file keys\n",
    "# keys = data_g21.files\n",
    "\n",
    "# # Prepare the data for display\n",
    "# table_data = [keys]  # Column headers\n",
    "\n",
    "# # Get the length of the data (assuming all arrays have the same length)\n",
    "# num_rows = len(data_g21[keys[0]])\n",
    "\n",
    "# # Collect rows of data\n",
    "# for i in range(num_rows):\n",
    "#     row = [data_g21[key][i] for key in keys]\n",
    "#     table_data.append(row)\n",
    "\n",
    "# # Print the table\n",
    "# print(tabulate(table_data, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a674b5-2596-43a6-b0da-515373314aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b36e59e1-ab4b-4bd2-b4c5-8aaa876af7e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (Archive) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e09265-ea94-4a9a-a647-63efa93df37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps//correction_data/correction_data_2019_str_update.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66305622-62f0-42c0-bd93-08719b69f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the arrays from the .npz file\n",
    "upd_matching_epochs = data['matching_epochs']\n",
    "upd_matching_clock_bias = data['matching_clock_bias']\n",
    "upd_matching_poly_values = data['matching_poly_values']\n",
    "upd_correction_vals = data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2ec2f-33e6-4c13-ab42-e05a5fd4d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(upd_matching_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab14260-ef9a-496b-a718-9cfd3f601608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correction values\n",
    "match_epochs_obj = [datetime.strptime(ts, '%Y:%m:%d:%H:%M:%S') for ts in upd_matching_epochs]\n",
    "\n",
    "plt.figure(figsize=(50, 10))\n",
    "plt.scatter(match_epochs_obj, upd_correction_vals, label='Clock Bias Correction (s)')\n",
    "plt.xlabel('Time (YYYY:MM:DD:HH:MI:SS)')\n",
    "plt.ylabel('Bias Correction Values (s)')\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=5))  # Set major ticks every hour\n",
    "# plt.tight_layout()\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.25)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.title(f'<test>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14768f-ec65-41ba-a6cb-04b1d02b40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(upd_matching_clock_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7d292-a5a1-4fcc-a117-b092bccc39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_epoch_dates = [datetime.strptime(t, '%Y:%m:%d:%H:%M:%S') for t in upd_matching_epochs]\n",
    "end_date = datetime(2019, 1, 31, 23, 59, 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb11b22-24ab-4d0e-844e-fbe6e930e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [(date <= end_date) and (date.second == 0 or date.second == 30) for date in upd_epoch_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2e539-cb1e-4ad5-8e58-adff103393c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(mask)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1637a75-a6b6-497f-bfca-a5e740fe2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mask to filter the arrays\n",
    "filtered_epochs = upd_matching_epochs[mask]\n",
    "filtered_clock_bias = upd_matching_clock_bias[mask]\n",
    "filtered_poly_values = upd_matching_poly_values[mask]\n",
    "filtered_correction_vals = upd_correction_vals[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba5999-092b-477c-8f71-56673bc20784",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef958a8-a60e-454d-8670-da574dacb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered data into a new .npz file\n",
    "np.savez('/Volumes/MARI/ssdl_gps/correction_data_2019_mo1_upd.npz',\n",
    "         matching_epochs = filtered_epochs,\n",
    "         matching_clock_bias=filtered_clock_bias,\n",
    "         matching_poly_values=filtered_poly_values,\n",
    "         correction_vals=filtered_correction_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7b4e0-69ea-4635-b81f-a31935b2e928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844c728-a73f-4d8c-afb7-f4cf40ec7843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3655b-9db2-4343-aee1-f5753944cfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4aff76-bcfb-437d-9d89-226075922d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d0c1b-e423-4eb1-b0e9-13aaaf1bf06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe02ffd-189a-448f-9ebe-ad5bc4ed952a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e04cd-fdb0-4be6-91d8-18321658e64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29635606-ae2b-4582-80b4-242f15218110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6ade4-67b9-4136-9492-85a40ebbd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data_2019_mo1.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160fe2b-352d-4d4a-9cf0-9826c1b8a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_poly_values = data['matching_poly_values']\n",
    "correction_vals = data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f39f6-50b5-4b57-a510-308d5187c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is in the correct shape [num_samples, num_features]\n",
    "# Assuming each sample has one feature (i.e., 1D data)\n",
    "matching_poly_values = matching_poly_values[:, np.newaxis]  # Shape: [num_samples, 1]\n",
    "correction_vals = correction_vals[:, np.newaxis]  # Shape: [num_samples, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e360e-30fe-49b1-8329-7183b366b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(matching_poly_values))\n",
    "test_size = len(matching_poly_values) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631dc617-5440-40db-9622-99e9d00ca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_poly_values = matching_poly_values[:train_size]\n",
    "train_correction_vals = correction_vals[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238fdb1-ef84-4b2c-b2b6-3fb19f1914a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poly_values = matching_poly_values[train_size:]\n",
    "test_correction_vals = correction_vals[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47237819-43d0-42d3-b506-fe6d129df3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_poly_values = torch.from_numpy(train_poly_values).float()\n",
    "train_correction_vals = torch.from_numpy(train_correction_vals).float()\n",
    "test_poly_values = torch.from_numpy(test_poly_values).float()\n",
    "test_correction_vals = torch.from_numpy(test_correction_vals).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97e89d-3420-4cf4-b2dd-05652fe28651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_poly_values, train_correction_vals)\n",
    "test_dataset = TensorDataset(test_poly_values, test_correction_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c073c-a317-428c-a391-755377abba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_covariates = data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68110c-e1e7-4588-af8d-420fd85bede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Load data\n",
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data_2019_mo1.npz')\n",
    "\n",
    "matching_poly_values = data['matching_poly_values']\n",
    "correction_vals = data['correction_vals']\n",
    "\n",
    "# Ensure data is in the correct shape [num_samples, num_features]\n",
    "# Assuming each sample has one feature (i.e., 1D data)\n",
    "matching_poly_values = matching_poly_values[:, np.newaxis]  # Shape: [num_samples, 1]\n",
    "correction_vals = correction_vals[:, np.newaxis]  # Shape: [num_samples, 1]\n",
    "\n",
    "# Apply min-max scaling\n",
    "scaler_poly = MinMaxScaler()\n",
    "scaler_correction = MinMaxScaler()\n",
    "\n",
    "matching_poly_values_scaled = scaler_poly.fit_transform(matching_poly_values)\n",
    "correction_vals_scaled = scaler_correction.fit_transform(correction_vals)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(0.7 * len(matching_poly_values_scaled))\n",
    "test_size = len(matching_poly_values_scaled) - train_size\n",
    "\n",
    "train_poly_values = matching_poly_values_scaled[:train_size]\n",
    "train_correction_vals = correction_vals_scaled[:train_size]\n",
    "\n",
    "test_poly_values = matching_poly_values_scaled[train_size:]\n",
    "test_correction_vals = correction_vals_scaled[train_size:]\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_poly_values = torch.from_numpy(train_poly_values).float()\n",
    "train_correction_vals = torch.from_numpy(train_correction_vals).float()\n",
    "test_poly_values = torch.from_numpy(test_poly_values).float()\n",
    "test_correction_vals = torch.from_numpy(test_correction_vals).float()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_poly_values, train_correction_vals)\n",
    "test_dataset = TensorDataset(test_poly_values, test_correction_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e4114-02cd-4a6b-b667-80d190493471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,embed_size,num_heads):\n",
    "        super(transformer_block, self).__init__()\n",
    "\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        self.fc = torch.nn.Sequential(nn.Linear(embed_size, 4 * embed_size),\n",
    "                                 nn.LeakyReLU(),\n",
    "                                 nn.Linear(4 * embed_size, embed_size))\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "        self.ln1 = torch.nn.LayerNorm(embed_size, eps=1e-6)\n",
    "        self.ln2 = torch.nn.LayerNorm(embed_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attn_out, _ = self.attention(x, x, x, need_weights=False)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        fc_out = self.fc(x)\n",
    "        x = x + self.dropout(fc_out)\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class transformer_forecaster(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,embed_size,num_heads,num_blocks):\n",
    "        super(transformer_forecaster, self).__init__()\n",
    "\n",
    "        num_len = len(numeric_covariates)\n",
    "\n",
    "        self.blocks = torch.nn.ModuleList([transformer_block(embed_size,num_heads) for n in range(num_blocks)])\n",
    "\n",
    "        self.forecast_head = torch.nn.Sequential(nn.Linear(embed_size, embed_size*2),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           nn.Dropout(drop_prob),\n",
    "                                           nn.Linear(embed_size*2, embed_size*4),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           nn.Linear(embed_size*4, forecast_length),\n",
    "                                           nn.ReLU())\n",
    "\n",
    "    def forward(self, x_numeric, x_category, x_static):\n",
    "\n",
    "        tmp_list = []\n",
    "        for i,embed_layer in enumerate(self.embedding_static):\n",
    "            tmp_list.append(embed_layer(x_static[:,i]))\n",
    "        categroical_static_embeddings = torch.stack(tmp_list).mean(dim=0).unsqueeze(1)\n",
    "\n",
    "        tmp_list = []\n",
    "        for i,embed_layer in enumerate(self.embedding_cov):\n",
    "            tmp_list.append(embed_layer(x_category[:,:,i]))\n",
    "        categroical_covariates_embeddings = torch.stack(tmp_list).mean(dim=0)\n",
    "        T = categroical_covariates_embeddings.shape[1]\n",
    "\n",
    "        embed_out = (categroical_covariates_embeddings + categroical_static_embeddings.repeat(1,T,1))/2\n",
    "        x = torch.concat((x_numeric,embed_out),dim=-1)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.forecast_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e26e0-ac62-453c-9276-c19cd3343429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91086e-9cdd-4d88-bf71-62ed6470ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 1000\n",
    "min_val_loss = 999\n",
    "\n",
    "num_blocks = 1\n",
    "embed_size = 500\n",
    "num_heads = 50\n",
    "batch_size = 128\n",
    "learning_rate = 3e-4\n",
    "time_shuffle = False\n",
    "drop_prob = 0.1\n",
    "\n",
    "model = transformer_forecaster(embed_size,num_heads,num_blocks).to(device)\n",
    "criterion = RMSLELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12ed87-771b-42f7-a9ee-97a5115fe748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bdfe23-13d1-4795-aee3-1d1c9490e722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac8990-793f-4404-b514-3b16f710eff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdab449-177b-4e11-8aa9-b2194c9748aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a625bef-83a2-4daf-8330-cb3a3a02b4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70c22d-5d11-4f04-bf33-210f5e77b600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b6ce0-983a-44bd-be19-7232877739f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2b789-a106-4b6f-8656-5e7aec4353d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d574b996-91fb-4825-8dce-79693793b45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d11e23-94be-4d1d-98c5-4de01432efb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd1144-fd1b-435c-bdd7-ebc8af3d0a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3b6ed-b2f2-452e-b553-60ccc7b69c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab01fe-19e6-40c3-96d2-b5e4d9700f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58144891-b5bf-4bf7-804b-6dab0d052862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754ffba-24f7-43b7-adfc-0c297d980e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d99324a-9524-4070-ab36-5b855411655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 100  # Batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6fdb85-ef38-4307-b4e7-f4b2654fa53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Poly Values Shape:\", train_poly_values.shape)\n",
    "print(\"Train Correction Vals Shape:\", train_correction_vals.shape)\n",
    "print(\"Test Poly Values Shape:\", test_poly_values.shape)\n",
    "print(\"Test Correction Vals Shape:\", test_correction_vals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d604787-13b6-4e16-b90c-7d904c06d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(\n",
    "    sequence: np.ndarray, ratio: float = 0.7\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Splits a sequence into 2 (3) parts, as is required by our transformer\n",
    "    model.\n",
    "\n",
    "    Assume our sequence length is L, we then split this into src of length N\n",
    "    and tgt_y of length M, with N + M = L.\n",
    "    src, the first part of the input sequence, is the input to the encoder, and we\n",
    "    expect the decoder to predict tgt_y, the second part of the input sequence.\n",
    "    In addition we generate tgt, which is tgt_y but \"shifted left\" by one - i.e. it\n",
    "    starts with the last token of src, and ends with the second-last token in tgt_y.\n",
    "    This sequence will be the input to the decoder.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        sequence: batched input sequences to split [bs, seq_len, num_features]\n",
    "        ratio: split ratio, N = ratio * L\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: src, tgt, tgt_y\n",
    "    \"\"\"\n",
    "    src_end = int(sequence.shape[1] * ratio)\n",
    "    # [bs, src_seq_len, num_features]\n",
    "    src = sequence[:, :src_end]\n",
    "    # [bs, tgt_seq_len, num_features]\n",
    "    tgt = sequence[:, src_end - 1 : -1]\n",
    "    # [bs, tgt_seq_len, num_features]\n",
    "    tgt_y = sequence[:, src_end:]\n",
    "\n",
    "    return src, tgt, tgt_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb3cea-22fa-4704-9ad9-9a1e6b8f9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html,\n",
    "# only modified to account for \"batch first\"\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "      \"\"\"\n",
    "      Args:\n",
    "        d_model (int): Dimension of the model (embedding dimension)\n",
    "        dropout (float, optional): Dropout probability. Default is 0.1.\n",
    "        max_len (int, optional): Maximum length of input sequences. Default is 5000.\n",
    "\n",
    "      Attributes:\n",
    "        pe (torch.Tensor): Positional encoding tensor. Shape: (1, max_len, d_model)\n",
    "\n",
    "      Returns:\n",
    "        torch.Tensor: input tensor with added positional encoding.\n",
    "\n",
    "      \"\"\"\n",
    "      super().__init__()\n",
    "      self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "      position = torch.arange(max_len).unsqueeze(1)                           # 1-D tensor from 0 to max_len -1. Unsqueeze \"adds\" a superficial 1 dim.\n",
    "      div_term = torch.exp(\n",
    "          torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "      )\n",
    "      pe = torch.zeros(1, max_len, d_model)\n",
    "      pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "      pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "      self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      \"\"\"Adds positional encoding to the given tensor.\n",
    "\n",
    "      Args:\n",
    "          x: tensor to add PE to [bs, seq_len, embed_dim]\n",
    "\n",
    "      Returns:\n",
    "          torch.Tensor: tensor with PE [bs, seq_len, embed_dim]\n",
    "      \"\"\"\n",
    "      x = x + self.pe[:, : x.size(1)]\n",
    "      return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e87b4-d097-4676-8f15-e88e25e843c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithPE(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim: int, out_dim: int, embed_dim: int, num_heads: int, num_layers: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a transformer model with positional encoding.\n",
    "\n",
    "        Args:\n",
    "            in_dim: number of input features\n",
    "            out_dim: number of features to predict\n",
    "            embed_dim: embed features to this dimension\n",
    "            num_heads: number of transformer heads\n",
    "            num_layers: number of encoder and decoder layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
    "\n",
    "        # transform input features into embedded features\n",
    "        self.encoder_embedding = torch.nn.Linear(\n",
    "            in_features=in_dim, out_features=embed_dim\n",
    "        )\n",
    "        self.decoder_embedding = torch.nn.Linear(\n",
    "            in_features=out_dim, out_features=embed_dim\n",
    "        )\n",
    "\n",
    "        # map output into output dimension\n",
    "        self.output_layer = torch.nn.Linear(in_features=embed_dim, out_features=out_dim)\n",
    "\n",
    "\n",
    "        self.transformer = torch.nn.Transformer(\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            d_model=embed_dim,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward function of the model.\n",
    "\n",
    "        Args:\n",
    "            src: input sequence to the encoder [bs, src_seq_len, num_features]\n",
    "            tgt: input sequence to the decoder [bs, tgt_seq_len, num_features]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: predicted sequence [bs, tgt_seq_len, feat_dim]\n",
    "        \"\"\"\n",
    "        # if self.train:\n",
    "        # Add noise to decoder inputs during training\n",
    "        # tgt = tgt + torch.normal(0, 0.1, size=tgt.shape).to(tgt.device)\n",
    "\n",
    "        # Embed encoder input and add positional encoding.\n",
    "        # [bs, src_seq_len, embed_dim]\n",
    "        src = self.encoder_embedding(src)\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        # Generate mask to avoid attention to future outputs.\n",
    "        # [tgt_seq_len, tgt_seq_len]\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(tgt.shape[1])\n",
    "        # Embed decoder input and add positional encoding.\n",
    "        # [bs, tgt_seq_len, embed_dim]\n",
    "        tgt = self.decoder_embedding(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        # Get prediction from transformer and map to output dimension.\n",
    "        # [bs, tgt_seq_len, embed_dim]\n",
    "        pred = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        pred = self.output_layer(pred)\n",
    "\n",
    "        return pred                                                             # return predicted sequence\n",
    "\n",
    "\n",
    "    def infer(self, src: torch.Tensor, tgt_len: int) -> torch.Tensor:\n",
    "        \"\"\"Runs inference with the model, meaning: predicts future values\n",
    "        for an unknown sequence.\n",
    "        For this, iteratively generate the next output token while\n",
    "        feeding the already generated ones as input sequence to the decoder.\n",
    "\n",
    "        Args:\n",
    "            src: input to the encoder [bs, src_seq_len, num_features]\n",
    "            tgt_len: desired length of the output\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: inferred sequence\n",
    "        \"\"\"\n",
    "        output = torch.zeros((src.shape[0], tgt_len + 1, src.shape[2])).to(src.device)\n",
    "        output[:, 0] = src[:, -1]\n",
    "        for i in range(tgt_len):\n",
    "            output[:, i + 1] = self.forward(src, output)[:, i]\n",
    "\n",
    "        return output[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7bc94-3713-4ab5-a390-fc27ace9dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_partition_data(\n",
    "    data_path: Path, seq_length: int = 100\n",
    ") -> tuple[np.ndarray, int]:\n",
    "    \"\"\"Loads the given data and paritions it into sequences of equal length.\n",
    "\n",
    "    Args:\n",
    "        data_path: path to the dataset\n",
    "        sequence_length: length of the generated sequences\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, int]: tuple of generated sequences and number of\n",
    "            features in dataset\n",
    "    \"\"\"\n",
    "    data = np.load(data_path)\n",
    "    num_features = len(data.keys())\n",
    "\n",
    "    # Check that each feature provides the same number of data points\n",
    "    data_lens = [len(data[key]) for key in data.keys()]\n",
    "    assert len(set(data_lens)) == 1\n",
    "\n",
    "    num_sequences = data_lens[0] // seq_length\n",
    "    sequences = np.empty((num_sequences, seq_length, num_features))\n",
    "\n",
    "    for i in range(0, num_sequences):\n",
    "        # [sequence_length, num_features]\n",
    "        sample = np.asarray(\n",
    "            [data[key][i * seq_length : (i + 1) * seq_length] for key in data.keys()]\n",
    "        ).swapaxes(0, 1)\n",
    "        sequences[i] = sample\n",
    "\n",
    "    return sequences, num_features\n",
    "\n",
    "\n",
    "def make_datasets(sequences: np.ndarray) -> tuple[TensorDataset, TensorDataset]:\n",
    "    \"\"\"Create train and test dataset.\n",
    "\n",
    "    Args:\n",
    "        sequences: sequences to use [num_sequences, sequence_length, num_features]\n",
    "\n",
    "    Returns:\n",
    "        tuple[TensorDataset, TensorDataset]: train and test dataset\n",
    "    \"\"\"\n",
    "    # Split sequences into train and test split\n",
    "    train, test = train_test_split(sequences, test_size=0.2)\n",
    "    return TensorDataset(torch.Tensor(train)), TensorDataset(torch.Tensor(test))\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    src: torch.Tensor,\n",
    "    tgt: torch.Tensor,\n",
    "    # 529 pred: torch.Tensor,\n",
    "    pred_infer: torch.Tensor,\n",
    "    idx=0,\n",
    ") -> None:\n",
    "    \"\"\"Visualizes a given sample including predictions.\n",
    "\n",
    "    Args:\n",
    "        src: source sequence [bs, src_seq_len, num_features]\n",
    "        tgt: target sequence [bs, tgt_seq_len, num_features]\n",
    "        pred: prediction of the model [bs, tgt_seq_len, num_features]\n",
    "        pred_infer: prediction obtained by running inference\n",
    "            [bs, tgt_seq_len, num_features]\n",
    "        idx: batch index to visualize\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(src.shape[1] + tgt.shape[1])\n",
    "    src_len = src.shape[1]\n",
    "\n",
    "    # 529 commented out\n",
    "    plt.plot(x[:src_len], src[idx].cpu().detach(), \"bo-\", label=\"src\")\n",
    "    plt.plot(x[src_len:], tgt[idx].cpu().detach(), \"go-\", label=\"tgt\")\n",
    "    # 529 plt.plot(x[src_len:], pred[idx].cpu().detach(), \"ro-\", label=\"pred\")\n",
    "    plt.plot(x[src_len:], pred_infer[idx].cpu().detach(), \"yo-\", label=\"pred_infer\")\n",
    "\n",
    "    # plt.plot(x[:src_len], scaler.inverse_transform(src[idx].cpu().detach()), \"bo-\", label=\"src\")\n",
    "    # plt.plot(x[src_len:], scaler.inverse_transform(tgt[idx].cpu().detach()), \"go-\", label=\"tgt\")\n",
    "    # # 529 plt.plot(x[src_len:], pred[idx].cpu().detach(), \"ro-\", label=\"pred\")\n",
    "    # plt.plot(x[src_len:], scaler.inverse_transform(pred_infer[idx].cpu().detach()), \"yo-\", label=\"pred_infer\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def split_sequence(\n",
    "    sequence: np.ndarray, ratio: float = 0.7\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Splits a sequence into 2 (3) parts, as is required by our transformer\n",
    "    model.\n",
    "\n",
    "    Assume our sequence length is L, we then split this into src of length N\n",
    "    and tgt_y of length M, with N + M = L.\n",
    "    src, the first part of the input sequence, is the input to the encoder, and we\n",
    "    expect the decoder to predict tgt_y, the second part of the input sequence.\n",
    "    In addition we generate tgt, which is tgt_y but \"shifted left\" by one - i.e. it\n",
    "    starts with the last token of src, and ends with the second-last token in tgt_y.\n",
    "    This sequence will be the input to the decoder.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequence: batched input sequences to split [bs, seq_len, num_features]\n",
    "        ratio: split ratio, N = ratio * L\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: src, tgt, tgt_y\n",
    "    \"\"\"\n",
    "    src_end = int(sequence.shape[1] * ratio)\n",
    "    # [bs, src_seq_len, num_features]\n",
    "    src = sequence[:, :src_end]\n",
    "    # [bs, tgt_seq_len, num_features]\n",
    "    tgt = sequence[:, src_end - 1 : -1]\n",
    "    # [bs, tgt_seq_len, num_features]\n",
    "    tgt_y = sequence[:, src_end:]\n",
    "\n",
    "    return src, tgt, tgt_y\n",
    "\n",
    "\n",
    "def move_to_device(device: torch.Tensor, *tensors: torch.Tensor) -> list[torch.Tensor]:\n",
    "    \"\"\"Move all given tensors to the given device.\n",
    "\n",
    "    Args:\n",
    "        device: device to move tensors to\n",
    "        tensors: tensors to move\n",
    "\n",
    "    Returns:\n",
    "        list[torch.Tensor]: moved tensors\n",
    "    \"\"\"\n",
    "    moved_tensors = []\n",
    "    for tensor in tensors:\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            moved_tensors.append(tensor.to(device))\n",
    "        else:\n",
    "            moved_tensors.append(tensor)\n",
    "    return moved_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06a048-360e-4221-9c94-87ce478dd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 100                                                                        # batch size\n",
    "FEATURE_DIM = 128                                                              # dimensionality of input features\n",
    "NUM_HEADS = 16                                                                  # number of attention heads in the multi-head attention mechanism\n",
    "NUM_EPOCHS = 5                                                                 # number of times entire dataset is passed for training\n",
    "NUM_VIS_EXAMPLES = 1\n",
    "NUM_LAYERS = 2                                                                # number of encoder and decoder layers in the mdel\n",
    "LR = 0.001    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcc3f7-82da-48b2-97e7-f23dd95cd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and generate train and test datasets / dataloaders\n",
    "sequences, num_features = load_and_partition_data(\"/Volumes/MARI/ssdl_gps/correction_data_2019_mo1_upd.npz\",500) # change data file name\n",
    "train_set, test_set = make_datasets(sequences)\n",
    "train_loader, test_loader = DataLoader(\n",
    "    train_set, batch_size=BS, shuffle=True\n",
    "), DataLoader(test_set, batch_size=BS, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b142c3f-bd11-4679-8ed4-c8ef73a77594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer and loss criterion\n",
    "model = TransformerWithPE(\n",
    "    num_features, num_features, FEATURE_DIM, NUM_HEADS, NUM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38944d7f-1c3f-4676-b8d3-786ccf085283",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "losses = []\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0                                                            # initialize epoch loss\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, tgt, tgt_y = split_sequence(batch[0])\n",
    "        src, tgt, tgt_y = move_to_device(device, src, tgt, tgt_y)\n",
    "        \n",
    "        # [bs, tgt_seq_len, num_features]\n",
    "        pred = model(src, tgt)\n",
    "        loss = criterion(pred, tgt_y)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_epoch_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: \"\n",
    "        f\"{(avg_epoch_loss):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa9f9b-fb42-44c8-90fa-d715b7c3e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dea57e-7840-47e3-8ff2-4a57c4f4e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "eval_loss = 0.0\n",
    "infer_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        src, tgt, tgt_y = split_sequence(batch[0])\n",
    "        src, tgt, tgt_y = move_to_device(device, src, tgt, tgt_y)\n",
    "\n",
    "        # [bs, tgt_seq_len, num_features]\n",
    "        pred = model(src, tgt)\n",
    "        loss = criterion(pred, tgt_y)\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        # Run inference with model\n",
    "        pred_infer = model.infer(src, tgt.shape[1])\n",
    "        loss_infer = criterion(pred_infer, tgt_y)\n",
    "        infer_loss += loss_infer.item()\n",
    "\n",
    "        if idx < NUM_VIS_EXAMPLES:\n",
    "            visualize(src, tgt, pred_infer)\n",
    "            # 529 visualize(src, tgt, pred, pred_infer)\n",
    "\n",
    "avg_eval_loss = eval_loss / len(test_loader)\n",
    "avg_infer_loss = infer_loss / len(test_loader)\n",
    "\n",
    "print(f\"Eval Loss on test set: {avg_eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2301936-e589-4f06-b941-96d008482373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ddc2f-bf4e-47f3-990d-4166bbc8337a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64359b-a212-4ad5-86c0-101b0d494087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87e95b-aa22-4fc6-997c-2eb51cfb5b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KmOszSjtoCf_",
   "metadata": {
    "id": "KmOszSjtoCf_"
   },
   "outputs": [],
   "source": [
    "def split_sequence(\n",
    "    sequence: np.ndarray, ratio: float = 0.7\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Splits a sequence into 2 (3) parts, as is required by our transformer\n",
    "    model.\n",
    "\n",
    "    Assume our sequence length is L, we then split this into src of length N\n",
    "    and tgt_y of length M, with N + M = L.\n",
    "    src, the first part of the input sequence, is the input to the encoder, and we\n",
    "    expect the decoder to predict tgt_y, the second part of the input sequence.\n",
    "    In addition we generate tgt, which is tgt_y but \"shifted left\" by one - i.e. it\n",
    "    starts with the last token of src, and ends with the second-last token in tgt_y.\n",
    "    This sequence will be the input to the decoder.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        sequence: batched input sequences to split [bs, seq_len, num_features]\n",
    "        ratio: split ratio, N = ratio * L\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: src, tgt, tgt_y\n",
    "    \"\"\"\n",
    "    src_end = int(sequence.shape[1] * ratio)\n",
    "    # [bs, src_seq_len, num_features]\n",
    "    src = sequence[:, :src_end]\n",
    "    # [bs, tgt_seq_len, num_features]\n",
    "    tgt = sequence[:, src_end - 1 : -1]\n",
    "    # [bs, tgt_seq_len, num_features]\n",
    "    tgt_y = sequence[:, src_end:]\n",
    "\n",
    "    return src, tgt, tgt_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a04e78",
   "metadata": {
    "id": "53a04e78"
   },
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html,\n",
    "# only modified to account for \"batch first\"\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "      \"\"\"\n",
    "      Args:\n",
    "        d_model (int): Dimension of the model (embedding dimension)\n",
    "        dropout (float, optional): Dropout probability. Default is 0.1.\n",
    "        max_len (int, optional): Maximum length of input sequences. Default is 5000.\n",
    "\n",
    "      Attributes:\n",
    "        pe (torch.Tensor): Positional encoding tensor. Shape: (1, max_len, d_model)\n",
    "\n",
    "      Returns:\n",
    "        torch.Tensor: input tensor with added positional encoding.\n",
    "\n",
    "      \"\"\"\n",
    "      super().__init__()\n",
    "      self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "      position = torch.arange(max_len).unsqueeze(1)                           # 1-D tensor from 0 to max_len -1. Unsqueeze \"adds\" a superficial 1 dim.\n",
    "      div_term = torch.exp(\n",
    "          torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "      )\n",
    "      pe = torch.zeros(1, max_len, d_model)\n",
    "      pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "      pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "      self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      \"\"\"Adds positional encoding to the given tensor.\n",
    "\n",
    "      Args:\n",
    "          x: tensor to add PE to [bs, seq_len, embed_dim]\n",
    "\n",
    "      Returns:\n",
    "          torch.Tensor: tensor with PE [bs, seq_len, embed_dim]\n",
    "      \"\"\"\n",
    "      x = x + self.pe[:, : x.size(1)]\n",
    "      return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kh3uyZwjCEkb",
   "metadata": {
    "id": "kh3uyZwjCEkb"
   },
   "outputs": [],
   "source": [
    "class TransformerWithPE(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim: int, out_dim: int, embed_dim: int, num_heads: int, num_layers: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a transformer model with positional encoding.\n",
    "\n",
    "        Args:\n",
    "            in_dim: number of input features\n",
    "            out_dim: number of features to predict\n",
    "            embed_dim: embed features to this dimension\n",
    "            num_heads: number of transformer heads\n",
    "            num_layers: number of encoder and decoder layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
    "\n",
    "        # transform input features into embedded features\n",
    "        self.encoder_embedding = torch.nn.Linear(\n",
    "            in_features=in_dim, out_features=embed_dim\n",
    "        )\n",
    "        self.decoder_embedding = torch.nn.Linear(\n",
    "            in_features=out_dim, out_features=embed_dim\n",
    "        )\n",
    "\n",
    "        # map output into output dimension\n",
    "        self.output_layer = torch.nn.Linear(in_features=embed_dim, out_features=out_dim)\n",
    "\n",
    "\n",
    "        self.transformer = torch.nn.Transformer(\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            d_model=embed_dim,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward function of the model.\n",
    "\n",
    "        Args:\n",
    "            src: input sequence to the encoder [bs, src_seq_len, num_features]\n",
    "            tgt: input sequence to the decoder [bs, tgt_seq_len, num_features]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: predicted sequence [bs, tgt_seq_len, feat_dim]\n",
    "        \"\"\"\n",
    "        # if self.train:\n",
    "        # Add noise to decoder inputs during training\n",
    "        # tgt = tgt + torch.normal(0, 0.1, size=tgt.shape).to(tgt.device)\n",
    "\n",
    "        # Embed encoder input and add positional encoding.\n",
    "        # [bs, src_seq_len, embed_dim]\n",
    "        src = self.encoder_embedding(src)\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        # Generate mask to avoid attention to future outputs.\n",
    "        # [tgt_seq_len, tgt_seq_len]\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(tgt.shape[1])\n",
    "        # Embed decoder input and add positional encoding.\n",
    "        # [bs, tgt_seq_len, embed_dim]\n",
    "        tgt = self.decoder_embedding(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        # Get prediction from transformer and map to output dimension.\n",
    "        # [bs, tgt_seq_len, embed_dim]\n",
    "        pred = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        pred = self.output_layer(pred)\n",
    "\n",
    "        return pred                                                             # return predicted sequence\n",
    "\n",
    "\n",
    "    def infer(self, src: torch.Tensor, tgt_len: int) -> torch.Tensor:\n",
    "        \"\"\"Runs inference with the model, meaning: predicts future values\n",
    "        for an unknown sequence.\n",
    "        For this, iteratively generate the next output token while\n",
    "        feeding the already generated ones as input sequence to the decoder.\n",
    "\n",
    "        Args:\n",
    "            src: input to the encoder [bs, src_seq_len, num_features]\n",
    "            tgt_len: desired length of the output\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: inferred sequence\n",
    "        \"\"\"\n",
    "        output = torch.zeros((src.shape[0], tgt_len + 1, src.shape[2])).to(src.device)\n",
    "        output[:, 0] = src[:, -1]\n",
    "        for i in range(tgt_len):\n",
    "            output[:, i + 1] = self.forward(src, output)[:, i]\n",
    "\n",
    "        return output[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951ac2c",
   "metadata": {
    "id": "8951ac2c"
   },
   "outputs": [],
   "source": [
    "def load_and_partition_data(\n",
    "    data_path: Path, seq_length: int = 100\n",
    ") -> tuple[np.ndarray, int]:\n",
    "    \"\"\"Loads the given data and paritions it into sequences of equal length.\n",
    "\n",
    "    Args:\n",
    "        data_path: path to the dataset\n",
    "        sequence_length: length of the generated sequences\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, int]: tuple of generated sequences and number of\n",
    "            features in dataset\n",
    "    \"\"\"\n",
    "    data = np.load(data_path)\n",
    "    num_features = len(data.keys())\n",
    "\n",
    "    # Check that each feature provides the same number of data points\n",
    "    data_lens = [len(data[key]) for key in data.keys()]\n",
    "    assert len(set(data_lens)) == 1\n",
    "\n",
    "    num_sequences = data_lens[0] // seq_length\n",
    "    sequences = np.empty((num_sequences, seq_length, num_features))\n",
    "\n",
    "    for i in range(0, num_sequences):\n",
    "        # [sequence_length, num_features]\n",
    "        sample = np.asarray(\n",
    "            [data[key][i * seq_length : (i + 1) * seq_length] for key in data.keys()]\n",
    "        ).swapaxes(0, 1)\n",
    "        sequences[i] = sample\n",
    "\n",
    "    return sequences, num_features\n",
    "\n",
    "\n",
    "def make_datasets(sequences: np.ndarray) -> tuple[TensorDataset, TensorDataset]:\n",
    "    \"\"\"Create train and test dataset.\n",
    "\n",
    "    Args:\n",
    "        sequences: sequences to use [num_sequences, sequence_length, num_features]\n",
    "\n",
    "    Returns:\n",
    "        tuple[TensorDataset, TensorDataset]: train and test dataset\n",
    "    \"\"\"\n",
    "    # Split sequences into train and test split\n",
    "    train, test = train_test_split(sequences, test_size=0.2)\n",
    "    return TensorDataset(torch.Tensor(train)), TensorDataset(torch.Tensor(test))\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    src: torch.Tensor,\n",
    "    tgt: torch.Tensor,\n",
    "    # 529 pred: torch.Tensor,\n",
    "    pred_infer: torch.Tensor,\n",
    "    idx=0,\n",
    ") -> None:\n",
    "    \"\"\"Visualizes a given sample including predictions.\n",
    "\n",
    "    Args:\n",
    "        src: source sequence [bs, src_seq_len, num_features]\n",
    "        tgt: target sequence [bs, tgt_seq_len, num_features]\n",
    "        pred: prediction of the model [bs, tgt_seq_len, num_features]\n",
    "        pred_infer: prediction obtained by running inference\n",
    "            [bs, tgt_seq_len, num_features]\n",
    "        idx: batch index to visualize\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(src.shape[1] + tgt.shape[1])\n",
    "    src_len = src.shape[1]\n",
    "\n",
    "    # 529 commented out\n",
    "    # plt.plot(x[:src_len], src[idx].cpu().detach(), \"bo-\", label=\"src\")\n",
    "    # plt.plot(x[src_len:], tgt[idx].cpu().detach(), \"go-\", label=\"tgt\")\n",
    "    # # 529 plt.plot(x[src_len:], pred[idx].cpu().detach(), \"ro-\", label=\"pred\")\n",
    "    # plt.plot(x[src_len:], pred_infer[idx].cpu().detach(), \"yo-\", label=\"pred_infer\")\n",
    "\n",
    "    plt.plot(x[:src_len], scaler.inverse_transform(src[idx].cpu().detach()), \"bo-\", label=\"src\")\n",
    "    plt.plot(x[src_len:], scaler.inverse_transform(tgt[idx].cpu().detach()), \"go-\", label=\"tgt\")\n",
    "    # 529 plt.plot(x[src_len:], pred[idx].cpu().detach(), \"ro-\", label=\"pred\")\n",
    "    plt.plot(x[src_len:], scaler.inverse_transform(pred_infer[idx].cpu().detach()), \"yo-\", label=\"pred_infer\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def split_sequence(\n",
    "    sequence: np.ndarray, ratio: float = 0.7\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Splits a sequence into 2 (3) parts, as is required by our transformer\n",
    "    model.\n",
    "\n",
    "    Assume our sequence length is L, we then split this into src of length N\n",
    "    and tgt_y of length M, with N + M = L.\n",
    "    src, the first part of the input sequence, is the input to the encoder, and we\n",
    "    expect the decoder to predict tgt_y, the second part of the input sequence.\n",
    "    In addition we generate tgt, which is tgt_y but \"shifted left\" by one - i.e. it\n",
    "    starts with the last token of src, and ends with the second-last token in tgt_y.\n",
    "    This sequence will be the input to the decoder.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequence: batched input sequences to split [bs, seq_len, num_features]\n",
    "        ratio: split ratio, N = ratio * L\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: src, tgt, tgt_y\n",
    "    \"\"\"\n",
    "    src_end = int(sequence.shape[1] * ratio)\n",
    "    # [bs, src_seq_len, num_features]\n",
    "    src = sequence[:, :src_end]\n",
    "    # [bs, tgt_seq_len, num_features]\n",
    "    tgt = sequence[:, src_end - 1 : -1]\n",
    "    # [bs, tgt_seq_len, num_features]\n",
    "    tgt_y = sequence[:, src_end:]\n",
    "\n",
    "    return src, tgt, tgt_y\n",
    "\n",
    "\n",
    "def move_to_device(device: torch.Tensor, *tensors: torch.Tensor) -> list[torch.Tensor]:\n",
    "    \"\"\"Move all given tensors to the given device.\n",
    "\n",
    "    Args:\n",
    "        device: device to move tensors to\n",
    "        tensors: tensors to move\n",
    "\n",
    "    Returns:\n",
    "        list[torch.Tensor]: moved tensors\n",
    "    \"\"\"\n",
    "    moved_tensors = []\n",
    "    for tensor in tensors:\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            moved_tensors.append(tensor.to(device))\n",
    "        else:\n",
    "            moved_tensors.append(tensor)\n",
    "    return moved_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d252722",
   "metadata": {
    "id": "6d252722"
   },
   "outputs": [],
   "source": [
    "BS = 10                                                                        # batch size\n",
    "FEATURE_DIM = 10                                                              # dimensionality of input features\n",
    "NUM_HEADS = 10                                                                  # number of attention heads in the multi-head attention mechanism\n",
    "NUM_EPOCHS = 50                                                                 # number of times entire dataset is passed for training\n",
    "NUM_VIS_EXAMPLES = 1\n",
    "NUM_LAYERS = 2                                                                # number of encoder and decoder layers in the mdel\n",
    "LR = 0.001                                                                     # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946e68f",
   "metadata": {
    "id": "9946e68f"
   },
   "outputs": [],
   "source": [
    "# # Load data and generate train and test datasets / dataloaders\n",
    "# sequences, num_features = load_and_partition_data(\"short_file.npz\",500) # change data file name\n",
    "# train_set, test_set = make_datasets(sequences)\n",
    "# train_loader, test_loader = DataLoader(\n",
    "#     train_set, batch_size=BS, shuffle=True\n",
    "# ), DataLoader(test_set, batch_size=BS, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c97419",
   "metadata": {
    "id": "c0c97419"
   },
   "outputs": [],
   "source": [
    "# Initialize model, optimizer and loss criterion\n",
    "num_features = 1\n",
    "model = TransformerWithPE(\n",
    "    num_features, num_features, FEATURE_DIM, NUM_HEADS, NUM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79659e2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79659e2e",
    "outputId": "41e9c7e1-09ff-48f2-87b2-490e5bb37476"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "losses = []\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0   # initialize epoch loss\n",
    "    model.train() # added this based on gpt rec\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # [bs, tgt_seq_len, num_features]\n",
    "        pred = model(src)  # Using src for both src and tgt here\n",
    "\n",
    "        # assert pred.shape == tgt.shape, f\"Shape mismatch: pred {pred.shape} vs tgt {tgt.shape}\"\n",
    "\n",
    "        # loss = criterion(pred, tgt)\n",
    "        \n",
    "    #     epoch_loss += loss.item()\n",
    "\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    #     losses.append(avg_epoch_loss)\n",
    "\n",
    "    # print(\n",
    "    #     f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: \"\n",
    "    #     f\"{(avg_epoch_loss):.4f}\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd594303-9aa7-4417-b129-9613c3965325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb966a23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "eb966a23",
    "outputId": "45a3b5bf-0942-4b7f-efc3-232831b0fafb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "eval_loss = 0.0\n",
    "infer_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        src, tgt, tgt_y = split_sequence(batch[0])\n",
    "        src, tgt, tgt_y = move_to_device(device, src, tgt, tgt_y)\n",
    "\n",
    "        # [bs, tgt_seq_len, num_features]\n",
    "        pred = model(src, tgt)\n",
    "        loss = criterion(pred, tgt_y)\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        # Run inference with model\n",
    "        pred_infer = model.infer(src, tgt.shape[1])\n",
    "        loss_infer = criterion(pred_infer, tgt_y)\n",
    "        infer_loss += loss_infer.item()\n",
    "\n",
    "        if idx < NUM_VIS_EXAMPLES:\n",
    "            visualize(src, tgt, pred_infer)\n",
    "            # 529 visualize(src, tgt, pred, pred_infer)\n",
    "\n",
    "avg_eval_loss = eval_loss / len(test_loader)\n",
    "avg_infer_loss = infer_loss / len(test_loader)\n",
    "\n",
    "print(f\"Eval Loss on test set: {avg_eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd1334-b921-4d31-8146-c0a2bcb9e40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
