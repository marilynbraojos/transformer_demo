{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe5ae8b-a683-4441-ad75-cf0e7c639d48",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232193c5-5986-463f-9ea6-901b88ba1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1924123-8019-4f8e-a2d2-91bb64860364",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Set History and Projection Data, and Months of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d46415-526b-4ac9-bcb9-0c1694bd340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each time step is roughly 30 seconds - not all (1 hr = 120, 2 hrs = 240, 12 hrs = 1440, 24 hrs = 2880)\n",
    "\n",
    "timesteps = 2880 # history \n",
    "corr_timestep = 240 # projection\n",
    "\n",
    "sel_mon = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cf97b-d20a-4a13-871e-c48e41a155ca",
   "metadata": {},
   "source": [
    "# Shaping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a6ec74-9a67-430b-93bc-115596344f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Volumes/MARI/ssdl_gps/correction_data/2018_2019/scaled_continuous_unique_correction_data_2018_2019_jan1_dec31_clipped_015_9985.npz')\n",
    "epochs = data['matching_epochs']\n",
    "final_clock_bias = data['matching_clock_bias']\n",
    "broadcast_clock_bias = data['matching_poly_values']\n",
    "correction_value = data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f76f19f-d588-4e0c-b793-da78e18e5aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit data considered\n",
    "\n",
    "no_mon = 24 # number of months considered (user defined)\n",
    "\n",
    "mon_len = len(epochs)/24\n",
    "tot_len = int(np.floor(no_mon * mon_len))\n",
    "\n",
    "epochs = epochs[:tot_len]\n",
    "final_clock_bias = final_clock_bias[:tot_len]\n",
    "broadcast_clock_bias = broadcast_clock_bias[:tot_len]\n",
    "correction_value = correction_value[:tot_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9cb92b-04dc-45a8-9f94-39d018efd5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100720\n",
      "2100720\n",
      "2100720\n",
      "2100720\n",
      "2100720\n"
     ]
    }
   ],
   "source": [
    "print(tot_len)\n",
    "print(len(epochs))\n",
    "print(len(final_clock_bias))\n",
    "print(len(broadcast_clock_bias))\n",
    "print(len(correction_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53616f7e-a8a2-4594-9f67-ddbab3907276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01 02:00:00\n",
      "2019-12-31 11:59:30\n"
     ]
    }
   ],
   "source": [
    "time_datetimes = [datetime.strptime(ts, '%Y:%m:%d:%H:%M:%S') for ts in epochs]\n",
    "print(min(time_datetimes))\n",
    "print(max(time_datetimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f894be5-b8d4-484f-a4a0-b839114e293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [] # input\n",
    "y = [] # labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55197631-0fb5-4990-bfcc-8fb04334447f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100720"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(broadcast_clock_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7720ee06-ff16-4121-9d34-3ee5952c35b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(2880, 2100720)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(timesteps,len(broadcast_clock_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a29eca-47f7-4b43-ab5f-10932144f0a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(timesteps,len(broadcast_clock_bias)-corr_timestep):\n",
    "    history = broadcast_clock_bias[i-timesteps:i]\n",
    "    correction = correction_value[i:i+corr_timestep]\n",
    "    print(correction)\n",
    "    x.append(history)\n",
    "    y.append(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7241b9-fddd-42f3-ad45-38ea803f146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c9e25-d086-497c-a86a-21938b82febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efa856-7e45-496f-acab-7c5b8ed7297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f46b91-7eb8-4004-9679-f010ee49714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84d825-5886-4625-b8a0-6c26cd8f23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5b100-d152-42f1-baa0-bef30265065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp, x_test, y_temp, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69b6c8-8dd4-4de6-b5a7-9a04cd64e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of X_temp: {x_temp.shape}')\n",
    "print(f'Shape of X_test: {x_test.shape}')\n",
    "print(f'Shape of y_temp: {y_temp.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da05623b-fe03-47da-ac7e-2dd2a4d7fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, train_size=7/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f00c97-3dad-4ede-bcee-78b74a32ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of X_train: {x_train.shape}')\n",
    "print(f'Shape of X_val: {x_val.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of y_val: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead3482-17c1-424d-b290-96a54efacb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train,y_train)\n",
    "val_dataset = TensorDataset(x_val,y_val)\n",
    "test_dataset = TensorDataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a316d-3efe-4a01-8c61-3495340e1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, f'/Volumes/MARI/ssdl_gps/torch_datasets/mo{no_mon}_train_dataset_step{timesteps}_corr{corr_timestep}.pt')\n",
    "torch.save(val_dataset, f'/Volumes/MARI/ssdl_gps/torch_datasets/mo{no_mon}_val_dataset_step{timesteps}_corr{corr_timestep}.pt')\n",
    "torch.save(test_dataset, f'/Volumes/MARI/ssdl_gps/torch_datasets/mo{no_mon}_test_dataset_step{timesteps}_corr{corr_timestep}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca599be-7fe3-4ffd-8933-94bcbf7c6800",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fdf32-b766-4688-92a6-ba27d77c0786",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.load(f'/Volumes/MARI/ssdl_gps/torch_datasets/mo{sel_mon}_train_dataset_step{timesteps}_corr{corr_timestep}.pt')\n",
    "val = torch.load(f'/Volumes/MARI/ssdl_gps/torch_datasets/mo{sel_mon}_val_dataset_step{timesteps}_corr{corr_timestep}.pt')\n",
    "test = torch.load(f'/Volumes/MARI/ssdl_gps/torch_datasets/mo{sel_mon}_test_dataset_step{timesteps}_corr{corr_timestep}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de01c8-8453-44c6-95bb-713ad7d761bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val[0][0].shape)\n",
    "print(val[0][1].shape)\n",
    "print(val[1][1].shape)\n",
    "print(len(val))\n",
    "print(len(val[1]))\n",
    "print(len(val[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e4ac7-a231-4cf6-a459-06ea37df9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "BS = 64\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size = BS, shuffle = True)\n",
    "val_dataloader = DataLoader(val, batch_size = BS, shuffle = False)\n",
    "test_dataloader = DataLoader(test, batch_size = BS, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65b9e0-7be2-4a50-bbdc-fdd80940da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((len(train_dataloader)))\n",
    "print((len(val_dataloader)))\n",
    "print((len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695055d4-98d0-49e9-a3aa-7b880fc21e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b81e2-5411-4793-bf45-2182acb695d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, output_dim):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x.squeeze()  # Ensure the output has the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e471f3-1c17-49bb-b469-de935ec1b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        return torch.sqrt(self.mse(yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d2594-f95a-434e-97e6-eec1b47fd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = timesteps  # Size of each input sample (timesteps)\n",
    "d_model = 128  # Embedding dimension\n",
    "nhead = 4  # Number of attention heads (Syam: 2)\n",
    "num_layers = 12  # Number of transformer layers (Syam: 1)\n",
    "dim_feedforward = 20  # Dimension of the feedforward network (hidden units)\n",
    "dropout = 0.1\n",
    "output_dim = corr_timestep  # Size of the output (correction value 240= 2 hours; was 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c9c29-b476-4fee-a73b-66c50d03836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "model = TransformerEncoderModel(input_dim, d_model, nhead, num_layers, dim_feedforward, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c7410-7d6e-49df-8490-182b48a77111",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc2220-cf17-48b9-9e1f-2826b58a58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_mae = []\n",
    "    val_mae = []\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_train_preds = []\n",
    "        all_train_actuals = []\n",
    "\n",
    "        \n",
    "        for X_train, y_train in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            all_train_preds.extend(outputs.detach().cpu().numpy())\n",
    "            all_train_actuals.extend(y_train.detach().cpu().numpy())\n",
    "\n",
    "        \n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        train_mae_val = mean_absolute_error(all_train_actuals, all_train_preds)\n",
    "        train_mae.append(train_mae_val)\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_actuals = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_dataloader:\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                all_val_preds.extend(outputs.detach().cpu().numpy())\n",
    "                all_val_actuals.extend(y_val.detach().cpu().numpy())\n",
    "\n",
    "        \n",
    "        val_loss = running_loss / len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        val_mae_val = mean_absolute_error(all_val_actuals, all_val_preds)\n",
    "        val_mae.append(val_mae_val)\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, train_mae, val_losses, val_mae\n",
    "\n",
    "num_epochs = EPOCHS\n",
    "train_losses, train_mae, val_losses, val_mae = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.plot(range(num_epochs), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(num_epochs), train_mae, label='Train MAE')\n",
    "plt.plot(range(num_epochs), val_mae, label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd436e19-fd4b-41c9-80d4-8fbb530c956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def evaluate_model(model, test_dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_dataloader:\n",
    "            outputs = model(X_test)\n",
    "            loss = criterion(outputs, y_test)\n",
    "            running_loss += loss.item()\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(y_test.cpu().numpy())\n",
    "    \n",
    "    test_loss = running_loss / len(test_dataloader)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "\n",
    "    return test_loss, mae, predictions, actuals\n",
    "\n",
    "test_loss, mae, predictions, actuals = evaluate_model(model, test_dataloader, criterion)\n",
    "print(f'Test RMSE Loss: {test_loss:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56d538-69bf-4f72-8a6f-6e2cbd37d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(actuals))\n",
    "print(actuals[-1].shape)\n",
    "print(actuals[2].shape)\n",
    "print(predictions[0].shape)\n",
    "print(len(predictions))\n",
    "\n",
    "# Scatter plot of actual and predicted values on the same axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(actuals[-1])), actuals[-1], alpha=0.5, label='Actual Values')\n",
    "plt.scatter(range(len(predictions[-1])), predictions[-1], alpha=0.5, label='Predicted Values')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Scatter Plot of Actual and Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions vs actuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(actuals[-1], predictions[-1], alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Scatter Plot of Actual vs Predicted Values')\n",
    "plt.plot([min(actuals[-1]), max(actuals[-1])], [min(actuals[-1]), max(actuals[-1])], 'r--')  # Add a diagonal line\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "actuals_array = np.array(actuals)\n",
    "predictions_array = np.array(predictions)\n",
    "\n",
    "# Calculate R² score\n",
    "r2 = r2_score(actuals_array, predictions_array)\n",
    "print(f'R² Score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c5a6cc-e0d9-47e9-a12b-0828a36f62da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d6c83-0dc9-4897-823c-fd87e287ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = np.load('/Volumes/MARI/ssdl_gps/correction_data/unique_correction_data_2019_minmaxscale_postoutlier.npz')\n",
    "og_final_clock_bias = og_data['matching_clock_bias']\n",
    "og_broadcast_clock_bias = og_data['matching_poly_values']\n",
    "og_correction_value = og_data['correction_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0dfad9-5b7d-4d1d-a3c3-3e560771cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_final_clock_bias = og_final_clock_bias[:tot_len]\n",
    "og_broadcast_clock_bias = og_broadcast_clock_bias[:tot_len]\n",
    "og_correction_value = og_correction_value[:tot_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21245a5-315b-4cfd-8f77-3f2ea1d4f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_min = min(og_correction_value)\n",
    "original_max = max(og_correction_value)\n",
    "\n",
    "re_scaler = MinMaxScaler(feature_range=(original_min, original_max))\n",
    "\n",
    "predictions_reshaped = predictions_array.reshape(-1, 1)\n",
    "actuals_reshaped = actuals_array.reshape(-1, 1)\n",
    "re_scaler.fit(np.vstack([predictions_reshaped, actuals_reshaped]))\n",
    "\n",
    "predictions_rescaled = re_scaler.transform(predictions_reshaped).reshape(predictions_array.shape)\n",
    "actuals_rescaled = re_scaler.transform(actuals_reshaped).reshape(actuals_array.shape)\n",
    "\n",
    "mae_rescaled = mean_absolute_error(actuals_rescaled, predictions_rescaled)\n",
    "rmse_rescaled =  np.sqrt(mean_squared_error(actuals_rescaled, predictions_rescaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf9e42-7489-47bc-9432-9242a576871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE (ns):\", mae_rescaled/1e-9)\n",
    "print(\"RMSE (ns):\", rmse_rescaled/1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f558c89-f512-412e-8e0a-d49255270abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max Actuals (s):\", np.max(actuals_reshaped))\n",
    "print(\"Max Predictions (s):\", np.max(predictions_reshaped))\n",
    "print(\"Min Actuals (s):\", np.min(actuals_reshaped))\n",
    "print(\"Min Predictions (s):\", np.min(predictions_reshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72d173-1451-411c-9d49-70f4eb266ba2",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bef859-3108-474f-9b81-daac2669b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import ParameterGrid\n",
    "# # Define a parameter grid\n",
    "# # param_grid = {\n",
    "# #     'learning_rate': [0.0001, 0.001],\n",
    "# #     'batch_size': [32, 64],\n",
    "# #     'd_model': [8, 16],\n",
    "# #     'nhead': [2, 4],\n",
    "# #     'num_layers': [1, 2],\n",
    "# #     'dim_feedforward': [10, 20],\n",
    "# #     'num_epochs': [5, 10]\n",
    "# # }\n",
    "\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.001],\n",
    "#     'batch_size': [64],\n",
    "#     'd_model': [8],\n",
    "#     'nhead': [2],\n",
    "#     'num_layers': [1],\n",
    "#     'dim_feedforward': [10],\n",
    "#     'num_epochs': [10]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f8bb2-c012-4e79-bd9d-6edcc9067f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40efd1-98da-4921-a1d0-341c101d8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_r2 = float('-inf')\n",
    "# best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c3e63-95a7-44ab-b339-3ae3de9261c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for params in grid:\n",
    "#     print(f'Trying params: {params}')\n",
    "    \n",
    "#     # Create DataLoader with the current batch size\n",
    "#     train_dataloader = DataLoader(train, batch_size=params['batch_size'], shuffle=True)\n",
    "#     val_dataloader = DataLoader(val, batch_size=params['batch_size'], shuffle=False)\n",
    "#     test_dataloader = DataLoader(test, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "#     # Initialize the model with current hyperparameters\n",
    "#     model = TransformerEncoderModel(input_dim, \n",
    "#                                     params['d_model'], \n",
    "#                                     params['nhead'], \n",
    "#                                     params['num_layers'], \n",
    "#                                     params['dim_feedforward'], \n",
    "#                                     output_dim)\n",
    "    \n",
    "#     # Define the optimizer with the current learning rate\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    \n",
    "#     # Train the model\n",
    "#     train_losses, val_losses = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, params['num_epochs'])\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     test_loss, predictions, actuals = evaluate_model(model, test_dataloader, criterion)\n",
    "    \n",
    "#     # Calculate R² score\n",
    "#     actuals_array = np.array(actuals)\n",
    "#     predictions_array = np.array(predictions)\n",
    "#     r2 = r2_score(actuals_array, predictions_array)\n",
    "    \n",
    "#     print(f'R² Score: {r2:.4f}')\n",
    "    \n",
    "#     # Check if the current model is the best so far\n",
    "#     if r2 > best_r2:\n",
    "#         best_r2 = r2\n",
    "#         best_params = params\n",
    "\n",
    "# print(f'Best R² Score: {best_r2:.4f}')\n",
    "# print(f'Best Parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ce1f1-958e-4ef5-b04f-4c18f701affa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06e908-f2ca-449c-a619-223dd688378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# from ray import tune\n",
    "# from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe13b30-e8f9-412c-8ba8-f23e5b983598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_transformer(config, checkpoint_dir=None):\n",
    "#     # Load datasets\n",
    "#     sel_mon = 12\n",
    "#     train = torch.load(f'/Volumes/MARI/ssdl_gps/torch_datasets/mo{sel_mon}_train_dataset.pt')\n",
    "#     val = torch.load(f'/Volumes/MARI/ssdl_gps/torch_datasets/mo{sel_mon}_val_dataset.pt')\n",
    "#     test = torch.load(f'/Volumes/MARI/ssdl_gps/torch_datasets/mo{sel_mon}_test_dataset.pt')\n",
    "\n",
    "#     train_dataloader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "#     val_dataloader = DataLoader(val, batch_size=64, shuffle=False)\n",
    "#     test_dataloader = DataLoader(test, batch_size=64, shuffle=False)\n",
    "\n",
    "#     # Initialize model, criterion, optimizer\n",
    "#     model = TransformerEncoderModel(\n",
    "#         input_dim=240,\n",
    "#         d_model=config[\"d_model\"],\n",
    "#         nhead=config[\"nhead\"],\n",
    "#         num_layers=config[\"num_layers\"],\n",
    "#         dim_feedforward=config[\"dim_feedforward\"],\n",
    "#         output_dim=1\n",
    "#     )\n",
    "    \n",
    "#     criterion = RMSELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "#     if checkpoint_dir:\n",
    "#         checkpoint = torch.load(os.path.join(checkpoint_dir, \"checkpoint.pth\"))\n",
    "#         model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "#         optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(10):  # You can make this configurable\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for X_train, y_train in train_dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(X_train)\n",
    "#             loss = criterion(outputs, y_train)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         train_loss = running_loss / len(train_dataloader)\n",
    "        \n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for X_val, y_val in val_dataloader:\n",
    "#                 outputs = model(X_val)\n",
    "#                 loss = criterion(outputs, y_val)\n",
    "#                 val_loss += loss.item()\n",
    "        \n",
    "#         val_loss /= len(val_dataloader)\n",
    "        \n",
    "#         tune.report(loss=val_loss)\n",
    "        \n",
    "#         if epoch % 5 == 0:\n",
    "#             with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "#                 path = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n",
    "#                 torch.save({\n",
    "#                     \"model_state_dict\": model.state_dict(),\n",
    "#                     \"optimizer_state_dict\": optimizer.state_dict()\n",
    "#                 }, path)\n",
    "\n",
    "# # Define search space\n",
    "# config = {\n",
    "#     \"d_model\": tune.choice([8, 16, 32]),\n",
    "#     \"nhead\": tune.choice([2, 4]),\n",
    "#     \"num_layers\": tune.choice([2, 4]),\n",
    "#     \"dim_feedforward\": tune.choice([10, 20, 50]),\n",
    "#     \"lr\": tune.loguniform(1e-4, 1e-2)\n",
    "# }\n",
    "\n",
    "# # Run hyperparameter tuning\n",
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"loss\",\n",
    "#     mode=\"min\",\n",
    "#     max_t=10,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )\n",
    "\n",
    "# analysis = tune.run(\n",
    "#     train_transformer,\n",
    "#     resources_per_trial={\"cpu\": 2},\n",
    "#     config=config,\n",
    "#     num_samples=10,\n",
    "#     scheduler=scheduler\n",
    "# )\n",
    "\n",
    "# print(\"Best config: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a963aef-3f9b-4054-8d4b-1593f309c085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
