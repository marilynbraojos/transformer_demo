{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marilynbraojos/transformer_demo/blob/main/gpt_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit: Andrej Karpathy\n",
        "\n",
        "*Note: This is not exactly what Andrej Karpathy released and I've modified the way the notebook looks so that it can more closely match with my application of a transformer model (i.e. adding detailed comments, adjusting parameters, adding notes to refer to at a later time, plotting).* However, he was the OG creator of this idea and code, therefore he deserves credit."
      ],
      "metadata": {
        "id": "yO4uEH0I_Iqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Characterizing Data\n"
      ],
      "metadata": {
        "id": "61ahdGHGC4P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this model, we will be using a toy dataset called Tiny Shakespeare. This dataset contains 40,000 lines of Shakespeare from a variety of his plays. The Tiny Shakespeare dataset has a manageable size, and the complexity of Shakespeare's language."
      ],
      "metadata": {
        "id": "q7hyz7pYKBmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the dataset used for training using the \"web get\" command !wget\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# Count the number of lines in input.txt - Confirm Number of Lines\n",
        "!wc -l input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UrJnhSQJ1hG",
        "outputId": "ab6e8030-ca17-403b-96ef-1dd154b45fd7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-09 17:22:08--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.9’\n",
            "\n",
            "\rinput.txt.9           0%[                    ]       0  --.-KB/s               \rinput.txt.9         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-04-09 17:22:09 (21.3 MB/s) - ‘input.txt.9’ saved [1115394/1115394]\n",
            "\n",
            "40000 input.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be modeling how these characters follow one another. The transformer model will try to produce character sequences that look like this following the patterns in this data. Once the model is trained, we will generate Shakespeare-like writing. In contrast to ChatGPT, our model will be producing writing on a character-by-character level - whereas ChatGPT produces this in a token-by-token level, where their tokens were \"sub-word\" pieces."
      ],
      "metadata": {
        "id": "NRhOKwnbNLna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in text as a string in to inspect it\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:                             # open input.txt in read mode, 'r'\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "QcxXVULJJ1ez"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows that we are working with ≈ 1M characters:"
      ],
      "metadata": {
        "id": "1DLPqlE3Ts6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quRt5xw8J1ZU",
        "outputId": "b4164ba8-6d0c-4fee-b94b-7a12f6be9e43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below,\n",
        "\n",
        "\n",
        "```\n",
        "set(text)                   % contains unique elements (characters) from the \"text\"\n",
        "\n",
        "list(set(text))             % converts the set into a list (preserves order of the elements)\n",
        "\n",
        "sorted(list(set(text)))     % sorts list in ascending order (default)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OL0LKnOEnyC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # here are all the unique characters that occur in this text\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)                                                         # return numer of character elements\n",
        "print(''.join(chars))                                                           # join the characters from 'chars' into a single string without a separator between them, concatenating them into one string\n",
        "print(vocab_size)                                                               # possible elements/characters of our sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBMiJgvrJ1Qb",
        "outputId": "7afb26ca-d146-46f7-8840-2be062c6f64b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Tokenizing Input Text"
      ],
      "metadata": {
        "id": "rv9CxKjMrUFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization breaks a stream of data into a meaningful element (words, terms, sentences, symbols, etc). It breaks unstructured data into chunks of discrete elements. In this case, it involves converting the raw text as a string to a sequence of integers according to some vocabulary of possible elements. In our case, we're doing this at the character level because we are creating a character-level model.\n",
        "\n",
        "\n",
        "Reference: https://neptune.ai/blog/tokenization-in-nlp\n",
        "\n",
        "*Personal Note: I think \"encode\" and \"decode\" are poor names for these operations because it can lead to confusion with encoder/decoder in the context of a transformer but c'est la vie*\n",
        "\n",
        "Other schemas: Google's SentencePiece (sub-word tokenizer that doesn't encode entire words, but also not only character), OpenAI's tiktoken (bi-pair encoding). Sub-word tokenizers are commonly what's used in practice."
      ],
      "metadata": {
        "id": "95u-bmOIri5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # create a mapping from characters to integers by creating a look up table from character to integer and vice versa\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }                                      # create string to integer dictionary where each unique character, ch, from the 'chars' list is mapped to its corresponding index in the list\n",
        "itos = { i:ch for i,ch in enumerate(chars) }                                      # create integer to string dictionary where each unique index, i, is mapped to its corresponding character, 'ch,' in the 'chars' list\n",
        "encode = lambda s: [stoi[c] for c in s]                                           # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])                                  # decoder: take a list of integers, output a string\n",
        "\n",
        "# lambda functions are anonymous, one-line functions in the syntax: lambda args: expression\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl9LU3vYpf_m",
        "outputId": "84a5f1fb-5f2b-4c33-9421-4b8da6e52073"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F                                             # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# encode(text) converts all of the Shakespeare dataset text from string to integers.\n",
        "# torch.tensor creates a tensor from the list of integers obtained from the encoding. This is a multi-dimensional matrix with elements of a single data type.\n",
        "# torch.long corresponds to 64-bit integers.\n",
        "# reference: https://pytorch.org/docs/stable/tensors.html\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])                                                               # the first 1000 characters we looked at earlier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuLl_e_hsx4q",
        "outputId": "49a59533-a0bd-421d-b9e0-cf2b1af4be3d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this point, the data is now represented by integers."
      ],
      "metadata": {
        "id": "-QjRxLVW4Irt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Split Data into Train/Test Sets"
      ],
      "metadata": {
        "id": "zIppBtrW4PqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data))                                                          # first 90% will be train, rest val\n",
        "\n",
        "# len(data) >> returns length of the 'data' tensor\n",
        "# 0.9*len(data) >> calculates 90% of the total length of the data. This results in a floating-point value of the index up to the first 90% of the data to be considered for model training\n",
        "# int(0.9*len(data)) >> converts floating-point value to an integer. This rounds down the floating-point value to the nearest integer. This ensures that we can use this as a valud index value.\n",
        "\n",
        "train_data = data[:n]                                                           # first 'n' elements\n",
        "val_data = data[n:]                                                             # last 1-n elements"
      ],
      "metadata": {
        "id": "aBVzVzypsx6u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)                                                                 # number of sample in the training dataset"
      ],
      "metadata": {
        "id": "YrXo68Oy8eO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65807cbc-8478-47b1-ffae-ded99ffeeb0f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Loading the Data for Our Model to Use"
      ],
      "metadata": {
        "id": "HERkje436pSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Hyperparameters"
      ],
      "metadata": {
        "id": "bus95BWTwsdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16                                                                 # how many independent sequences will we process in parallel?\n",
        "block_size = 32                                                                 # what is the maximum context length for predictions?\n",
        "max_iters = 5000                                                                # maximum number of training iterations (after max_iters, training is done)\n",
        "eval_interval = 100                                                             # this is how often we evaluate our model's performance on the test set (so every 100 iterations, we produce a loss value)\n",
        "learning_rate = 1e-3                                                            # step size for optimizer\n",
        "eval_iters = 200\n",
        "n_embd = 64                                                                     # size of embedding vectors for tokens or features\n",
        "n_head = 4                                                                      # attention heads (# of parallel attention mechanisms)\n",
        "n_layer = 4                                                                     # layers in architecture - depth of model\n",
        "dropout = 0.2                                                                   # dropout used"
      ],
      "metadata": {
        "id": "Ks5kZuVqwEPP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are not going be feeding the entire text to the transformer all at once (too computationally expensive/prohibitive). Therefore, when we train the transformer, we're really only using chunks of the data to do this at a time. These chunks have a max length. This is known as the *block size*. This is in the time dimension of the tensors fed into the transformer."
      ],
      "metadata": {
        "id": "Il1ZTxFX6smN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "id": "8tZcKx_z7EtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b271dd8-c562-4a61-c8b9-7b0c6227dabb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " In this case, in the context of 18, we see 47 comes next, in the context of 18 and 47, 56 comes next. In the context of 18, 47, 56, then 57 likely comes next, and so on. The purpose of this is to get our transformer used to seeing context as little as 1 to as much as block_size, and everything in between. After block_size, we start truncating because the transformer will never see more than block_size inputs when predicting the next character. This is demonstrated below:"
      ],
      "metadata": {
        "id": "KtVrzZlD9Mp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]                                                      # inputs to the transformer (x block size characters)\n",
        "y = train_data[1:block_size+1]                                                   # starts from the second element up to block size amount characters (these are the targets for each position in the input; the next block size characters). These are our predictions\n",
        "for t in range(block_size):                                                      # loop to iterate \"block_size\" times where the loop variable, 't', takes values from 0 to block_size-1\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "id": "CpDI6t_5sx9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449f6db6-6fd9-498b-f8ba-c37fbcfa7105"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]) the target: 64\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64]) the target: 43\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43]) the target: 52\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52]) the target: 10\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10]) the target: 0\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0]) the target: 14\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14]) the target: 43\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43]) the target: 44\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44]) the target: 53\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53]) the target: 56\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56]) the target: 43\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1]) the target: 61\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61]) the target: 43\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1]) the target: 54\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54]) the target: 56\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56]) the target: 53\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53]) the target: 41\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41]) the target: 43\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43]) the target: 43\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43]) the target: 42\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42]) the target: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we sample these chunks of text, we're going to have many batches of multiple chunks of text stacked in a single tensor for parallel data processing. These chunks are processed independently and do not communicate with each other. This is the *batch dimension*."
      ],
      "metadata": {
        "id": "Soqjz543Ao2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create data batches"
      ],
      "metadata": {
        "id": "R0crcMPXw1ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)                                                           # set random seed for PyTorch's random number generator https://pytorch.org/docs/stable/generated/torch.manual_seed.html\n",
        "\n",
        "def get_batch(split):                                                             # create function named 'get_batch' and it takes in a single arg, 'split'\n",
        "\n",
        "    # generates a small batch of data of inputs x and targets y\n",
        "\n",
        "    data = train_data if split == 'train' else val_data                           # selecting the data set to generate batch from. if split arg is 'train,' then train_data set is used, else val_data set is used. this returns our data array\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))                     # determine random positions to grab chunks out of. generate random indices to select the sequences from the data set. number of indices generated equal the 'batch_size' and the indices are chosen randomly within the range of the dataset boundaries. returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive). therefore, this generates batch_size numbers of random offsets, so ix are 4 numbers randomly generated between len(data) - block size. https://pytorch.org/docs/stable/generated/torch.randint.html\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])                           # first block size characters starting at i. construct input tensor, x, by selecting sequences of length of the block size from the dataset based on the random indices ix. this forms the inputs for the batch.\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])                       # constructs ^ for the targets - ^ offset by 1\n",
        "    return x, y                                                                   # return x and y as a tuple\n",
        "\n",
        "xb, yb = get_batch('train')                                                       # obtains batch of training data\n",
        "print('inputs to trasformer:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size):                                                      # batch dimension\n",
        "    for t in range(block_size):                                                  # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "sNkH0lg6sx_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48f9fb3-460c-446d-9ecd-ed7b4a83f5ce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs to trasformer:\n",
            "torch.Size([16, 32])\n",
            "tensor([[58, 53,  1, 41, 53, 56, 56, 59, 54, 58,  1, 39,  1, 51, 39, 52,  5, 57,\n",
            "          1, 61, 47, 44, 43,  1, 47, 57,  0, 61, 46, 43, 52,  1],\n",
            "        [49,  1, 39, 52,  1, 53, 39, 58, 46,  1, 40, 63,  1, 20, 47, 51,  6,  0,\n",
            "         32, 46, 43,  1, 59, 52, 47, 58, 63,  1, 58, 46, 43,  1],\n",
            "        [59, 50, 42,  1, 58, 46, 53, 59,  1, 61, 43, 56, 58,  1, 57, 53,  1, 58,\n",
            "         53, 53,  2,  0,  0, 24, 33, 15, 21, 27, 10,  0, 35, 43],\n",
            "        [ 8,  0,  0, 35, 13, 30, 35, 21, 15, 23, 10,  0, 28, 56, 53, 60, 43,  1,\n",
            "         47, 58,  6,  1, 20, 43, 52, 56, 63,  6,  1, 39, 52, 42],\n",
            "        [58,  1, 57, 46, 43,  8,  0,  0, 32, 30, 13, 26, 21, 27, 10,  0, 18, 53,\n",
            "         56,  1, 61, 46, 39, 58,  1, 56, 43, 39, 57, 53, 52,  6],\n",
            "        [56, 61, 47, 41, 49,  6,  1, 50, 43, 58,  1, 47, 58,  1, 40, 43, 11,  0,\n",
            "         18, 53, 56,  1, 47, 52,  1, 58, 46, 63,  1, 57, 46, 53],\n",
            "        [25, 10,  0, 35, 47, 58, 46, 42, 56, 39, 61,  1, 63, 53, 59,  1, 46, 43,\n",
            "         52, 41, 43,  6,  1, 51, 63,  1, 50, 53, 56, 42,  6,  1],\n",
            "        [43, 57, 58,  1, 61, 47, 58, 46,  1, 58, 46, 63,  1, 44, 56, 53, 64, 43,\n",
            "         52,  1, 39, 42, 51, 53, 52, 47, 58, 47, 53, 52,  0, 25],\n",
            "        [47, 52, 41, 43,  1, 58, 46, 53, 59,  6,  1, 41, 56, 43, 39, 58, 43, 42,\n",
            "          1, 58, 53,  1, 40, 43,  1, 39, 61, 43, 42,  1, 40, 63],\n",
            "        [53, 52, 57,  8,  0,  0, 34, 27, 24, 33, 25, 26, 21, 13, 10,  0, 27,  6,\n",
            "          1, 52, 53,  1, 51, 53, 56, 43,  6,  1, 52, 53,  1, 51],\n",
            "        [53, 59, 56,  1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58,  8,  1, 18, 53,\n",
            "         56,  1, 58, 46, 43,  1, 42, 43, 39, 56, 58, 46,  6,  0],\n",
            "        [53, 58, 46, 47, 52, 45,  8,  0,  0, 16, 33, 23, 17,  1, 27, 18,  1, 37,\n",
            "         27, 30, 23, 10,  0, 26, 53,  1, 51, 39, 58, 58, 43, 56],\n",
            "        [53, 61,  1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57,  6,  1, 57, 53,\n",
            "         50, 43, 51, 52,  1, 39, 52, 42,  1, 59, 52, 43, 39, 56],\n",
            "        [50, 50,  1, 57, 58, 47, 50, 50,  1, 50, 47, 60, 43,  1, 41, 46, 39, 57,\n",
            "         58, 43, 12,  0,  0, 30, 27, 25, 17, 27, 10,  0, 31, 46],\n",
            "        [21, 21, 10,  0, 13, 63,  6,  1, 47, 44,  1, 63, 53, 59, 56, 57, 43, 50,\n",
            "         44,  5, 57,  1, 56, 43, 51, 43, 51, 40, 56, 39, 52, 41],\n",
            "        [59, 58,  1, 47, 52,  1, 46, 43, 56,  1, 58, 43, 52, 42, 43, 56,  1, 46,\n",
            "         43, 39, 56, 58,  1, 58, 46, 43,  1, 39, 57, 54, 47, 56]])\n",
            "targets:\n",
            "torch.Size([16, 32])\n",
            "tensor([[53,  1, 41, 53, 56, 56, 59, 54, 58,  1, 39,  1, 51, 39, 52,  5, 57,  1,\n",
            "         61, 47, 44, 43,  1, 47, 57,  0, 61, 46, 43, 52,  1, 57],\n",
            "        [ 1, 39, 52,  1, 53, 39, 58, 46,  1, 40, 63,  1, 20, 47, 51,  6,  0, 32,\n",
            "         46, 43,  1, 59, 52, 47, 58, 63,  1, 58, 46, 43,  1, 49],\n",
            "        [50, 42,  1, 58, 46, 53, 59,  1, 61, 43, 56, 58,  1, 57, 53,  1, 58, 53,\n",
            "         53,  2,  0,  0, 24, 33, 15, 21, 27, 10,  0, 35, 43, 50],\n",
            "        [ 0,  0, 35, 13, 30, 35, 21, 15, 23, 10,  0, 28, 56, 53, 60, 43,  1, 47,\n",
            "         58,  6,  1, 20, 43, 52, 56, 63,  6,  1, 39, 52, 42,  1],\n",
            "        [ 1, 57, 46, 43,  8,  0,  0, 32, 30, 13, 26, 21, 27, 10,  0, 18, 53, 56,\n",
            "          1, 61, 46, 39, 58,  1, 56, 43, 39, 57, 53, 52,  6,  1],\n",
            "        [61, 47, 41, 49,  6,  1, 50, 43, 58,  1, 47, 58,  1, 40, 43, 11,  0, 18,\n",
            "         53, 56,  1, 47, 52,  1, 58, 46, 63,  1, 57, 46, 53, 59],\n",
            "        [10,  0, 35, 47, 58, 46, 42, 56, 39, 61,  1, 63, 53, 59,  1, 46, 43, 52,\n",
            "         41, 43,  6,  1, 51, 63,  1, 50, 53, 56, 42,  6,  1, 21],\n",
            "        [57, 58,  1, 61, 47, 58, 46,  1, 58, 46, 63,  1, 44, 56, 53, 64, 43, 52,\n",
            "          1, 39, 42, 51, 53, 52, 47, 58, 47, 53, 52,  0, 25, 39],\n",
            "        [52, 41, 43,  1, 58, 46, 53, 59,  6,  1, 41, 56, 43, 39, 58, 43, 42,  1,\n",
            "         58, 53,  1, 40, 43,  1, 39, 61, 43, 42,  1, 40, 63,  1],\n",
            "        [52, 57,  8,  0,  0, 34, 27, 24, 33, 25, 26, 21, 13, 10,  0, 27,  6,  1,\n",
            "         52, 53,  1, 51, 53, 56, 43,  6,  1, 52, 53,  1, 51, 53],\n",
            "        [59, 56,  1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58,  8,  1, 18, 53, 56,\n",
            "          1, 58, 46, 43,  1, 42, 43, 39, 56, 58, 46,  6,  0, 32],\n",
            "        [58, 46, 47, 52, 45,  8,  0,  0, 16, 33, 23, 17,  1, 27, 18,  1, 37, 27,\n",
            "         30, 23, 10,  0, 26, 53,  1, 51, 39, 58, 58, 43, 56,  6],\n",
            "        [61,  1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57,  6,  1, 57, 53, 50,\n",
            "         43, 51, 52,  1, 39, 52, 42,  1, 59, 52, 43, 39, 56, 58],\n",
            "        [50,  1, 57, 58, 47, 50, 50,  1, 50, 47, 60, 43,  1, 41, 46, 39, 57, 58,\n",
            "         43, 12,  0,  0, 30, 27, 25, 17, 27, 10,  0, 31, 46, 43],\n",
            "        [21, 10,  0, 13, 63,  6,  1, 47, 44,  1, 63, 53, 59, 56, 57, 43, 50, 44,\n",
            "          5, 57,  1, 56, 43, 51, 43, 51, 40, 56, 39, 52, 41, 43],\n",
            "        [58,  1, 47, 52,  1, 46, 43, 56,  1, 58, 43, 52, 42, 43, 56,  1, 46, 43,\n",
            "         39, 56, 58,  1, 58, 46, 43,  1, 39, 57, 54, 47, 56, 47]])\n",
            "----\n",
            "when input is [58] the target: 53\n",
            "when input is [58, 53] the target: 1\n",
            "when input is [58, 53, 1] the target: 41\n",
            "when input is [58, 53, 1, 41] the target: 53\n",
            "when input is [58, 53, 1, 41, 53] the target: 56\n",
            "when input is [58, 53, 1, 41, 53, 56] the target: 56\n",
            "when input is [58, 53, 1, 41, 53, 56, 56] the target: 59\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59] the target: 54\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54] the target: 58\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58] the target: 1\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1] the target: 39\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39] the target: 1\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1] the target: 51\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51] the target: 39\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39] the target: 52\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52] the target: 5\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5] the target: 57\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57] the target: 1\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1] the target: 61\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61] the target: 47\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47] the target: 44\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44] the target: 43\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43] the target: 1\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43, 1] the target: 47\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43, 1, 47] the target: 57\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43, 1, 47, 57] the target: 0\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43, 1, 47, 57, 0] the target: 61\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43, 1, 47, 57, 0, 61] the target: 46\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43, 1, 47, 57, 0, 61, 46] the target: 43\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43, 1, 47, 57, 0, 61, 46, 43] the target: 52\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43, 1, 47, 57, 0, 61, 46, 43, 52] the target: 1\n",
            "when input is [58, 53, 1, 41, 53, 56, 56, 59, 54, 58, 1, 39, 1, 51, 39, 52, 5, 57, 1, 61, 47, 44, 43, 1, 47, 57, 0, 61, 46, 43, 52, 1] the target: 57\n",
            "when input is [49] the target: 1\n",
            "when input is [49, 1] the target: 39\n",
            "when input is [49, 1, 39] the target: 52\n",
            "when input is [49, 1, 39, 52] the target: 1\n",
            "when input is [49, 1, 39, 52, 1] the target: 53\n",
            "when input is [49, 1, 39, 52, 1, 53] the target: 39\n",
            "when input is [49, 1, 39, 52, 1, 53, 39] the target: 58\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58] the target: 46\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46] the target: 1\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1] the target: 40\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40] the target: 63\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63] the target: 1\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1] the target: 20\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20] the target: 47\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47] the target: 51\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51] the target: 6\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6] the target: 0\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0] the target: 32\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32] the target: 46\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46] the target: 43\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43] the target: 1\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1] the target: 59\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59] the target: 52\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59, 52] the target: 47\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59, 52, 47] the target: 58\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59, 52, 47, 58] the target: 63\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59, 52, 47, 58, 63] the target: 1\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59, 52, 47, 58, 63, 1] the target: 58\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59, 52, 47, 58, 63, 1, 58] the target: 46\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59, 52, 47, 58, 63, 1, 58, 46] the target: 43\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59, 52, 47, 58, 63, 1, 58, 46, 43] the target: 1\n",
            "when input is [49, 1, 39, 52, 1, 53, 39, 58, 46, 1, 40, 63, 1, 20, 47, 51, 6, 0, 32, 46, 43, 1, 59, 52, 47, 58, 63, 1, 58, 46, 43, 1] the target: 49\n",
            "when input is [59] the target: 50\n",
            "when input is [59, 50] the target: 42\n",
            "when input is [59, 50, 42] the target: 1\n",
            "when input is [59, 50, 42, 1] the target: 58\n",
            "when input is [59, 50, 42, 1, 58] the target: 46\n",
            "when input is [59, 50, 42, 1, 58, 46] the target: 53\n",
            "when input is [59, 50, 42, 1, 58, 46, 53] the target: 59\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59] the target: 1\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1] the target: 61\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61] the target: 43\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43] the target: 56\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56] the target: 58\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58] the target: 1\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1] the target: 57\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57] the target: 53\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53] the target: 1\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1] the target: 58\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58] the target: 53\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53] the target: 53\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53] the target: 2\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2] the target: 0\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0] the target: 0\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0] the target: 24\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0, 24] the target: 33\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0, 24, 33] the target: 15\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0, 24, 33, 15] the target: 21\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0, 24, 33, 15, 21] the target: 27\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0, 24, 33, 15, 21, 27] the target: 10\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0, 24, 33, 15, 21, 27, 10] the target: 0\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0, 24, 33, 15, 21, 27, 10, 0] the target: 35\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0, 24, 33, 15, 21, 27, 10, 0, 35] the target: 43\n",
            "when input is [59, 50, 42, 1, 58, 46, 53, 59, 1, 61, 43, 56, 58, 1, 57, 53, 1, 58, 53, 53, 2, 0, 0, 24, 33, 15, 21, 27, 10, 0, 35, 43] the target: 50\n",
            "when input is [8] the target: 0\n",
            "when input is [8, 0] the target: 0\n",
            "when input is [8, 0, 0] the target: 35\n",
            "when input is [8, 0, 0, 35] the target: 13\n",
            "when input is [8, 0, 0, 35, 13] the target: 30\n",
            "when input is [8, 0, 0, 35, 13, 30] the target: 35\n",
            "when input is [8, 0, 0, 35, 13, 30, 35] the target: 21\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21] the target: 15\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15] the target: 23\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23] the target: 10\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10] the target: 0\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0] the target: 28\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28] the target: 56\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56] the target: 53\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53] the target: 60\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60] the target: 43\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43] the target: 1\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1] the target: 47\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47] the target: 58\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58] the target: 6\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6] the target: 1\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1] the target: 20\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20] the target: 43\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20, 43] the target: 52\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20, 43, 52] the target: 56\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20, 43, 52, 56] the target: 63\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20, 43, 52, 56, 63] the target: 6\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20, 43, 52, 56, 63, 6] the target: 1\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20, 43, 52, 56, 63, 6, 1] the target: 39\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20, 43, 52, 56, 63, 6, 1, 39] the target: 52\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20, 43, 52, 56, 63, 6, 1, 39, 52] the target: 42\n",
            "when input is [8, 0, 0, 35, 13, 30, 35, 21, 15, 23, 10, 0, 28, 56, 53, 60, 43, 1, 47, 58, 6, 1, 20, 43, 52, 56, 63, 6, 1, 39, 52, 42] the target: 1\n",
            "when input is [58] the target: 1\n",
            "when input is [58, 1] the target: 57\n",
            "when input is [58, 1, 57] the target: 46\n",
            "when input is [58, 1, 57, 46] the target: 43\n",
            "when input is [58, 1, 57, 46, 43] the target: 8\n",
            "when input is [58, 1, 57, 46, 43, 8] the target: 0\n",
            "when input is [58, 1, 57, 46, 43, 8, 0] the target: 0\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0] the target: 32\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32] the target: 30\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30] the target: 13\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13] the target: 26\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26] the target: 21\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21] the target: 27\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27] the target: 10\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10] the target: 0\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0] the target: 18\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18] the target: 53\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53] the target: 56\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56] the target: 1\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1] the target: 61\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61] the target: 46\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46] the target: 39\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39] the target: 58\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39, 58] the target: 1\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39, 58, 1] the target: 56\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39, 58, 1, 56] the target: 43\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39, 58, 1, 56, 43] the target: 39\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39, 58, 1, 56, 43, 39] the target: 57\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39, 58, 1, 56, 43, 39, 57] the target: 53\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39, 58, 1, 56, 43, 39, 57, 53] the target: 52\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39, 58, 1, 56, 43, 39, 57, 53, 52] the target: 6\n",
            "when input is [58, 1, 57, 46, 43, 8, 0, 0, 32, 30, 13, 26, 21, 27, 10, 0, 18, 53, 56, 1, 61, 46, 39, 58, 1, 56, 43, 39, 57, 53, 52, 6] the target: 1\n",
            "when input is [56] the target: 61\n",
            "when input is [56, 61] the target: 47\n",
            "when input is [56, 61, 47] the target: 41\n",
            "when input is [56, 61, 47, 41] the target: 49\n",
            "when input is [56, 61, 47, 41, 49] the target: 6\n",
            "when input is [56, 61, 47, 41, 49, 6] the target: 1\n",
            "when input is [56, 61, 47, 41, 49, 6, 1] the target: 50\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50] the target: 43\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43] the target: 58\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58] the target: 1\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1] the target: 47\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47] the target: 58\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58] the target: 1\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1] the target: 40\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40] the target: 43\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43] the target: 11\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11] the target: 0\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0] the target: 18\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18] the target: 53\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53] the target: 56\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56] the target: 1\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1] the target: 47\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47] the target: 52\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47, 52] the target: 1\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47, 52, 1] the target: 58\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47, 52, 1, 58] the target: 46\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47, 52, 1, 58, 46] the target: 63\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47, 52, 1, 58, 46, 63] the target: 1\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47, 52, 1, 58, 46, 63, 1] the target: 57\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47, 52, 1, 58, 46, 63, 1, 57] the target: 46\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47, 52, 1, 58, 46, 63, 1, 57, 46] the target: 53\n",
            "when input is [56, 61, 47, 41, 49, 6, 1, 50, 43, 58, 1, 47, 58, 1, 40, 43, 11, 0, 18, 53, 56, 1, 47, 52, 1, 58, 46, 63, 1, 57, 46, 53] the target: 59\n",
            "when input is [25] the target: 10\n",
            "when input is [25, 10] the target: 0\n",
            "when input is [25, 10, 0] the target: 35\n",
            "when input is [25, 10, 0, 35] the target: 47\n",
            "when input is [25, 10, 0, 35, 47] the target: 58\n",
            "when input is [25, 10, 0, 35, 47, 58] the target: 46\n",
            "when input is [25, 10, 0, 35, 47, 58, 46] the target: 42\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42] the target: 56\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56] the target: 39\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39] the target: 61\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61] the target: 1\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1] the target: 63\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63] the target: 53\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53] the target: 59\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59] the target: 1\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1] the target: 46\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46] the target: 43\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43] the target: 52\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52] the target: 41\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41] the target: 43\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43] the target: 6\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6] the target: 1\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1] the target: 51\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1, 51] the target: 63\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1, 51, 63] the target: 1\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1, 51, 63, 1] the target: 50\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1, 51, 63, 1, 50] the target: 53\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1, 51, 63, 1, 50, 53] the target: 56\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1, 51, 63, 1, 50, 53, 56] the target: 42\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1, 51, 63, 1, 50, 53, 56, 42] the target: 6\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1, 51, 63, 1, 50, 53, 56, 42, 6] the target: 1\n",
            "when input is [25, 10, 0, 35, 47, 58, 46, 42, 56, 39, 61, 1, 63, 53, 59, 1, 46, 43, 52, 41, 43, 6, 1, 51, 63, 1, 50, 53, 56, 42, 6, 1] the target: 21\n",
            "when input is [43] the target: 57\n",
            "when input is [43, 57] the target: 58\n",
            "when input is [43, 57, 58] the target: 1\n",
            "when input is [43, 57, 58, 1] the target: 61\n",
            "when input is [43, 57, 58, 1, 61] the target: 47\n",
            "when input is [43, 57, 58, 1, 61, 47] the target: 58\n",
            "when input is [43, 57, 58, 1, 61, 47, 58] the target: 46\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46] the target: 1\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1] the target: 58\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58] the target: 46\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46] the target: 63\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63] the target: 1\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1] the target: 44\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44] the target: 56\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56] the target: 53\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53] the target: 64\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64] the target: 43\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43] the target: 52\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52] the target: 1\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1] the target: 39\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39] the target: 42\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42] the target: 51\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51] the target: 53\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51, 53] the target: 52\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51, 53, 52] the target: 47\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51, 53, 52, 47] the target: 58\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51, 53, 52, 47, 58] the target: 47\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51, 53, 52, 47, 58, 47] the target: 53\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51, 53, 52, 47, 58, 47, 53] the target: 52\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51, 53, 52, 47, 58, 47, 53, 52] the target: 0\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51, 53, 52, 47, 58, 47, 53, 52, 0] the target: 25\n",
            "when input is [43, 57, 58, 1, 61, 47, 58, 46, 1, 58, 46, 63, 1, 44, 56, 53, 64, 43, 52, 1, 39, 42, 51, 53, 52, 47, 58, 47, 53, 52, 0, 25] the target: 39\n",
            "when input is [47] the target: 52\n",
            "when input is [47, 52] the target: 41\n",
            "when input is [47, 52, 41] the target: 43\n",
            "when input is [47, 52, 41, 43] the target: 1\n",
            "when input is [47, 52, 41, 43, 1] the target: 58\n",
            "when input is [47, 52, 41, 43, 1, 58] the target: 46\n",
            "when input is [47, 52, 41, 43, 1, 58, 46] the target: 53\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53] the target: 59\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59] the target: 6\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6] the target: 1\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1] the target: 41\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41] the target: 56\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56] the target: 43\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43] the target: 39\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39] the target: 58\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58] the target: 43\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43] the target: 42\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42] the target: 1\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1] the target: 58\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58] the target: 53\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53] the target: 1\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1] the target: 40\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40] the target: 43\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40, 43] the target: 1\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40, 43, 1] the target: 39\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40, 43, 1, 39] the target: 61\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40, 43, 1, 39, 61] the target: 43\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40, 43, 1, 39, 61, 43] the target: 42\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40, 43, 1, 39, 61, 43, 42] the target: 1\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40, 43, 1, 39, 61, 43, 42, 1] the target: 40\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40, 43, 1, 39, 61, 43, 42, 1, 40] the target: 63\n",
            "when input is [47, 52, 41, 43, 1, 58, 46, 53, 59, 6, 1, 41, 56, 43, 39, 58, 43, 42, 1, 58, 53, 1, 40, 43, 1, 39, 61, 43, 42, 1, 40, 63] the target: 1\n",
            "when input is [53] the target: 52\n",
            "when input is [53, 52] the target: 57\n",
            "when input is [53, 52, 57] the target: 8\n",
            "when input is [53, 52, 57, 8] the target: 0\n",
            "when input is [53, 52, 57, 8, 0] the target: 0\n",
            "when input is [53, 52, 57, 8, 0, 0] the target: 34\n",
            "when input is [53, 52, 57, 8, 0, 0, 34] the target: 27\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27] the target: 24\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24] the target: 33\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33] the target: 25\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25] the target: 26\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26] the target: 21\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21] the target: 13\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13] the target: 10\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10] the target: 0\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0] the target: 27\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27] the target: 6\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6] the target: 1\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1] the target: 52\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52] the target: 53\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53] the target: 1\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1] the target: 51\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51] the target: 53\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51, 53] the target: 56\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51, 53, 56] the target: 43\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51, 53, 56, 43] the target: 6\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51, 53, 56, 43, 6] the target: 1\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51, 53, 56, 43, 6, 1] the target: 52\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51, 53, 56, 43, 6, 1, 52] the target: 53\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51, 53, 56, 43, 6, 1, 52, 53] the target: 1\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51, 53, 56, 43, 6, 1, 52, 53, 1] the target: 51\n",
            "when input is [53, 52, 57, 8, 0, 0, 34, 27, 24, 33, 25, 26, 21, 13, 10, 0, 27, 6, 1, 52, 53, 1, 51, 53, 56, 43, 6, 1, 52, 53, 1, 51] the target: 53\n",
            "when input is [53] the target: 59\n",
            "when input is [53, 59] the target: 56\n",
            "when input is [53, 59, 56] the target: 1\n",
            "when input is [53, 59, 56, 1] the target: 47\n",
            "when input is [53, 59, 56, 1, 47] the target: 51\n",
            "when input is [53, 59, 56, 1, 47, 51] the target: 54\n",
            "when input is [53, 59, 56, 1, 47, 51, 54] the target: 43\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43] the target: 42\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42] the target: 47\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47] the target: 51\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51] the target: 43\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43] the target: 52\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52] the target: 58\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58] the target: 8\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8] the target: 1\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1] the target: 18\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18] the target: 53\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53] the target: 56\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56] the target: 1\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1] the target: 58\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58] the target: 46\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46] the target: 43\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43] the target: 1\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43, 1] the target: 42\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43, 1, 42] the target: 43\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43, 1, 42, 43] the target: 39\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43, 1, 42, 43, 39] the target: 56\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43, 1, 42, 43, 39, 56] the target: 58\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43, 1, 42, 43, 39, 56, 58] the target: 46\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43, 1, 42, 43, 39, 56, 58, 46] the target: 6\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43, 1, 42, 43, 39, 56, 58, 46, 6] the target: 0\n",
            "when input is [53, 59, 56, 1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58, 8, 1, 18, 53, 56, 1, 58, 46, 43, 1, 42, 43, 39, 56, 58, 46, 6, 0] the target: 32\n",
            "when input is [53] the target: 58\n",
            "when input is [53, 58] the target: 46\n",
            "when input is [53, 58, 46] the target: 47\n",
            "when input is [53, 58, 46, 47] the target: 52\n",
            "when input is [53, 58, 46, 47, 52] the target: 45\n",
            "when input is [53, 58, 46, 47, 52, 45] the target: 8\n",
            "when input is [53, 58, 46, 47, 52, 45, 8] the target: 0\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0] the target: 0\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0] the target: 16\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16] the target: 33\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33] the target: 23\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23] the target: 17\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17] the target: 1\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1] the target: 27\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27] the target: 18\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18] the target: 1\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1] the target: 37\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37] the target: 27\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27] the target: 30\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30] the target: 23\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23] the target: 10\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10] the target: 0\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0] the target: 26\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0, 26] the target: 53\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0, 26, 53] the target: 1\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0, 26, 53, 1] the target: 51\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0, 26, 53, 1, 51] the target: 39\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0, 26, 53, 1, 51, 39] the target: 58\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0, 26, 53, 1, 51, 39, 58] the target: 58\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0, 26, 53, 1, 51, 39, 58, 58] the target: 43\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0, 26, 53, 1, 51, 39, 58, 58, 43] the target: 56\n",
            "when input is [53, 58, 46, 47, 52, 45, 8, 0, 0, 16, 33, 23, 17, 1, 27, 18, 1, 37, 27, 30, 23, 10, 0, 26, 53, 1, 51, 39, 58, 58, 43, 56] the target: 6\n",
            "when input is [53] the target: 61\n",
            "when input is [53, 61] the target: 1\n",
            "when input is [53, 61, 1] the target: 41\n",
            "when input is [53, 61, 1, 41] the target: 43\n",
            "when input is [53, 61, 1, 41, 43] the target: 56\n",
            "when input is [53, 61, 1, 41, 43, 56] the target: 43\n",
            "when input is [53, 61, 1, 41, 43, 56, 43] the target: 51\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51] the target: 53\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53] the target: 52\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52] the target: 47\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47] the target: 53\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53] the target: 59\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59] the target: 57\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57] the target: 6\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6] the target: 1\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1] the target: 57\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57] the target: 53\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53] the target: 50\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50] the target: 43\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43] the target: 51\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51] the target: 52\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52] the target: 1\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1] the target: 39\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1, 39] the target: 52\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1, 39, 52] the target: 42\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1, 39, 52, 42] the target: 1\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1, 39, 52, 42, 1] the target: 59\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1, 39, 52, 42, 1, 59] the target: 52\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1, 39, 52, 42, 1, 59, 52] the target: 43\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1, 39, 52, 42, 1, 59, 52, 43] the target: 39\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1, 39, 52, 42, 1, 59, 52, 43, 39] the target: 56\n",
            "when input is [53, 61, 1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57, 6, 1, 57, 53, 50, 43, 51, 52, 1, 39, 52, 42, 1, 59, 52, 43, 39, 56] the target: 58\n",
            "when input is [50] the target: 50\n",
            "when input is [50, 50] the target: 1\n",
            "when input is [50, 50, 1] the target: 57\n",
            "when input is [50, 50, 1, 57] the target: 58\n",
            "when input is [50, 50, 1, 57, 58] the target: 47\n",
            "when input is [50, 50, 1, 57, 58, 47] the target: 50\n",
            "when input is [50, 50, 1, 57, 58, 47, 50] the target: 50\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50] the target: 1\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1] the target: 50\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50] the target: 47\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47] the target: 60\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60] the target: 43\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43] the target: 1\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1] the target: 41\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41] the target: 46\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46] the target: 39\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39] the target: 57\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57] the target: 58\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58] the target: 43\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43] the target: 12\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12] the target: 0\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0] the target: 0\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0] the target: 30\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0, 30] the target: 27\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0, 30, 27] the target: 25\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0, 30, 27, 25] the target: 17\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0, 30, 27, 25, 17] the target: 27\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0, 30, 27, 25, 17, 27] the target: 10\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0, 30, 27, 25, 17, 27, 10] the target: 0\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0, 30, 27, 25, 17, 27, 10, 0] the target: 31\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0, 30, 27, 25, 17, 27, 10, 0, 31] the target: 46\n",
            "when input is [50, 50, 1, 57, 58, 47, 50, 50, 1, 50, 47, 60, 43, 1, 41, 46, 39, 57, 58, 43, 12, 0, 0, 30, 27, 25, 17, 27, 10, 0, 31, 46] the target: 43\n",
            "when input is [21] the target: 21\n",
            "when input is [21, 21] the target: 10\n",
            "when input is [21, 21, 10] the target: 0\n",
            "when input is [21, 21, 10, 0] the target: 13\n",
            "when input is [21, 21, 10, 0, 13] the target: 63\n",
            "when input is [21, 21, 10, 0, 13, 63] the target: 6\n",
            "when input is [21, 21, 10, 0, 13, 63, 6] the target: 1\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1] the target: 47\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47] the target: 44\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44] the target: 1\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1] the target: 63\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63] the target: 53\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53] the target: 59\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59] the target: 56\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56] the target: 57\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57] the target: 43\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43] the target: 50\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50] the target: 44\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44] the target: 5\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5] the target: 57\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57] the target: 1\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1] the target: 56\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56] the target: 43\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56, 43] the target: 51\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56, 43, 51] the target: 43\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56, 43, 51, 43] the target: 51\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56, 43, 51, 43, 51] the target: 40\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56, 43, 51, 43, 51, 40] the target: 56\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56, 43, 51, 43, 51, 40, 56] the target: 39\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56, 43, 51, 43, 51, 40, 56, 39] the target: 52\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56, 43, 51, 43, 51, 40, 56, 39, 52] the target: 41\n",
            "when input is [21, 21, 10, 0, 13, 63, 6, 1, 47, 44, 1, 63, 53, 59, 56, 57, 43, 50, 44, 5, 57, 1, 56, 43, 51, 43, 51, 40, 56, 39, 52, 41] the target: 43\n",
            "when input is [59] the target: 58\n",
            "when input is [59, 58] the target: 1\n",
            "when input is [59, 58, 1] the target: 47\n",
            "when input is [59, 58, 1, 47] the target: 52\n",
            "when input is [59, 58, 1, 47, 52] the target: 1\n",
            "when input is [59, 58, 1, 47, 52, 1] the target: 46\n",
            "when input is [59, 58, 1, 47, 52, 1, 46] the target: 43\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43] the target: 56\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56] the target: 1\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1] the target: 58\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58] the target: 43\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43] the target: 52\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52] the target: 42\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42] the target: 43\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43] the target: 56\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56] the target: 1\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1] the target: 46\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46] the target: 43\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43] the target: 39\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39] the target: 56\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56] the target: 58\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58] the target: 1\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1] the target: 58\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1, 58] the target: 46\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1, 58, 46] the target: 43\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1, 58, 46, 43] the target: 1\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1, 58, 46, 43, 1] the target: 39\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1, 58, 46, 43, 1, 39] the target: 57\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1, 58, 46, 43, 1, 39, 57] the target: 54\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1, 58, 46, 43, 1, 39, 57, 54] the target: 47\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1, 58, 46, 43, 1, 39, 57, 54, 47] the target: 56\n",
            "when input is [59, 58, 1, 47, 52, 1, 46, 43, 56, 1, 58, 43, 52, 42, 43, 56, 1, 46, 43, 39, 56, 58, 1, 58, 46, 43, 1, 39, 57, 54, 47, 56] the target: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 4 × 8 array above would give us 32 examples (gives us cases where we can say \"when the input is #, the target is #; when the input is #, #, the target is #). They're completely independent as far as transformer is concerned. These are 32 indepedent examples packed into a single batch. This integer tensor of x will feed into the transformer, and it will simultaneously process all these examples."
      ],
      "metadata": {
        "id": "31ol4vF_K672"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Creating the Transformer Model"
      ],
      "metadata": {
        "id": "76lQBensL61Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: *The simple bigram model will not perform well because the tokens are NOT talking to each other. This is where the transformer will help.*"
      ],
      "metadata": {
        "id": "0u_C4O8VSf0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Bigram Model Class"
      ],
      "metadata": {
        "id": "2tQdICbK3A5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by building a bigram model - one of the simplest neural networks.\n",
        "\n",
        "For more information on bigram model: https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/#:~:text=A%20bigram%20language%20model%20is,in%20a%20text%20or%20sentence."
      ],
      "metadata": {
        "id": "rr-B2WYXBsbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a classification ML task, cross-entropy loss (aka log loss or softmax loss) is the employed loss function. This is the difference between the projected probability distribution and the actual proability distribution of the target classes. In this case, we have ID of next character so this evaluates quality of prediction wrt to target. Correct dim of logits should yield high value, and other dims low. Because we have 65 possible vocabulary elements, we can guess loss as: $-ln(1/65)=4.17$. If the loss value is higher than our \"guess,\" meaning our predictions are a bit more diffuse.\n",
        "\n",
        "See more: https://www.educative.io/answers/what-is-cross-entropy-loss-in-pytorch.\n",
        "\n",
        "Lecture 28 ML"
      ],
      "metadata": {
        "id": "sVeJflrQgncl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Function: Turns vector of K real values into a vector of K real values that sum to 1. The inputs can be +, , 0, or greater than 1 but the softmax transforms them into values between 0 and 1, so that they can be interpreted as probabilities. See more: https://deepai.org/machine-learning-glossary-and-terms/softmax-layer#:~:text=The%20softmax%20function%20is%20a,can%20be%20interpreted%20as%20probabilities.\n",
        "\n",
        "\n",
        "Softmax($x_i$) = $\\frac{e^{x_i}}{∑_j e^{x_j}}$"
      ],
      "metadata": {
        "id": "V44Oo9g_kz-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()                                                                # decorator; disable gradient computation to avoid the gradients of other batches accumulating or impacting the gradient of the previous batch\n",
        "def estimate_loss():                                                            # evaluate loss on training and test\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)                                        # initialize losses tensor for the evaluation iterations\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)                                          # compute logits and loss for selected batch\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "NvOYjYblxBpr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In transformers, dependencies between sequences is established through self-attention.\n",
        "\n",
        "Dropout layers enable our model to avoid overfitting. Dropout is a regularization technique for neural networks"
      ],
      "metadata": {
        "id": "ATk3SZo2zFoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # apply linear transformation to input data https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)                     # importance of each position relative to other positions in sequence\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)                   # determines focus area when generating output\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)                   # content/info at each position\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):                                                       # forward pass\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)                                                         # (B,T,C)\n",
        "        q = self.query(x)                                                       # (B,T,C)\n",
        "\n",
        "        # compute attention scores (\"affinities\") between query and key representations\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5                                  # (B, T, C) @ (B, C, T) -> (B, T, T); scaled attn\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))            # (B, T, T); Applying this mask prevents the model from \"cheating\" and attending to future positions in the sequence. Elements in the attention score matrix for future positions is replaced with -inf so that attention is only allowed to losst or past or current positions in the sequence; delete this if desire decoder only.\n",
        "        wei = F.softmax(wei, dim=-1)                                            # (B, T, T); normalize weights\n",
        "        wei = self.dropout(wei)                                                 # dropout regularization: https://www.google.com/search?q=adding+dropout+layers&rlz=1C5CHFA_enUS737US737&oq=adding+dropout+layers+&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIICAEQABgWGB4yCAgCEAAYFhgeMg0IAxAAGIYDGIAEGIoFMg0IBBAAGIYDGIAEGIoFMg0IBRAAGIYDGIAEGIoFMg0IBhAAGIYDGIAEGIoFMg0IBxAAGIYDGIAEGIoF0gEINDU2M2owajeoAgCwAgA&sourceid=chrome&ie=UTF-8\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)                                                       # (B,T,C); vector that aggregates\n",
        "        out = wei @ v                                                           # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "FXRJOYwLvu50"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Self-attention is defined by the keys, query and values coming from the same source: x. Therefore, the nodes are self-attending. Cross-attention is instead when the queries and values come from a different source."
      ],
      "metadata": {
        "id": "yVVkwOnrp5tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # projects concatenated output back to embedding dimension\n",
        "        self.proj = nn.Linear(n_embd, n_embd)                                   # projection for residuals; linear transformation\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))                                      # for residual pathway\n",
        "        return out"
      ],
      "metadata": {
        "id": "TFgtNj7xvu2t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),                                      # projection layer going back into residual pathway\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "6rJ3Y2m7vuva"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)                                         # change from OG Attn is All You Need Paper, where it was originally done after transformation during Add & Norm. Now add layerorm (prenorm formulation) before transformation before it goes into self-attention and feedforward.\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))                                            # residual connection; fork off, do communication, come back\n",
        "        x = x + self.ffwd(self.ln2(x))                                          # residual connection; fork off, do computation, come back\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "MrqtsKuFvulR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual Connections:\n",
        "\n",
        "https://arxiv.org/abs/1512.03385,\n",
        "\n",
        "https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec"
      ],
      "metadata": {
        "id": "UYqW6ZFeysQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):                                                           # using nn.Module enables us to use PyTorch's neural network capabilities; this is the base for all neural network models https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)                           # create embedding table to map token indices to corresponding embedding vector https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html. Arg: vocab_size\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])    # this is where the transformer block is called\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    # predicting what comes next based on previous token only\n",
        "    def forward(self, idx, targets=None):                                                       # this method must be implemented under \"nn.Module\" and it defines how input data is processed through the network (incl activation functions, transformations, etc).\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)                                             # (B: batch, T: tokens, C: classes); retrieve embedding vectors. logits = scores\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:                                                                                   # if targets are provided the cross-entropy loss between the predicted logits and the actual targets is computed; otherwise the loss is none\n",
        "            B, T, C = logits.shape                                                              # define as idx to match cross-entropy function expected input\n",
        "            # reshaping to match cross entropy function inputs\n",
        "            logits = logits.view(B*T, C)                                                        # convert to a 2D tensor [row: logits for specific token in the batch for all time steps, col: logits for the different classes (in this case vocab_size, which is 65)]\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)                                             # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
        "\n",
        "        return logits, loss                                                                     # logist = raw score\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):                                                         # predict the new tokens based on the context of the previous token; note to self: '_' can be used when variable name is not relevant within the loop\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]                                                           # becomes (B, C) ; the -1 represents the selection o the last element along this dim (the sequence length)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)                                                   # (B, C); applying the softmax function to the logits (ensure they sum to 1 along the classes dimension) https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)                                  # (B, 1); draw one sample for each probability distribution https://pytorch.org/docs/stable/generated/torch.multinomial.html\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)                                  # concatenates tensor\n",
        "        return idx"
      ],
      "metadata": {
        "id": "ZWa0iXNYsyBv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Train and Validate Model"
      ],
      "metadata": {
        "id": "mJMWQGlCMZin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting an Optimizer"
      ],
      "metadata": {
        "id": "NWE3V1nE2use"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep learning, we want to update the learning rate so that we can perform reasonable updates. Adam (adaptive moment estimation) is a standard optimization learning algo for training DNNs."
      ],
      "metadata": {
        "id": "Gl28vJD23M6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHDP7wcjxXXV",
        "outputId": "359251db-9f4f-46bb-80bb-3d0a74070465"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "cJsJbnmIydnH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preallocate memory\n",
        "train_losses = []\n",
        "val_losses = []"
      ],
      "metadata": {
        "id": "zk84l7AdQ3ry"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        train_loss = losses['train']\n",
        "        val_loss = losses['val']\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # append losses to the lists for plotting\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id2Px7dSydkw",
        "outputId": "7cfc6286-3751-4609-d4bc-b8d6d262d976"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4049, val loss 4.3996\n",
            "step 100: train loss 2.6887, val loss 2.6931\n",
            "step 200: train loss 2.5309, val loss 2.5291\n",
            "step 300: train loss 2.4559, val loss 2.4477\n",
            "step 400: train loss 2.3965, val loss 2.4144\n",
            "step 500: train loss 2.3457, val loss 2.3450\n",
            "step 600: train loss 2.3037, val loss 2.3146\n",
            "step 700: train loss 2.2700, val loss 2.2778\n",
            "step 800: train loss 2.2448, val loss 2.2470\n",
            "step 900: train loss 2.2075, val loss 2.2263\n",
            "step 1000: train loss 2.1828, val loss 2.1934\n",
            "step 1100: train loss 2.1452, val loss 2.1753\n",
            "step 1200: train loss 2.1297, val loss 2.1593\n",
            "step 1300: train loss 2.1219, val loss 2.1547\n",
            "step 1400: train loss 2.0792, val loss 2.1250\n",
            "step 1500: train loss 2.0823, val loss 2.1209\n",
            "step 1600: train loss 2.0524, val loss 2.1051\n",
            "step 1700: train loss 2.0364, val loss 2.0867\n",
            "step 1800: train loss 2.0330, val loss 2.0728\n",
            "step 1900: train loss 2.0157, val loss 2.0559\n",
            "step 2000: train loss 1.9951, val loss 2.0444\n",
            "step 2100: train loss 1.9727, val loss 2.0401\n",
            "step 2200: train loss 1.9674, val loss 2.0415\n",
            "step 2300: train loss 1.9646, val loss 2.0378\n",
            "step 2400: train loss 1.9383, val loss 2.0263\n",
            "step 2500: train loss 1.9298, val loss 2.0144\n",
            "step 2600: train loss 1.9242, val loss 1.9953\n",
            "step 2700: train loss 1.9147, val loss 2.0071\n",
            "step 2800: train loss 1.9026, val loss 1.9879\n",
            "step 2900: train loss 1.8889, val loss 1.9809\n",
            "step 3000: train loss 1.8788, val loss 1.9691\n",
            "step 3100: train loss 1.8825, val loss 1.9656\n",
            "step 3200: train loss 1.8772, val loss 1.9721\n",
            "step 3300: train loss 1.8658, val loss 1.9578\n",
            "step 3400: train loss 1.8517, val loss 1.9578\n",
            "step 3500: train loss 1.8513, val loss 1.9506\n",
            "step 3600: train loss 1.8411, val loss 1.9500\n",
            "step 3700: train loss 1.8265, val loss 1.9476\n",
            "step 3800: train loss 1.8337, val loss 1.9591\n",
            "step 3900: train loss 1.8271, val loss 1.9387\n",
            "step 4000: train loss 1.8142, val loss 1.9264\n",
            "step 4100: train loss 1.8202, val loss 1.9393\n",
            "step 4200: train loss 1.8156, val loss 1.9257\n",
            "step 4300: train loss 1.8108, val loss 1.9349\n",
            "step 4400: train loss 1.8087, val loss 1.9234\n",
            "step 4500: train loss 1.8011, val loss 1.9406\n",
            "step 4600: train loss 1.7908, val loss 1.9210\n",
            "step 4700: train loss 1.7984, val loss 1.9128\n",
            "step 4800: train loss 1.7947, val loss 1.9228\n",
            "step 4900: train loss 1.7928, val loss 1.9172\n",
            "step 4999: train loss 1.7808, val loss 1.9031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "steps = range(max_iters//eval_interval+1)  # x-axis values (steps)\n",
        "plt.plot(steps, train_losses, label='Train Loss')\n",
        "plt.plot(steps, val_losses, label='Val Loss')\n",
        "plt.xlabel('Iter Count')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "8Rxr9u77QDy9",
        "outputId": "b28a1935-ad2a-40ac-fade-867eccdec120"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We see some overfitting defined by lower train losses.*"
      ],
      "metadata": {
        "id": "s1Wl99jU12hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKf1MWULydi6",
        "outputId": "01545c22-bec1-4796-f779-3e5792e66532"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "He complent, Gelow\n",
            "Which of, dave to marreare beoth: rew:\n",
            "Withing-beseed steek your faingma! How, youth, for me bcaly gall fauss agallie.\n",
            "\n",
            "EDWAM:\n",
            "You shall whenall let, diend may thee\n",
            "enour mans and not heir mors mode.\n",
            "\n",
            "AP'd Nrat:\n",
            "Who his kid and fantonen fraw priear.\n",
            "\n",
            "DUKE OF CARET:\n",
            "Oo det, go'lly have in nock I doevery:\n",
            "To shervit OXin what. Lords!\n",
            "\n",
            "EDWARD:\n",
            "And trad:\n",
            "Peint to luch.\n",
            "\n",
            "KING EDd Morget to my hash Dieeke at thing comzice,\n",
            "AnETwas\n",
            "What ind steile to cling bood: bar and it, as wond sande.\n",
            "\n",
            "VIISA LET:\n",
            "a not deare Long upatHthoust you corng hear\n",
            "lew knot mightill, make anin, why smay puece,tit Thyse\n",
            "I dave have are wart watte skind good ato parcimster?\n",
            "Belifkesh as mise\n",
            "Yorreving me to langhter of thesess kind to cartannare vereO:\n",
            "En seelf we? brow, inds an the croth crousht?\n",
            "\n",
            "GLOUCIOL:\n",
            "Thecouch leavis, isind's may;\n",
            "\n",
            "Fis reite man done voury's heyes troblouse:\n",
            "By betwet their Seres;\n",
            "Thou charmuch,estizes endss-will for whopled to this hunder make all is tillongus prines auchemands!\n",
            "And jecicind makity ier:\n",
            "Tolk, to'emnance; by hope he; with were wast for did:\n",
            "You yours dewpert thing whan he rry\n",
            "am thon with it sunder devorg,\n",
            "\n",
            "Not:\n",
            "\n",
            "No, countervereast parsed coan by dit law\n",
            "Why\n",
            "so lornay coned of itight in meinal,\n",
            "On and their Segaved upon them, at fher heras bes.\n",
            "\n",
            "CLourd's Shy:\n",
            "And in you; 'tis fage shall his know iding an chip theurbon grooth of have que town but youldist, bend word one whatn upoge son thread!\n",
            "\n",
            "HARWIV:\n",
            "Has avile Maccup in uplims:\n",
            "Pe\n",
            "IT aye mend, be roth dame\n",
            "its I have pargens 'At of they gaan.\n",
            "\n",
            "Clound alloarminds, his thach reir spoze:\n",
            "Lelemens, my too surry and at wwell\n",
            "As it prokep;\n",
            "Of no kca, in my briblesse clore!\n",
            "\n",
            "AY Rikamay Gruut\n",
            "Of any, and stand anore?\n",
            "\n",
            "Seme:\n",
            "That itle shell know singred nt trave hart bare'd I chan aull\n",
            "And swich youlsill-ow in thy duke boing dine upentir by mary alwitue and was;:\n",
            "Theit eve heread marence thy be\n",
            "deneence: Singgares alter whom by of enctrown\n",
            "Praw all sper: weith then how with hem not perful bled w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    }
  ]
}